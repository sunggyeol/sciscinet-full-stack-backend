{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e31fd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SciSciNet Preprocessing Notebook ===\n",
      "Python version: 3.13.2 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 19:03:27) [GCC 11.2.0]\n",
      "Pandas version: 2.3.0\n",
      "\n",
      "Loaded configuration from validation:\n",
      "  VT Affiliations: ['Virginia Tech']\n",
      "  CS Fields: 39 fields\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=== SciSciNet Preprocessing Notebook ===\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Pandas version: {pd.__version__}\\n\")\n",
    "\n",
    "# Load validation config\n",
    "try:\n",
    "    from validation_config import (\n",
    "        VT_AFFILIATION_IDS, CS_FIELD_IDS, DATA_DIR,\n",
    "        AFFILIATION_ID_COL, AFFILIATION_NAME_COL,\n",
    "        FIELD_ID_COL, FIELD_NAME_COL,\n",
    "        PAPER_ID_COL, AUTHOR_ID_COL, PAA_AFFIL_COL,\n",
    "        VT_AFFILIATION_NAMES, CS_FIELD_NAMES\n",
    "    )\n",
    "    print(\"Loaded configuration from validation:\")\n",
    "    print(f\"  VT Affiliations: {VT_AFFILIATION_NAMES}\")\n",
    "    print(f\"  CS Fields: {len(CS_FIELD_NAMES)} fields\")\n",
    "except ImportError:\n",
    "    print(\"ERROR: validation_config.py not found\")\n",
    "    print(\"Run 01_validation.ipynb first\")\n",
    "    raise\n",
    "\n",
    "# Processing configuration\n",
    "CHUNK_SIZE = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a7ef8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Detecting Dataset Year Range ===\n",
      "\n",
      "Scanning Papers.tsv to detect year range...\n",
      "(Scanning to find maximum year...)\n",
      "\n",
      "Found year column: 'Year'\n",
      "  Scanned 5,000,000 rows... Current max year: 2022\n",
      "\n",
      "Max year stable at 2022 for 50 chunks. Stopping scan.\n",
      "\n",
      "Completed scan: 5,100,000 rows\n",
      "\n",
      "Dataset year range detected:\n",
      "  Minimum year in dataset: 1800\n",
      "  Maximum year (cutoff): 2022\n",
      "\n",
      "Calculated 10-year range: 2013-2022\n",
      "\n",
      "Processing Configuration:\n",
      "  Database: ../data/sciscinet_vt_cs_2013_2022.db\n",
      "  Chunk size: 100,000 rows\n",
      "\n",
      "Year Ranges:\n",
      "  Project 1 T1 (Citation & Collaboration): 2018-2022 (past 5 years)\n",
      "  Project 1 T2 (Timeline): 2013-2022 (past 10 years)\n",
      "  Project 2 (LLM Agent Analysis): Uses all papers from 2013-2022\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Detecting Dataset Year Range ===\\n\")\n",
    "\n",
    "# Detect the actual cutoff year from the dataset\n",
    "papers_file = os.path.join(DATA_DIR, 'SciSciNet_Papers.tsv')\n",
    "\n",
    "print(\"Scanning Papers.tsv to detect year range...\")\n",
    "print(\"(Scanning to find maximum year...)\\n\")\n",
    "\n",
    "year_col = None\n",
    "max_year = None\n",
    "min_year = None\n",
    "total_rows_scanned = 0\n",
    "stable_count = 0  # Count how many chunks have the same max year\n",
    "\n",
    "# Scan through chunks to find the maximum year\n",
    "# Stop when max year is stable for 50 chunks (5M rows)\n",
    "for chunk in pd.read_csv(papers_file, sep='\\t', chunksize=CHUNK_SIZE):\n",
    "    # Identify year column on first chunk\n",
    "    if year_col is None:\n",
    "        for col in ['Year', 'PublicationYear']:\n",
    "            if col in chunk.columns:\n",
    "                year_col = col\n",
    "                break\n",
    "        if year_col is None:\n",
    "            raise ValueError(\"Could not find year column in Papers.tsv\")\n",
    "        print(f\"Found year column: '{year_col}'\")\n",
    "    \n",
    "    # Filter out NaN years\n",
    "    valid_years = chunk[year_col].dropna()\n",
    "    if len(valid_years) > 0:\n",
    "        chunk_max = int(valid_years.max())\n",
    "        chunk_min = int(valid_years.min())\n",
    "        \n",
    "        if max_year is None or chunk_max > max_year:\n",
    "            max_year = chunk_max\n",
    "            stable_count = 0  # Reset stability counter\n",
    "        elif chunk_max == max_year:\n",
    "            stable_count += 1\n",
    "        else:\n",
    "            stable_count = 0  # Reset if we see a lower max\n",
    "        \n",
    "        if min_year is None or chunk_min < min_year:\n",
    "            min_year = chunk_min\n",
    "    \n",
    "    total_rows_scanned += len(chunk)\n",
    "    \n",
    "    # Print progress every 50 chunks\n",
    "    if (total_rows_scanned // CHUNK_SIZE) % 50 == 0:\n",
    "        print(f\"  Scanned {total_rows_scanned:,} rows... Current max year: {max_year}\")\n",
    "    \n",
    "    # Stop if max year has been stable for 50 chunks (5M rows)\n",
    "    if stable_count >= 50:\n",
    "        print(f\"\\nMax year stable at {max_year} for 50 chunks. Stopping scan.\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nCompleted scan: {total_rows_scanned:,} rows\")\n",
    "\n",
    "if max_year is None:\n",
    "    raise ValueError(\"Could not detect year range from dataset\")\n",
    "\n",
    "# Calculate 10-year range from cutoff\n",
    "CUTOFF_YEAR = int(max_year)\n",
    "START_YEAR_T2 = CUTOFF_YEAR - 9  # 10 years inclusive (e.g., 2013-2022)\n",
    "START_YEAR_T1 = CUTOFF_YEAR - 4   # 5 years inclusive (e.g., 2018-2022)\n",
    "END_YEAR = CUTOFF_YEAR\n",
    "\n",
    "# Database name with year range indicator - save to data directory\n",
    "# DATA_DIR is '../data' relative to preprocessing/, so use it directly\n",
    "DB_PATH = os.path.join(DATA_DIR, f'sciscinet_vt_cs_{START_YEAR_T2}_{END_YEAR}.db')\n",
    "\n",
    "print(f\"\\nDataset year range detected:\")\n",
    "print(f\"  Minimum year in dataset: {min_year}\")\n",
    "print(f\"  Maximum year (cutoff): {CUTOFF_YEAR}\")\n",
    "print(f\"\\nCalculated 10-year range: {START_YEAR_T2}-{END_YEAR}\")\n",
    "print(f\"\\nProcessing Configuration:\")\n",
    "print(f\"  Database: {DB_PATH}\")\n",
    "print(f\"  Chunk size: {CHUNK_SIZE:,} rows\")\n",
    "print(f\"\\nYear Ranges:\")\n",
    "print(f\"  Project 1 T1 (Citation & Collaboration): {START_YEAR_T1}-{END_YEAR} (past 5 years)\")\n",
    "print(f\"  Project 1 T2 (Timeline): {START_YEAR_T2}-{END_YEAR} (past 10 years)\")\n",
    "print(f\"  Project 2 (LLM Agent Analysis): Uses all papers from {START_YEAR_T2}-{END_YEAR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b74687b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Creating Database Schema ===\n",
      "\n",
      "Old database '../data/sciscinet_vt_cs.db' exists and will be preserved.\n",
      "New database '../data/sciscinet_vt_cs_2013_2022.db' will be created separately.\n",
      "[OK] Created table: affiliations\n",
      "[OK] Created table: fields\n",
      "[OK] Created table: papers\n",
      "[OK] Created table: paper_details\n",
      "[OK] Created table: paper_author_affiliations\n",
      "[OK] Created table: paper_fields\n",
      "[OK] Created table: paper_references\n",
      "\n",
      "Database schema created\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Creating Database Schema ===\\n\")\n",
    "\n",
    "# Preserve old database name if it exists (check both old and new locations)\n",
    "OLD_DB_NAME_OLD_LOC = 'sciscinet_vt_cs.db'  # Old location in preprocessing/\n",
    "OLD_DB_NAME_NEW_LOC = os.path.join(DATA_DIR, 'sciscinet_vt_cs.db')  # New location in data/\n",
    "\n",
    "if os.path.exists(DB_PATH):\n",
    "    # Only remove if it's not the old database name\n",
    "    if DB_PATH != OLD_DB_NAME_OLD_LOC and DB_PATH != OLD_DB_NAME_NEW_LOC:\n",
    "        os.remove(DB_PATH)\n",
    "        print(f\"Removed existing database: {DB_PATH}\")\n",
    "    else:\n",
    "        print(f\"Preserving old database: {DB_PATH}\")\n",
    "        print(f\"New database will be created with a different name based on year range.\")\n",
    "elif os.path.exists(OLD_DB_NAME_OLD_LOC):\n",
    "    print(f\"Old database '{OLD_DB_NAME_OLD_LOC}' exists and will be preserved.\")\n",
    "    print(f\"New database '{DB_PATH}' will be created separately.\")\n",
    "elif os.path.exists(OLD_DB_NAME_NEW_LOC):\n",
    "    print(f\"Old database '{OLD_DB_NAME_NEW_LOC}' exists and will be preserved.\")\n",
    "    print(f\"New database '{DB_PATH}' will be created separately.\")\n",
    "\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE affiliations (\n",
    "    affiliation_id INTEGER PRIMARY KEY,\n",
    "    affiliation_name TEXT\n",
    ")\n",
    "''')\n",
    "print(\"[OK] Created table: affiliations\")\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE fields (\n",
    "    field_id INTEGER PRIMARY KEY,\n",
    "    field_name TEXT\n",
    ")\n",
    "''')\n",
    "print(\"[OK] Created table: fields\")\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE papers (\n",
    "    paper_id INTEGER PRIMARY KEY,\n",
    "    title TEXT,\n",
    "    year INTEGER,\n",
    "    citation_count INTEGER,\n",
    "    reference_count INTEGER,\n",
    "    patent_count INTEGER\n",
    ")\n",
    "''')\n",
    "print(\"[OK] Created table: papers\")\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE paper_details (\n",
    "    paper_id INTEGER PRIMARY KEY,\n",
    "    abstract TEXT\n",
    ")\n",
    "''')\n",
    "print(\"[OK] Created table: paper_details\")\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE paper_author_affiliations (\n",
    "    paper_id INTEGER,\n",
    "    author_id INTEGER,\n",
    "    affiliation_id INTEGER,\n",
    "    author_sequence INTEGER,\n",
    "    PRIMARY KEY (paper_id, author_id, affiliation_id)\n",
    ")\n",
    "''')\n",
    "print(\"[OK] Created table: paper_author_affiliations\")\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE paper_fields (\n",
    "    paper_id INTEGER,\n",
    "    field_id INTEGER,\n",
    "    score REAL,\n",
    "    PRIMARY KEY (paper_id, field_id)\n",
    ")\n",
    "''')\n",
    "print(\"[OK] Created table: paper_fields\")\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE paper_references (\n",
    "    paper_id INTEGER,\n",
    "    reference_id INTEGER,\n",
    "    PRIMARY KEY (paper_id, reference_id)\n",
    ")\n",
    "''')\n",
    "print(\"[OK] Created table: paper_references\")\n",
    "\n",
    "conn.commit()\n",
    "print(\"\\nDatabase schema created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21f30fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading Affiliations ===\n",
      "\n",
      "Total affiliations: 26,998\n",
      "Virginia Tech affiliations: 1\n",
      "Affiliations loaded to database\n",
      "\n",
      " affiliation_id affiliation_name\n",
      "      859038795    Virginia Tech\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Loading Affiliations ===\\n\")\n",
    "\n",
    "df_affiliations = pd.read_csv(\n",
    "    os.path.join(DATA_DIR, 'SciSciNet_Affiliations.tsv'),\n",
    "    sep='\\t'\n",
    ")\n",
    "\n",
    "print(f\"Total affiliations: {len(df_affiliations):,}\")\n",
    "\n",
    "vt_affiliations = df_affiliations[\n",
    "    df_affiliations[AFFILIATION_ID_COL].isin(VT_AFFILIATION_IDS)\n",
    "]\n",
    "\n",
    "print(f\"Virginia Tech affiliations: {len(vt_affiliations)}\")\n",
    "\n",
    "vt_affiliations_clean = vt_affiliations.rename(columns={\n",
    "    AFFILIATION_ID_COL: 'affiliation_id',\n",
    "    AFFILIATION_NAME_COL: 'affiliation_name'\n",
    "})[['affiliation_id', 'affiliation_name']]\n",
    "\n",
    "vt_affiliations_clean.to_sql('affiliations', conn, if_exists='replace', index=False)\n",
    "\n",
    "print(\"Affiliations loaded to database\")\n",
    "print(f\"\\n{vt_affiliations_clean.to_string(index=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b451f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading Fields ===\n",
      "\n",
      "Total fields: 311\n",
      "Computer Science fields: 39\n",
      "Fields loaded to database\n",
      "\n",
      "  field_id                   field_name\n",
      " 113775141         Computer engineering\n",
      " 124101348                  Data mining\n",
      "  56739046         Knowledge management\n",
      " 149635348              Embedded system\n",
      " 107457646   Humanâ€“computer interaction\n",
      "  11413529                    Algorithm\n",
      "  28490314           Speech recognition\n",
      " 111919701             Operating system\n",
      "  31972630              Computer vision\n",
      "  77088390                     Database\n",
      " 108827166             Internet privacy\n",
      "  76155785           Telecommunications\n",
      "   9390403            Computer hardware\n",
      " 154945302      Artificial intelligence\n",
      "  49774154                   Multimedia\n",
      "    459310        Computational science\n",
      " 199360897         Programming language\n",
      "  38652104            Computer security\n",
      "  79403827          Real-time computing\n",
      "  41008148             Computer science\n",
      " 204321447  Natural language processing\n",
      " 188147891            Cognitive science\n",
      " 136764020               World Wide Web\n",
      " 147597530      Computational chemistry\n",
      " 120314980        Distributed computing\n",
      "  80444323 Theoretical computer science\n",
      "  60644358               Bioinformatics\n",
      " 115903868         Software engineering\n",
      "2522767166                 Data science\n",
      "  31258907             Computer network\n",
      " 119857082             Machine learning\n",
      "  44154836                   Simulation\n",
      " 173608175           Parallel computing\n",
      "  23123220        Information retrieval\n",
      "  30475298        Computational physics\n",
      " 121684516   Computer graphics (images)\n",
      " 178980831          Pattern recognition\n",
      " 118524514        Computer architecture\n",
      "  70721500        Computational biology\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Loading Fields ===\\n\")\n",
    "\n",
    "df_fields = pd.read_csv(\n",
    "    os.path.join(DATA_DIR, 'SciSciNet_Fields.tsv'),\n",
    "    sep='\\t'\n",
    ")\n",
    "\n",
    "print(f\"Total fields: {len(df_fields):,}\")\n",
    "\n",
    "cs_fields = df_fields[\n",
    "    df_fields[FIELD_ID_COL].isin(CS_FIELD_IDS)\n",
    "]\n",
    "\n",
    "print(f\"Computer Science fields: {len(cs_fields)}\")\n",
    "\n",
    "cs_fields_clean = cs_fields.rename(columns={\n",
    "    FIELD_ID_COL: 'field_id',\n",
    "    FIELD_NAME_COL: 'field_name'\n",
    "})[['field_id', 'field_name']]\n",
    "\n",
    "cs_fields_clean.to_sql('fields', conn, if_exists='replace', index=False)\n",
    "\n",
    "print(\"Fields loaded to database\")\n",
    "print(f\"\\n{cs_fields_clean.to_string(index=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95a52a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 1: Filtering Paper-Author-Affiliations for VT ===\n",
      "\n",
      "Processing large file (~11.68 GB)...\n",
      "\n",
      "  Chunk 50: 5,000,000 rows | 1,933 VT records | 2126851 rows/sec\n",
      "  Chunk 100: 10,000,000 rows | 3,922 VT records | 2118229 rows/sec\n",
      "  Chunk 150: 15,000,000 rows | 4,954 VT records | 2177680 rows/sec\n",
      "  Chunk 200: 20,000,000 rows | 5,742 VT records | 2227797 rows/sec\n",
      "  Chunk 250: 25,000,000 rows | 7,701 VT records | 2182043 rows/sec\n",
      "  Chunk 300: 30,000,000 rows | 9,959 VT records | 2145627 rows/sec\n",
      "  Chunk 350: 35,000,000 rows | 12,298 VT records | 2125553 rows/sec\n",
      "  Chunk 400: 40,000,000 rows | 14,323 VT records | 2106511 rows/sec\n",
      "  Chunk 450: 45,000,000 rows | 17,179 VT records | 2084872 rows/sec\n",
      "  Chunk 500: 50,000,000 rows | 20,514 VT records | 2058679 rows/sec\n",
      "  Chunk 550: 55,000,000 rows | 23,959 VT records | 961225 rows/sec\n",
      "  Chunk 600: 60,000,000 rows | 27,480 VT records | 1002611 rows/sec\n",
      "  Chunk 650: 65,000,000 rows | 30,837 VT records | 1040894 rows/sec\n",
      "  Chunk 700: 70,000,000 rows | 34,293 VT records | 1075800 rows/sec\n",
      "  Chunk 750: 75,000,000 rows | 37,689 VT records | 1108497 rows/sec\n",
      "  Chunk 800: 80,000,000 rows | 40,998 VT records | 1139670 rows/sec\n",
      "  Chunk 850: 85,000,000 rows | 44,324 VT records | 1168397 rows/sec\n",
      "  Chunk 900: 90,000,000 rows | 47,457 VT records | 1195071 rows/sec\n",
      "  Chunk 950: 95,000,000 rows | 50,825 VT records | 1220464 rows/sec\n",
      "  Chunk 1000: 100,000,000 rows | 54,159 VT records | 1243689 rows/sec\n",
      "  Chunk 1050: 105,000,000 rows | 57,411 VT records | 1265766 rows/sec\n",
      "  Chunk 1100: 110,000,000 rows | 60,721 VT records | 1286983 rows/sec\n",
      "  Chunk 1150: 115,000,000 rows | 63,923 VT records | 1306863 rows/sec\n",
      "  Chunk 1200: 120,000,000 rows | 67,244 VT records | 994639 rows/sec\n",
      "  Chunk 1250: 125,000,000 rows | 70,625 VT records | 1014875 rows/sec\n",
      "  Chunk 1300: 130,000,000 rows | 73,844 VT records | 1034093 rows/sec\n",
      "  Chunk 1350: 135,000,000 rows | 77,003 VT records | 1052755 rows/sec\n",
      "  Chunk 1400: 140,000,000 rows | 80,388 VT records | 1070673 rows/sec\n",
      "  Chunk 1450: 145,000,000 rows | 83,699 VT records | 1087877 rows/sec\n",
      "  Chunk 1500: 150,000,000 rows | 87,005 VT records | 1104284 rows/sec\n",
      "  Chunk 1550: 155,000,000 rows | 90,340 VT records | 1119999 rows/sec\n",
      "  Chunk 1600: 160,000,000 rows | 93,602 VT records | 1134942 rows/sec\n",
      "  Chunk 1650: 165,000,000 rows | 97,053 VT records | 1149093 rows/sec\n",
      "  Chunk 1700: 170,000,000 rows | 100,426 VT records | 1162927 rows/sec\n",
      "  Chunk 1750: 175,000,000 rows | 103,934 VT records | 1176164 rows/sec\n",
      "  Chunk 1800: 180,000,000 rows | 107,241 VT records | 1189009 rows/sec\n",
      "  Chunk 1850: 185,000,000 rows | 110,745 VT records | 1201481 rows/sec\n",
      "  Chunk 1900: 190,000,000 rows | 114,305 VT records | 1213719 rows/sec\n",
      "  Chunk 1950: 195,000,000 rows | 116,447 VT records | 1226488 rows/sec\n",
      "  Chunk 2000: 200,000,000 rows | 117,987 VT records | 1239374 rows/sec\n",
      "  Chunk 2050: 205,000,000 rows | 120,062 VT records | 1251805 rows/sec\n",
      "  Chunk 2100: 210,000,000 rows | 123,000 VT records | 1264447 rows/sec\n",
      "  Chunk 2150: 215,000,000 rows | 126,050 VT records | 1275432 rows/sec\n",
      "  Chunk 2200: 220,000,000 rows | 127,978 VT records | 1285769 rows/sec\n",
      "  Chunk 2250: 225,000,000 rows | 128,134 VT records | 1298114 rows/sec\n",
      "  Chunk 2300: 230,000,000 rows | 128,860 VT records | 1310239 rows/sec\n",
      "  Chunk 2350: 235,000,000 rows | 129,595 VT records | 1322367 rows/sec\n",
      "  Chunk 2400: 240,000,000 rows | 130,259 VT records | 1334311 rows/sec\n",
      "  Chunk 2450: 245,000,000 rows | 131,221 VT records | 1345287 rows/sec\n",
      "  Chunk 2500: 250,000,000 rows | 132,230 VT records | 1356295 rows/sec\n",
      "  Chunk 2550: 255,000,000 rows | 133,651 VT records | 1366090 rows/sec\n",
      "  Chunk 2600: 260,000,000 rows | 135,432 VT records | 1374893 rows/sec\n",
      "  Chunk 2650: 265,000,000 rows | 137,610 VT records | 1383360 rows/sec\n",
      "  Chunk 2700: 270,000,000 rows | 139,410 VT records | 1392011 rows/sec\n",
      "  Chunk 2750: 275,000,000 rows | 141,166 VT records | 1400521 rows/sec\n",
      "  Chunk 2800: 280,000,000 rows | 142,997 VT records | 1408757 rows/sec\n",
      "  Chunk 2850: 285,000,000 rows | 144,878 VT records | 1416899 rows/sec\n",
      "  Chunk 2900: 290,000,000 rows | 147,122 VT records | 1424037 rows/sec\n",
      "  Chunk 2950: 295,000,000 rows | 149,165 VT records | 1431341 rows/sec\n",
      "  Chunk 3000: 300,000,000 rows | 151,003 VT records | 1438616 rows/sec\n",
      "  Chunk 3050: 305,000,000 rows | 153,033 VT records | 1445382 rows/sec\n",
      "  Chunk 3100: 310,000,000 rows | 155,156 VT records | 1451958 rows/sec\n",
      "  Chunk 3150: 315,000,000 rows | 157,387 VT records | 1458299 rows/sec\n",
      "  Chunk 3200: 320,000,000 rows | 159,563 VT records | 1464881 rows/sec\n",
      "  Chunk 3250: 325,000,000 rows | 161,499 VT records | 1471060 rows/sec\n",
      "  Chunk 3300: 330,000,000 rows | 163,111 VT records | 1477492 rows/sec\n",
      "  Chunk 3350: 335,000,000 rows | 165,773 VT records | 1483281 rows/sec\n",
      "  Chunk 3400: 340,000,000 rows | 167,425 VT records | 1489435 rows/sec\n",
      "  Chunk 3450: 345,000,000 rows | 169,307 VT records | 1495299 rows/sec\n",
      "  Chunk 3500: 350,000,000 rows | 171,362 VT records | 1501017 rows/sec\n",
      "  Chunk 3550: 355,000,000 rows | 173,189 VT records | 1506610 rows/sec\n",
      "  Chunk 3600: 360,000,000 rows | 174,156 VT records | 1512768 rows/sec\n",
      "  Chunk 3650: 365,000,000 rows | 176,152 VT records | 1518049 rows/sec\n",
      "  Chunk 3700: 370,000,000 rows | 177,896 VT records | 1523064 rows/sec\n",
      "  Chunk 3750: 375,000,000 rows | 180,033 VT records | 1528182 rows/sec\n",
      "  Chunk 3800: 380,000,000 rows | 181,965 VT records | 1533351 rows/sec\n",
      "  Chunk 3850: 385,000,000 rows | 183,860 VT records | 1538416 rows/sec\n",
      "  Chunk 3900: 390,000,000 rows | 185,500 VT records | 1543386 rows/sec\n",
      "  Chunk 3950: 395,000,000 rows | 187,186 VT records | 1548301 rows/sec\n",
      "  Chunk 4000: 400,000,000 rows | 188,782 VT records | 1553047 rows/sec\n",
      "  Chunk 4050: 405,000,000 rows | 190,710 VT records | 1557506 rows/sec\n",
      "  Chunk 4100: 410,000,000 rows | 192,254 VT records | 1401492 rows/sec\n",
      "\n",
      "Step 1 Complete\n",
      "  Time: 294.4 seconds\n",
      "  Total rows: 413,869,501\n",
      "  VT records: 193,408\n",
      "  Unique VT papers: 94,577\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Step 1: Filtering Paper-Author-Affiliations for VT ===\\n\")\n",
    "print(\"Processing large file (~11.68 GB)...\\n\")\n",
    "\n",
    "file_path = os.path.join(DATA_DIR, 'SciSciNet_PaperAuthorAffiliations.tsv')\n",
    "vt_papers_set = set()\n",
    "\n",
    "chunk_count = 0\n",
    "total_rows = 0\n",
    "filtered_rows = 0\n",
    "start_time = datetime.now()\n",
    "\n",
    "for chunk in pd.read_csv(file_path, sep='\\t', chunksize=CHUNK_SIZE):\n",
    "    chunk_count += 1\n",
    "    total_rows += len(chunk)\n",
    "    \n",
    "    vt_chunk = chunk[chunk[PAA_AFFIL_COL].isin(VT_AFFILIATION_IDS)]\n",
    "    filtered_rows += len(vt_chunk)\n",
    "    \n",
    "    if len(vt_chunk) > 0:\n",
    "        vt_papers_set.update(vt_chunk[PAPER_ID_COL].dropna().astype(int).values)\n",
    "        \n",
    "        # Determine sequence column\n",
    "        seq_col = None\n",
    "        for col in ['AuthorSequenceNumber', 'SequenceNumber', 'AuthorSequence']:\n",
    "            if col in vt_chunk.columns:\n",
    "                seq_col = col\n",
    "                break\n",
    "        \n",
    "        cols_map = {\n",
    "            PAPER_ID_COL: 'paper_id',\n",
    "            AUTHOR_ID_COL: 'author_id',\n",
    "            PAA_AFFIL_COL: 'affiliation_id'\n",
    "        }\n",
    "        if seq_col:\n",
    "            cols_map[seq_col] = 'author_sequence'\n",
    "        \n",
    "        vt_chunk_clean = vt_chunk.rename(columns=cols_map)\n",
    "        keep_cols = [col for col in ['paper_id', 'author_id', 'affiliation_id', 'author_sequence'] if col in vt_chunk_clean.columns]\n",
    "        vt_chunk_clean[keep_cols].to_sql('paper_author_affiliations', conn, if_exists='append', index=False)\n",
    "    \n",
    "    if chunk_count % 50 == 0:\n",
    "        elapsed = (datetime.now() - start_time).total_seconds()\n",
    "        rate = total_rows / elapsed if elapsed > 0 else 0\n",
    "        print(f\"  Chunk {chunk_count}: {total_rows:,} rows | {filtered_rows:,} VT records | {rate:.0f} rows/sec\")\n",
    "\n",
    "elapsed_time = (datetime.now() - start_time).total_seconds()\n",
    "print(f\"\\nStep 1 Complete\")\n",
    "print(f\"  Time: {elapsed_time:.1f} seconds\")\n",
    "print(f\"  Total rows: {total_rows:,}\")\n",
    "print(f\"  VT records: {filtered_rows:,}\")\n",
    "print(f\"  Unique VT papers: {len(vt_papers_set):,}\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "399ea000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 2: Filtering Papers for VT + Year Range ===\n",
      "\n",
      "Processing Papers.tsv (~16.46 GB)...\n",
      "\n",
      "Using year range: 2013-2022 (to support all projects)\n",
      "\n",
      "  Chunk 50: 5,000,000 rows | 1,480 filtered | 278465 rows/sec\n",
      "  Chunk 100: 10,000,000 rows | 2,957 filtered | 278331 rows/sec\n",
      "  Chunk 150: 15,000,000 rows | 4,421 filtered | 277349 rows/sec\n",
      "  Chunk 200: 20,000,000 rows | 5,910 filtered | 276762 rows/sec\n",
      "  Chunk 250: 25,000,000 rows | 7,358 filtered | 276964 rows/sec\n",
      "  Chunk 300: 30,000,000 rows | 8,824 filtered | 276984 rows/sec\n",
      "  Chunk 350: 35,000,000 rows | 10,315 filtered | 276972 rows/sec\n",
      "  Chunk 400: 40,000,000 rows | 11,782 filtered | 276755 rows/sec\n",
      "  Chunk 450: 45,000,000 rows | 13,255 filtered | 276576 rows/sec\n",
      "  Chunk 500: 50,000,000 rows | 14,822 filtered | 276250 rows/sec\n",
      "  Chunk 550: 55,000,000 rows | 16,309 filtered | 275855 rows/sec\n",
      "  Chunk 600: 60,000,000 rows | 17,809 filtered | 275579 rows/sec\n",
      "  Chunk 650: 65,000,000 rows | 19,291 filtered | 275073 rows/sec\n",
      "  Chunk 700: 70,000,000 rows | 20,824 filtered | 274760 rows/sec\n",
      "  Chunk 750: 75,000,000 rows | 22,335 filtered | 274566 rows/sec\n",
      "  Chunk 800: 80,000,000 rows | 23,780 filtered | 274326 rows/sec\n",
      "  Chunk 850: 85,000,000 rows | 25,264 filtered | 274041 rows/sec\n",
      "  Chunk 900: 90,000,000 rows | 26,728 filtered | 273931 rows/sec\n",
      "  Chunk 950: 95,000,000 rows | 28,264 filtered | 273792 rows/sec\n",
      "  Chunk 1000: 100,000,000 rows | 29,711 filtered | 273576 rows/sec\n",
      "  Chunk 1050: 105,000,000 rows | 31,179 filtered | 273260 rows/sec\n",
      "  Chunk 1100: 110,000,000 rows | 32,709 filtered | 272916 rows/sec\n",
      "  Chunk 1150: 115,000,000 rows | 34,232 filtered | 272776 rows/sec\n",
      "  Chunk 1200: 120,000,000 rows | 35,691 filtered | 272684 rows/sec\n",
      "  Chunk 1250: 125,000,000 rows | 37,187 filtered | 272589 rows/sec\n",
      "  Chunk 1300: 130,000,000 rows | 38,656 filtered | 272424 rows/sec\n",
      "\n",
      "Step 2 Complete\n",
      "  Time: 492.6 seconds\n",
      "  VT papers in year range: 39,903\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Step 2: Filtering Papers for VT + Year Range ===\\n\")\n",
    "print(\"Processing Papers.tsv (~16.46 GB)...\\n\")\n",
    "print(f\"Using year range: {START_YEAR_T2}-{END_YEAR} (to support all projects)\\n\")\n",
    "\n",
    "file_path = os.path.join(DATA_DIR, 'SciSciNet_Papers.tsv')\n",
    "vt_papers_in_range_set = set()\n",
    "\n",
    "chunk_count = 0\n",
    "total_rows = 0\n",
    "filtered_rows = 0\n",
    "start_time = datetime.now()\n",
    "\n",
    "for chunk in pd.read_csv(file_path, sep='\\t', chunksize=CHUNK_SIZE):\n",
    "    chunk_count += 1\n",
    "    total_rows += len(chunk)\n",
    "    \n",
    "    # Identify year column\n",
    "    year_col = None\n",
    "    for col in ['Year', 'PublicationYear']:\n",
    "        if col in chunk.columns:\n",
    "            year_col = col\n",
    "            break\n",
    "    \n",
    "    filtered_chunk = chunk[\n",
    "        (chunk[PAPER_ID_COL].isin(vt_papers_set)) &\n",
    "        (chunk[year_col] >= START_YEAR_T2) &\n",
    "        (chunk[year_col] <= END_YEAR)\n",
    "    ]\n",
    "    filtered_rows += len(filtered_chunk)\n",
    "    \n",
    "    if len(filtered_chunk) > 0:\n",
    "        vt_papers_in_range_set.update(filtered_chunk[PAPER_ID_COL].dropna().astype(int).values)\n",
    "        \n",
    "        # Identify columns\n",
    "        title_col = None\n",
    "        for col in ['PaperTitle', 'OriginalTitle', 'Title']:\n",
    "            if col in filtered_chunk.columns:\n",
    "                title_col = col\n",
    "                break\n",
    "        \n",
    "        citation_col = None\n",
    "        for col in ['CitationCount', 'Citations', 'Citation_Count']:\n",
    "            if col in filtered_chunk.columns:\n",
    "                citation_col = col\n",
    "                break\n",
    "        \n",
    "        reference_col = None\n",
    "        for col in ['ReferenceCount', 'References', 'Reference_Count']:\n",
    "            if col in filtered_chunk.columns:\n",
    "                reference_col = col\n",
    "                break\n",
    "        \n",
    "        patent_col = None\n",
    "        for col in ['Patent_Count', 'PatentCount']:\n",
    "            if col in filtered_chunk.columns:\n",
    "                patent_col = col\n",
    "                break\n",
    "        \n",
    "        cols_map = {PAPER_ID_COL: 'paper_id'}\n",
    "        if year_col:\n",
    "            cols_map[year_col] = 'year'\n",
    "        if title_col:\n",
    "            cols_map[title_col] = 'title'\n",
    "        if citation_col:\n",
    "            cols_map[citation_col] = 'citation_count'\n",
    "        if reference_col:\n",
    "            cols_map[reference_col] = 'reference_count'\n",
    "        if patent_col:\n",
    "            cols_map[patent_col] = 'patent_count'\n",
    "        \n",
    "        filtered_chunk_clean = filtered_chunk.rename(columns=cols_map)\n",
    "        \n",
    "        keep_cols = [col for col in ['paper_id', 'title', 'year', 'citation_count', 'reference_count', 'patent_count'] if col in filtered_chunk_clean.columns]\n",
    "        \n",
    "        filtered_chunk_clean[keep_cols].to_sql('papers', conn, if_exists='append', index=False)\n",
    "    \n",
    "    if chunk_count % 50 == 0:\n",
    "        elapsed = (datetime.now() - start_time).total_seconds()\n",
    "        rate = total_rows / elapsed if elapsed > 0 else 0\n",
    "        print(f\"  Chunk {chunk_count}: {total_rows:,} rows | {filtered_rows:,} filtered | {rate:.0f} rows/sec\")\n",
    "\n",
    "elapsed_time = (datetime.now() - start_time).total_seconds()\n",
    "print(f\"\\nStep 2 Complete\")\n",
    "print(f\"  Time: {elapsed_time:.1f} seconds\")\n",
    "print(f\"  VT papers in year range: {len(vt_papers_in_range_set):,}\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "248a546b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 3: Filtering for Computer Science Papers ===\n",
      "\n",
      "Processing PaperFields.tsv (~11.62 GB)...\n",
      "\n",
      "Using columns: paper='PaperID', field='FieldID', score='None'\n",
      "\n",
      "  Chunk 50: 5,000,000 rows | 88 CS papers | 1199755 rows/sec\n",
      "  Chunk 100: 10,000,000 rows | 203 CS papers | 1194513 rows/sec\n",
      "  Chunk 150: 15,000,000 rows | 376 CS papers | 1182288 rows/sec\n",
      "  Chunk 200: 20,000,000 rows | 503 CS papers | 1175363 rows/sec\n",
      "  Chunk 250: 25,000,000 rows | 620 CS papers | 1171805 rows/sec\n",
      "  Chunk 300: 30,000,000 rows | 751 CS papers | 1170495 rows/sec\n",
      "  Chunk 350: 35,000,000 rows | 881 CS papers | 1169634 rows/sec\n",
      "  Chunk 400: 40,000,000 rows | 1,028 CS papers | 1169598 rows/sec\n",
      "  Chunk 450: 45,000,000 rows | 1,174 CS papers | 1169503 rows/sec\n",
      "  Chunk 500: 50,000,000 rows | 1,305 CS papers | 1168932 rows/sec\n",
      "  Chunk 550: 55,000,000 rows | 1,445 CS papers | 1168822 rows/sec\n",
      "  Chunk 600: 60,000,000 rows | 1,714 CS papers | 1171321 rows/sec\n",
      "  Chunk 650: 65,000,000 rows | 1,843 CS papers | 1177072 rows/sec\n",
      "  Chunk 700: 70,000,000 rows | 1,916 CS papers | 1184614 rows/sec\n",
      "  Chunk 750: 75,000,000 rows | 2,022 CS papers | 1190102 rows/sec\n",
      "  Chunk 800: 80,000,000 rows | 2,216 CS papers | 1193268 rows/sec\n",
      "  Chunk 850: 85,000,000 rows | 2,670 CS papers | 1194517 rows/sec\n",
      "  Chunk 900: 90,000,000 rows | 3,309 CS papers | 1195703 rows/sec\n",
      "  Chunk 950: 95,000,000 rows | 4,282 CS papers | 1196485 rows/sec\n",
      "  Chunk 1000: 100,000,000 rows | 5,179 CS papers | 1197432 rows/sec\n",
      "  Chunk 1050: 105,000,000 rows | 6,165 CS papers | 1198404 rows/sec\n",
      "  Chunk 1100: 110,000,000 rows | 7,070 CS papers | 1199877 rows/sec\n",
      "  Chunk 1150: 115,000,000 rows | 7,905 CS papers | 1202688 rows/sec\n",
      "  Chunk 1200: 120,000,000 rows | 8,297 CS papers | 1204932 rows/sec\n",
      "  Chunk 1250: 125,000,000 rows | 8,388 CS papers | 1206021 rows/sec\n",
      "  Chunk 1300: 130,000,000 rows | 8,545 CS papers | 1205750 rows/sec\n",
      "  Chunk 1350: 135,000,000 rows | 8,741 CS papers | 1204449 rows/sec\n",
      "  Chunk 1400: 140,000,000 rows | 8,953 CS papers | 1202599 rows/sec\n",
      "  Chunk 1450: 145,000,000 rows | 9,111 CS papers | 1200573 rows/sec\n",
      "  Chunk 1500: 150,000,000 rows | 9,278 CS papers | 1198481 rows/sec\n",
      "  Chunk 1550: 155,000,000 rows | 9,430 CS papers | 1197100 rows/sec\n",
      "  Chunk 1600: 160,000,000 rows | 9,599 CS papers | 1195708 rows/sec\n",
      "  Chunk 1650: 165,000,000 rows | 9,761 CS papers | 1194611 rows/sec\n",
      "  Chunk 1700: 170,000,000 rows | 9,917 CS papers | 1193868 rows/sec\n",
      "  Chunk 1750: 175,000,000 rows | 10,074 CS papers | 1193190 rows/sec\n",
      "  Chunk 1800: 180,000,000 rows | 10,244 CS papers | 1192768 rows/sec\n",
      "  Chunk 1850: 185,000,000 rows | 10,396 CS papers | 1192155 rows/sec\n",
      "  Chunk 1900: 190,000,000 rows | 10,549 CS papers | 1191100 rows/sec\n",
      "  Chunk 1950: 195,000,000 rows | 10,699 CS papers | 1190074 rows/sec\n",
      "  Chunk 2000: 200,000,000 rows | 10,922 CS papers | 1189381 rows/sec\n",
      "  Chunk 2050: 205,000,000 rows | 11,245 CS papers | 1189883 rows/sec\n",
      "  Chunk 2100: 210,000,000 rows | 11,491 CS papers | 1190517 rows/sec\n",
      "  Chunk 2150: 215,000,000 rows | 11,579 CS papers | 1193036 rows/sec\n",
      "  Chunk 2200: 220,000,000 rows | 11,663 CS papers | 1195039 rows/sec\n",
      "  Chunk 2250: 225,000,000 rows | 11,734 CS papers | 1196997 rows/sec\n",
      "  Chunk 2300: 230,000,000 rows | 11,902 CS papers | 1197826 rows/sec\n",
      "  Chunk 2350: 235,000,000 rows | 12,144 CS papers | 1198362 rows/sec\n",
      "  Chunk 2400: 240,000,000 rows | 12,723 CS papers | 1198638 rows/sec\n",
      "  Chunk 2450: 245,000,000 rows | 13,259 CS papers | 1199139 rows/sec\n",
      "  Chunk 2500: 250,000,000 rows | 13,904 CS papers | 1199200 rows/sec\n",
      "  Chunk 2550: 255,000,000 rows | 14,624 CS papers | 1199425 rows/sec\n",
      "  Chunk 2600: 260,000,000 rows | 15,426 CS papers | 1199733 rows/sec\n",
      "  Chunk 2650: 265,000,000 rows | 16,199 CS papers | 1200056 rows/sec\n",
      "  Chunk 2700: 270,000,000 rows | 16,928 CS papers | 1200473 rows/sec\n",
      "  Chunk 2750: 275,000,000 rows | 17,655 CS papers | 1201645 rows/sec\n",
      "\n",
      "Step 3 Complete\n",
      "  Time: 230.8 seconds\n",
      "  Final VT CS papers: 10,293\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Step 3: Filtering for Computer Science Papers ===\\n\")\n",
    "print(\"Processing PaperFields.tsv (~11.62 GB)...\\n\")\n",
    "\n",
    "file_path = os.path.join(DATA_DIR, 'SciSciNet_PaperFields.tsv')\n",
    "final_cs_papers_set = set()\n",
    "\n",
    "chunk_count = 0\n",
    "total_rows = 0\n",
    "filtered_rows = 0\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Peek at columns\n",
    "sample = pd.read_csv(file_path, sep='\\t', nrows=1)\n",
    "pf_paper_col = None\n",
    "pf_field_col = None\n",
    "pf_score_col = None\n",
    "\n",
    "for col in sample.columns:\n",
    "    col_lower = col.lower()\n",
    "    if 'paper' in col_lower and 'id' in col_lower:\n",
    "        pf_paper_col = col\n",
    "    if 'field' in col_lower and 'id' in col_lower:\n",
    "        pf_field_col = col\n",
    "    if 'score' in col_lower:\n",
    "        pf_score_col = col\n",
    "\n",
    "print(f\"Using columns: paper='{pf_paper_col}', field='{pf_field_col}', score='{pf_score_col}'\\n\")\n",
    "\n",
    "for chunk in pd.read_csv(file_path, sep='\\t', chunksize=CHUNK_SIZE):\n",
    "    chunk_count += 1\n",
    "    total_rows += len(chunk)\n",
    "    \n",
    "    cs_chunk = chunk[\n",
    "        (chunk[pf_paper_col].isin(vt_papers_in_range_set)) &\n",
    "        (chunk[pf_field_col].isin(CS_FIELD_IDS))\n",
    "    ]\n",
    "    filtered_rows += len(cs_chunk)\n",
    "    \n",
    "    if len(cs_chunk) > 0:\n",
    "        final_cs_papers_set.update(cs_chunk[pf_paper_col].dropna().astype(int).values)\n",
    "        \n",
    "        cols_map = {\n",
    "            pf_paper_col: 'paper_id',\n",
    "            pf_field_col: 'field_id'\n",
    "        }\n",
    "        if pf_score_col:\n",
    "            cols_map[pf_score_col] = 'score'\n",
    "        \n",
    "        cs_chunk_clean = cs_chunk.rename(columns=cols_map)\n",
    "        keep_cols = [col for col in ['paper_id', 'field_id', 'score'] if col in cs_chunk_clean.columns]\n",
    "        cs_chunk_clean[keep_cols].to_sql('paper_fields', conn, if_exists='append', index=False)\n",
    "    \n",
    "    if chunk_count % 50 == 0:\n",
    "        elapsed = (datetime.now() - start_time).total_seconds()\n",
    "        rate = total_rows / elapsed if elapsed > 0 else 0\n",
    "        print(f\"  Chunk {chunk_count}: {total_rows:,} rows | {filtered_rows:,} CS papers | {rate:.0f} rows/sec\")\n",
    "\n",
    "elapsed_time = (datetime.now() - start_time).total_seconds()\n",
    "print(f\"\\nStep 3 Complete\")\n",
    "print(f\"  Time: {elapsed_time:.1f} seconds\")\n",
    "print(f\"  Final VT CS papers: {len(final_cs_papers_set):,}\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54ff1187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 4: Building Citation Network ===\n",
      "\n",
      "Processing PaperReferences.tsv (~32.41 GB)...\n",
      "\n",
      "Available columns: ['Citing_PaperID', 'Cited_PaperID']\n",
      "\n",
      "Detected columns:\n",
      "  Paper (citing): 'Citing_PaperID'\n",
      "  Reference (cited): 'Cited_PaperID'\n",
      "\n",
      "  Chunk 50: 5,000,000 rows | 134 citations | 2721945 rows/sec\n",
      "  Chunk 100: 10,000,000 rows | 470 citations | 2786990 rows/sec\n",
      "  Chunk 150: 15,000,000 rows | 656 citations | 2777544 rows/sec\n",
      "  Chunk 200: 20,000,000 rows | 1,060 citations | 2789385 rows/sec\n",
      "  Chunk 250: 25,000,000 rows | 1,167 citations | 2776417 rows/sec\n",
      "  Chunk 300: 30,000,000 rows | 1,380 citations | 2763943 rows/sec\n",
      "  Chunk 350: 35,000,000 rows | 1,565 citations | 2750236 rows/sec\n",
      "  Chunk 400: 40,000,000 rows | 1,985 citations | 2731276 rows/sec\n",
      "  Chunk 450: 45,000,000 rows | 2,386 citations | 2722707 rows/sec\n",
      "  Chunk 500: 50,000,000 rows | 3,069 citations | 2711202 rows/sec\n",
      "  Chunk 550: 55,000,000 rows | 3,781 citations | 2703653 rows/sec\n",
      "  Chunk 600: 60,000,000 rows | 4,626 citations | 2693005 rows/sec\n",
      "  Chunk 650: 65,000,000 rows | 5,564 citations | 2674789 rows/sec\n",
      "  Chunk 700: 70,000,000 rows | 6,105 citations | 2660495 rows/sec\n",
      "  Chunk 750: 75,000,000 rows | 6,432 citations | 2651009 rows/sec\n",
      "  Chunk 800: 80,000,000 rows | 6,817 citations | 2642687 rows/sec\n",
      "  Chunk 850: 85,000,000 rows | 7,076 citations | 2634838 rows/sec\n",
      "  Chunk 900: 90,000,000 rows | 7,476 citations | 2626752 rows/sec\n",
      "  Chunk 950: 95,000,000 rows | 7,955 citations | 2618072 rows/sec\n",
      "  Chunk 1000: 100,000,000 rows | 8,461 citations | 2614317 rows/sec\n",
      "  Chunk 1050: 105,000,000 rows | 9,292 citations | 2607167 rows/sec\n",
      "  Chunk 1100: 110,000,000 rows | 9,637 citations | 2602585 rows/sec\n",
      "  Chunk 1150: 115,000,000 rows | 9,998 citations | 2598299 rows/sec\n",
      "  Chunk 1200: 120,000,000 rows | 10,589 citations | 2592618 rows/sec\n",
      "  Chunk 1250: 125,000,000 rows | 11,068 citations | 2586933 rows/sec\n",
      "  Chunk 1300: 130,000,000 rows | 11,617 citations | 2582332 rows/sec\n",
      "  Chunk 1350: 135,000,000 rows | 12,146 citations | 2578558 rows/sec\n",
      "  Chunk 1400: 140,000,000 rows | 12,516 citations | 2575570 rows/sec\n",
      "  Chunk 1450: 145,000,000 rows | 13,200 citations | 2570810 rows/sec\n",
      "  Chunk 1500: 150,000,000 rows | 14,022 citations | 2564860 rows/sec\n",
      "  Chunk 1550: 155,000,000 rows | 15,160 citations | 2560108 rows/sec\n",
      "  Chunk 1600: 160,000,000 rows | 15,478 citations | 2559837 rows/sec\n",
      "  Chunk 1650: 165,000,000 rows | 15,770 citations | 2561207 rows/sec\n",
      "  Chunk 1700: 170,000,000 rows | 15,949 citations | 2561510 rows/sec\n",
      "  Chunk 1750: 175,000,000 rows | 16,216 citations | 2563006 rows/sec\n",
      "  Chunk 1800: 180,000,000 rows | 16,380 citations | 2564134 rows/sec\n",
      "  Chunk 1850: 185,000,000 rows | 16,679 citations | 2564782 rows/sec\n",
      "  Chunk 1900: 190,000,000 rows | 17,076 citations | 2564898 rows/sec\n",
      "  Chunk 1950: 195,000,000 rows | 17,444 citations | 2565613 rows/sec\n",
      "  Chunk 2000: 200,000,000 rows | 17,689 citations | 2566337 rows/sec\n",
      "  Chunk 2050: 205,000,000 rows | 17,931 citations | 2566546 rows/sec\n",
      "  Chunk 2100: 210,000,000 rows | 18,100 citations | 2567979 rows/sec\n",
      "  Chunk 2150: 215,000,000 rows | 18,269 citations | 2568743 rows/sec\n",
      "  Chunk 2200: 220,000,000 rows | 18,510 citations | 2568997 rows/sec\n",
      "  Chunk 2250: 225,000,000 rows | 18,814 citations | 2567219 rows/sec\n",
      "  Chunk 2300: 230,000,000 rows | 19,342 citations | 2567121 rows/sec\n",
      "  Chunk 2350: 235,000,000 rows | 19,610 citations | 2567911 rows/sec\n",
      "  Chunk 2400: 240,000,000 rows | 19,925 citations | 2567695 rows/sec\n",
      "  Chunk 2450: 245,000,000 rows | 20,398 citations | 2567888 rows/sec\n",
      "  Chunk 2500: 250,000,000 rows | 20,629 citations | 2568664 rows/sec\n",
      "  Chunk 2550: 255,000,000 rows | 21,070 citations | 2568578 rows/sec\n",
      "  Chunk 2600: 260,000,000 rows | 21,582 citations | 2568775 rows/sec\n",
      "  Chunk 2650: 265,000,000 rows | 21,750 citations | 2569121 rows/sec\n",
      "  Chunk 2700: 270,000,000 rows | 21,906 citations | 2569910 rows/sec\n",
      "  Chunk 2750: 275,000,000 rows | 22,088 citations | 2570436 rows/sec\n",
      "  Chunk 2800: 280,000,000 rows | 22,225 citations | 2571111 rows/sec\n",
      "  Chunk 2850: 285,000,000 rows | 22,445 citations | 2571422 rows/sec\n",
      "  Chunk 2900: 290,000,000 rows | 22,569 citations | 2571889 rows/sec\n",
      "  Chunk 2950: 295,000,000 rows | 22,923 citations | 2571852 rows/sec\n",
      "  Chunk 3000: 300,000,000 rows | 23,405 citations | 2573865 rows/sec\n",
      "  Chunk 3050: 305,000,000 rows | 23,837 citations | 2575459 rows/sec\n",
      "  Chunk 3100: 310,000,000 rows | 24,105 citations | 2576298 rows/sec\n",
      "  Chunk 3150: 315,000,000 rows | 24,406 citations | 2578801 rows/sec\n",
      "  Chunk 3200: 320,000,000 rows | 24,676 citations | 2581100 rows/sec\n",
      "  Chunk 3250: 325,000,000 rows | 24,969 citations | 2583337 rows/sec\n",
      "  Chunk 3300: 330,000,000 rows | 25,352 citations | 2584048 rows/sec\n",
      "  Chunk 3350: 335,000,000 rows | 25,573 citations | 2584914 rows/sec\n",
      "  Chunk 3400: 340,000,000 rows | 25,927 citations | 2586966 rows/sec\n",
      "  Chunk 3450: 345,000,000 rows | 26,292 citations | 2588671 rows/sec\n",
      "  Chunk 3500: 350,000,000 rows | 26,563 citations | 2590067 rows/sec\n",
      "  Chunk 3550: 355,000,000 rows | 26,677 citations | 2591243 rows/sec\n",
      "  Chunk 3600: 360,000,000 rows | 27,159 citations | 2591913 rows/sec\n",
      "  Chunk 3650: 365,000,000 rows | 27,309 citations | 2593706 rows/sec\n",
      "  Chunk 3700: 370,000,000 rows | 27,569 citations | 2595189 rows/sec\n",
      "  Chunk 3750: 375,000,000 rows | 27,893 citations | 2596286 rows/sec\n",
      "  Chunk 3800: 380,000,000 rows | 28,185 citations | 2596628 rows/sec\n",
      "  Chunk 3850: 385,000,000 rows | 28,401 citations | 2597664 rows/sec\n",
      "  Chunk 3900: 390,000,000 rows | 28,653 citations | 2598214 rows/sec\n",
      "  Chunk 3950: 395,000,000 rows | 29,057 citations | 2599258 rows/sec\n",
      "  Chunk 4000: 400,000,000 rows | 29,181 citations | 2600459 rows/sec\n",
      "  Chunk 4050: 405,000,000 rows | 29,534 citations | 2601883 rows/sec\n",
      "  Chunk 4100: 410,000,000 rows | 29,761 citations | 2603335 rows/sec\n",
      "  Chunk 4150: 415,000,000 rows | 30,172 citations | 2604222 rows/sec\n",
      "  Chunk 4200: 420,000,000 rows | 30,550 citations | 2605659 rows/sec\n",
      "  Chunk 4250: 425,000,000 rows | 30,852 citations | 2606774 rows/sec\n",
      "  Chunk 4300: 430,000,000 rows | 31,111 citations | 2607838 rows/sec\n",
      "  Chunk 4350: 435,000,000 rows | 31,687 citations | 2608111 rows/sec\n",
      "  Chunk 4400: 440,000,000 rows | 32,204 citations | 2608600 rows/sec\n",
      "  Chunk 4450: 445,000,000 rows | 32,476 citations | 2609426 rows/sec\n",
      "  Chunk 4500: 450,000,000 rows | 32,649 citations | 2610451 rows/sec\n",
      "  Chunk 4550: 455,000,000 rows | 32,838 citations | 2612228 rows/sec\n",
      "  Chunk 4600: 460,000,000 rows | 33,129 citations | 2613522 rows/sec\n",
      "  Chunk 4650: 465,000,000 rows | 33,369 citations | 2614705 rows/sec\n",
      "  Chunk 4700: 470,000,000 rows | 33,828 citations | 2615133 rows/sec\n",
      "  Chunk 4750: 475,000,000 rows | 34,340 citations | 2615452 rows/sec\n",
      "  Chunk 4800: 480,000,000 rows | 34,566 citations | 2616452 rows/sec\n",
      "  Chunk 4850: 485,000,000 rows | 34,790 citations | 2617372 rows/sec\n",
      "  Chunk 4900: 490,000,000 rows | 35,035 citations | 2618083 rows/sec\n",
      "  Chunk 4950: 495,000,000 rows | 35,522 citations | 2618582 rows/sec\n",
      "  Chunk 5000: 500,000,000 rows | 35,780 citations | 2619333 rows/sec\n",
      "  Chunk 5050: 505,000,000 rows | 36,084 citations | 2620272 rows/sec\n",
      "  Chunk 5100: 510,000,000 rows | 36,329 citations | 2621574 rows/sec\n",
      "  Chunk 5150: 515,000,000 rows | 36,690 citations | 2621967 rows/sec\n",
      "  Chunk 5200: 520,000,000 rows | 36,930 citations | 2622414 rows/sec\n",
      "  Chunk 5250: 525,000,000 rows | 37,286 citations | 2622732 rows/sec\n",
      "  Chunk 5300: 530,000,000 rows | 37,593 citations | 2623619 rows/sec\n",
      "  Chunk 5350: 535,000,000 rows | 37,886 citations | 2624206 rows/sec\n",
      "  Chunk 5400: 540,000,000 rows | 38,135 citations | 2624481 rows/sec\n",
      "  Chunk 5450: 545,000,000 rows | 38,472 citations | 2624865 rows/sec\n",
      "  Chunk 5500: 550,000,000 rows | 38,841 citations | 2625518 rows/sec\n",
      "  Chunk 5550: 555,000,000 rows | 39,028 citations | 2625922 rows/sec\n",
      "  Chunk 5600: 560,000,000 rows | 39,386 citations | 2625988 rows/sec\n",
      "  Chunk 5650: 565,000,000 rows | 39,757 citations | 2626913 rows/sec\n",
      "  Chunk 5700: 570,000,000 rows | 40,105 citations | 2627097 rows/sec\n",
      "  Chunk 5750: 575,000,000 rows | 40,326 citations | 2627411 rows/sec\n",
      "  Chunk 5800: 580,000,000 rows | 40,580 citations | 2628002 rows/sec\n",
      "  Chunk 5850: 585,000,000 rows | 40,962 citations | 2628325 rows/sec\n",
      "  Chunk 5900: 590,000,000 rows | 41,517 citations | 2628670 rows/sec\n",
      "  Chunk 5950: 595,000,000 rows | 41,804 citations | 2629284 rows/sec\n",
      "  Chunk 6000: 600,000,000 rows | 42,077 citations | 2629741 rows/sec\n",
      "  Chunk 6050: 605,000,000 rows | 42,231 citations | 2630576 rows/sec\n",
      "  Chunk 6100: 610,000,000 rows | 42,585 citations | 2631598 rows/sec\n",
      "  Chunk 6150: 615,000,000 rows | 42,833 citations | 2632065 rows/sec\n",
      "  Chunk 6200: 620,000,000 rows | 43,159 citations | 2632625 rows/sec\n",
      "  Chunk 6250: 625,000,000 rows | 43,612 citations | 2633419 rows/sec\n",
      "  Chunk 6300: 630,000,000 rows | 43,809 citations | 2633603 rows/sec\n",
      "  Chunk 6350: 635,000,000 rows | 44,138 citations | 2633713 rows/sec\n",
      "  Chunk 6400: 640,000,000 rows | 44,429 citations | 2634167 rows/sec\n",
      "  Chunk 6450: 645,000,000 rows | 44,674 citations | 2634732 rows/sec\n",
      "  Chunk 6500: 650,000,000 rows | 44,938 citations | 2635173 rows/sec\n",
      "  Chunk 6550: 655,000,000 rows | 45,338 citations | 2635725 rows/sec\n",
      "  Chunk 6600: 660,000,000 rows | 45,456 citations | 2636503 rows/sec\n",
      "  Chunk 6650: 665,000,000 rows | 45,941 citations | 2636876 rows/sec\n",
      "  Chunk 6700: 670,000,000 rows | 46,128 citations | 2637342 rows/sec\n",
      "  Chunk 6750: 675,000,000 rows | 46,528 citations | 2637596 rows/sec\n",
      "  Chunk 6800: 680,000,000 rows | 46,874 citations | 2638145 rows/sec\n",
      "  Chunk 6850: 685,000,000 rows | 47,255 citations | 2638593 rows/sec\n",
      "  Chunk 6900: 690,000,000 rows | 47,384 citations | 2639307 rows/sec\n",
      "  Chunk 6950: 695,000,000 rows | 47,665 citations | 2639600 rows/sec\n",
      "  Chunk 7000: 700,000,000 rows | 48,115 citations | 2639618 rows/sec\n",
      "  Chunk 7050: 705,000,000 rows | 48,509 citations | 2639946 rows/sec\n",
      "  Chunk 7100: 710,000,000 rows | 49,053 citations | 2639744 rows/sec\n",
      "  Chunk 7150: 715,000,000 rows | 49,237 citations | 2639895 rows/sec\n",
      "  Chunk 7200: 720,000,000 rows | 49,455 citations | 2640174 rows/sec\n",
      "  Chunk 7250: 725,000,000 rows | 49,617 citations | 2640559 rows/sec\n",
      "  Chunk 7300: 730,000,000 rows | 50,069 citations | 2641013 rows/sec\n",
      "  Chunk 7350: 735,000,000 rows | 50,555 citations | 2641378 rows/sec\n",
      "  Chunk 7400: 740,000,000 rows | 51,010 citations | 2641894 rows/sec\n",
      "  Chunk 7450: 745,000,000 rows | 51,417 citations | 2642071 rows/sec\n",
      "  Chunk 7500: 750,000,000 rows | 51,602 citations | 2642242 rows/sec\n",
      "  Chunk 7550: 755,000,000 rows | 51,918 citations | 2642774 rows/sec\n",
      "  Chunk 7600: 760,000,000 rows | 52,251 citations | 2642889 rows/sec\n",
      "  Chunk 7650: 765,000,000 rows | 52,383 citations | 2643203 rows/sec\n",
      "  Chunk 7700: 770,000,000 rows | 52,534 citations | 2643828 rows/sec\n",
      "  Chunk 7750: 775,000,000 rows | 52,904 citations | 2644166 rows/sec\n",
      "  Chunk 7800: 780,000,000 rows | 53,246 citations | 2644364 rows/sec\n",
      "  Chunk 7850: 785,000,000 rows | 53,378 citations | 2644715 rows/sec\n",
      "  Chunk 7900: 790,000,000 rows | 53,845 citations | 2644615 rows/sec\n",
      "  Chunk 7950: 795,000,000 rows | 54,118 citations | 2644470 rows/sec\n",
      "  Chunk 8000: 800,000,000 rows | 54,414 citations | 2644871 rows/sec\n",
      "  Chunk 8050: 805,000,000 rows | 55,006 citations | 2645124 rows/sec\n",
      "  Chunk 8100: 810,000,000 rows | 55,135 citations | 2645541 rows/sec\n",
      "  Chunk 8150: 815,000,000 rows | 55,305 citations | 2646041 rows/sec\n",
      "  Chunk 8200: 820,000,000 rows | 55,510 citations | 2646187 rows/sec\n",
      "  Chunk 8250: 825,000,000 rows | 55,805 citations | 2646543 rows/sec\n",
      "  Chunk 8300: 830,000,000 rows | 56,295 citations | 2646584 rows/sec\n",
      "  Chunk 8350: 835,000,000 rows | 56,527 citations | 2646911 rows/sec\n",
      "  Chunk 8400: 840,000,000 rows | 56,959 citations | 2647464 rows/sec\n",
      "  Chunk 8450: 845,000,000 rows | 57,142 citations | 2647655 rows/sec\n",
      "  Chunk 8500: 850,000,000 rows | 57,275 citations | 2647618 rows/sec\n",
      "  Chunk 8550: 855,000,000 rows | 57,749 citations | 2647937 rows/sec\n",
      "  Chunk 8600: 860,000,000 rows | 58,253 citations | 2647648 rows/sec\n",
      "  Chunk 8650: 865,000,000 rows | 59,310 citations | 2646693 rows/sec\n",
      "  Chunk 8700: 870,000,000 rows | 60,149 citations | 2645789 rows/sec\n",
      "  Chunk 8750: 875,000,000 rows | 61,354 citations | 2644641 rows/sec\n",
      "  Chunk 8800: 880,000,000 rows | 62,401 citations | 2643702 rows/sec\n",
      "  Chunk 8850: 885,000,000 rows | 63,612 citations | 2642755 rows/sec\n",
      "  Chunk 8900: 890,000,000 rows | 64,955 citations | 2641871 rows/sec\n",
      "  Chunk 8950: 895,000,000 rows | 66,137 citations | 2640874 rows/sec\n",
      "  Chunk 9000: 900,000,000 rows | 67,589 citations | 2639936 rows/sec\n",
      "  Chunk 9050: 905,000,000 rows | 69,562 citations | 2639008 rows/sec\n",
      "  Chunk 9100: 910,000,000 rows | 70,644 citations | 2638278 rows/sec\n",
      "  Chunk 9150: 915,000,000 rows | 71,747 citations | 2637614 rows/sec\n",
      "  Chunk 9200: 920,000,000 rows | 72,380 citations | 2636895 rows/sec\n",
      "  Chunk 9250: 925,000,000 rows | 73,106 citations | 2636272 rows/sec\n",
      "  Chunk 9300: 930,000,000 rows | 74,014 citations | 2635735 rows/sec\n",
      "  Chunk 9350: 935,000,000 rows | 74,872 citations | 2635074 rows/sec\n",
      "  Chunk 9400: 940,000,000 rows | 75,983 citations | 2634078 rows/sec\n",
      "  Chunk 9450: 945,000,000 rows | 77,705 citations | 2633325 rows/sec\n",
      "  Chunk 9500: 950,000,000 rows | 78,969 citations | 2632539 rows/sec\n",
      "  Chunk 9550: 955,000,000 rows | 80,358 citations | 2631858 rows/sec\n",
      "  Chunk 9600: 960,000,000 rows | 81,202 citations | 2631180 rows/sec\n",
      "  Chunk 9650: 965,000,000 rows | 82,379 citations | 2630472 rows/sec\n",
      "  Chunk 9700: 970,000,000 rows | 83,625 citations | 2629694 rows/sec\n",
      "  Chunk 9750: 975,000,000 rows | 84,631 citations | 2629022 rows/sec\n",
      "  Chunk 9800: 980,000,000 rows | 85,271 citations | 2628339 rows/sec\n",
      "  Chunk 9850: 985,000,000 rows | 85,920 citations | 2627674 rows/sec\n",
      "  Chunk 9900: 990,000,000 rows | 87,053 citations | 2627009 rows/sec\n",
      "  Chunk 9950: 995,000,000 rows | 88,474 citations | 2626331 rows/sec\n",
      "  Chunk 10000: 1,000,000,000 rows | 90,593 citations | 2625595 rows/sec\n",
      "  Chunk 10050: 1,005,000,000 rows | 92,214 citations | 2624874 rows/sec\n",
      "  Chunk 10100: 1,010,000,000 rows | 94,140 citations | 2624102 rows/sec\n",
      "  Chunk 10150: 1,015,000,000 rows | 95,779 citations | 2623310 rows/sec\n",
      "  Chunk 10200: 1,020,000,000 rows | 97,566 citations | 2622528 rows/sec\n",
      "  Chunk 10250: 1,025,000,000 rows | 99,644 citations | 2621964 rows/sec\n",
      "  Chunk 10300: 1,030,000,000 rows | 101,458 citations | 2621328 rows/sec\n",
      "  Chunk 10350: 1,035,000,000 rows | 103,407 citations | 2620681 rows/sec\n",
      "  Chunk 10400: 1,040,000,000 rows | 105,339 citations | 2619849 rows/sec\n",
      "  Chunk 10450: 1,045,000,000 rows | 107,556 citations | 2619261 rows/sec\n",
      "  Chunk 10500: 1,050,000,000 rows | 109,191 citations | 2618708 rows/sec\n",
      "  Chunk 10550: 1,055,000,000 rows | 111,018 citations | 2617986 rows/sec\n",
      "  Chunk 10600: 1,060,000,000 rows | 113,641 citations | 2617372 rows/sec\n",
      "  Chunk 10650: 1,065,000,000 rows | 115,673 citations | 2616586 rows/sec\n",
      "  Chunk 10700: 1,070,000,000 rows | 117,387 citations | 2616049 rows/sec\n",
      "  Chunk 10750: 1,075,000,000 rows | 119,155 citations | 2615491 rows/sec\n",
      "  Chunk 10800: 1,080,000,000 rows | 120,968 citations | 2614736 rows/sec\n",
      "  Chunk 10850: 1,085,000,000 rows | 122,816 citations | 2614202 rows/sec\n",
      "  Chunk 10900: 1,090,000,000 rows | 124,757 citations | 2613642 rows/sec\n",
      "  Chunk 10950: 1,095,000,000 rows | 127,425 citations | 2613077 rows/sec\n",
      "  Chunk 11000: 1,100,000,000 rows | 130,113 citations | 2612356 rows/sec\n",
      "  Chunk 11050: 1,105,000,000 rows | 132,652 citations | 2611682 rows/sec\n",
      "  Chunk 11100: 1,110,000,000 rows | 135,312 citations | 2611151 rows/sec\n",
      "  Chunk 11150: 1,115,000,000 rows | 138,103 citations | 2610576 rows/sec\n",
      "  Chunk 11200: 1,120,000,000 rows | 141,314 citations | 2609860 rows/sec\n",
      "  Chunk 11250: 1,125,000,000 rows | 143,871 citations | 2609240 rows/sec\n",
      "  Chunk 11300: 1,130,000,000 rows | 145,993 citations | 2608598 rows/sec\n",
      "  Chunk 11350: 1,135,000,000 rows | 147,697 citations | 2608011 rows/sec\n",
      "  Chunk 11400: 1,140,000,000 rows | 150,123 citations | 2607387 rows/sec\n",
      "  Chunk 11450: 1,145,000,000 rows | 152,737 citations | 2606709 rows/sec\n",
      "  Chunk 11500: 1,150,000,000 rows | 154,516 citations | 2606135 rows/sec\n",
      "  Chunk 11550: 1,155,000,000 rows | 156,722 citations | 2605493 rows/sec\n",
      "  Chunk 11600: 1,160,000,000 rows | 159,369 citations | 2605037 rows/sec\n",
      "  Chunk 11650: 1,165,000,000 rows | 161,874 citations | 2604470 rows/sec\n",
      "  Chunk 11700: 1,170,000,000 rows | 165,141 citations | 2603640 rows/sec\n",
      "  Chunk 11750: 1,175,000,000 rows | 167,665 citations | 2603174 rows/sec\n",
      "  Chunk 11800: 1,180,000,000 rows | 169,961 citations | 2602746 rows/sec\n",
      "  Chunk 11850: 1,185,000,000 rows | 172,384 citations | 2602248 rows/sec\n",
      "  Chunk 11900: 1,190,000,000 rows | 175,087 citations | 2601693 rows/sec\n",
      "  Chunk 11950: 1,195,000,000 rows | 177,433 citations | 2601269 rows/sec\n",
      "  Chunk 12000: 1,200,000,000 rows | 180,136 citations | 2600704 rows/sec\n",
      "  Chunk 12050: 1,205,000,000 rows | 183,385 citations | 2600298 rows/sec\n",
      "  Chunk 12100: 1,210,000,000 rows | 186,554 citations | 2599891 rows/sec\n",
      "  Chunk 12150: 1,215,000,000 rows | 189,053 citations | 2599334 rows/sec\n",
      "  Chunk 12200: 1,220,000,000 rows | 191,441 citations | 2598954 rows/sec\n",
      "  Chunk 12250: 1,225,000,000 rows | 194,496 citations | 2598511 rows/sec\n",
      "  Chunk 12300: 1,230,000,000 rows | 196,903 citations | 2598141 rows/sec\n",
      "  Chunk 12350: 1,235,000,000 rows | 199,174 citations | 2597788 rows/sec\n",
      "  Chunk 12400: 1,240,000,000 rows | 201,565 citations | 2597426 rows/sec\n",
      "  Chunk 12450: 1,245,000,000 rows | 204,210 citations | 2597059 rows/sec\n",
      "  Chunk 12500: 1,250,000,000 rows | 207,080 citations | 2596631 rows/sec\n",
      "  Chunk 12550: 1,255,000,000 rows | 209,813 citations | 2596072 rows/sec\n",
      "  Chunk 12600: 1,260,000,000 rows | 212,477 citations | 2595666 rows/sec\n",
      "  Chunk 12650: 1,265,000,000 rows | 220,157 citations | 2595049 rows/sec\n",
      "  Chunk 12700: 1,270,000,000 rows | 224,542 citations | 2594657 rows/sec\n",
      "  Chunk 12750: 1,275,000,000 rows | 227,603 citations | 2594250 rows/sec\n",
      "  Chunk 12800: 1,280,000,000 rows | 230,435 citations | 2593823 rows/sec\n",
      "  Chunk 12850: 1,285,000,000 rows | 233,353 citations | 2593335 rows/sec\n",
      "  Chunk 12900: 1,290,000,000 rows | 236,343 citations | 2593005 rows/sec\n",
      "  Chunk 12950: 1,295,000,000 rows | 239,458 citations | 2592596 rows/sec\n",
      "  Chunk 13000: 1,300,000,000 rows | 242,719 citations | 2592262 rows/sec\n",
      "  Chunk 13050: 1,305,000,000 rows | 245,021 citations | 2591867 rows/sec\n",
      "  Chunk 13100: 1,310,000,000 rows | 249,148 citations | 2591408 rows/sec\n",
      "  Chunk 13150: 1,315,000,000 rows | 252,125 citations | 2590969 rows/sec\n",
      "  Chunk 13200: 1,320,000,000 rows | 255,294 citations | 2590519 rows/sec\n",
      "  Chunk 13250: 1,325,000,000 rows | 257,813 citations | 2590019 rows/sec\n",
      "  Chunk 13300: 1,330,000,000 rows | 261,358 citations | 2589598 rows/sec\n",
      "  Chunk 13350: 1,335,000,000 rows | 264,734 citations | 2589114 rows/sec\n",
      "  Chunk 13400: 1,340,000,000 rows | 267,916 citations | 2588776 rows/sec\n",
      "  Chunk 13450: 1,345,000,000 rows | 270,839 citations | 2588432 rows/sec\n",
      "  Chunk 13500: 1,350,000,000 rows | 272,775 citations | 2588125 rows/sec\n",
      "  Chunk 13550: 1,355,000,000 rows | 275,786 citations | 2587707 rows/sec\n",
      "  Chunk 13600: 1,360,000,000 rows | 278,527 citations | 2587407 rows/sec\n",
      "  Chunk 13650: 1,365,000,000 rows | 281,504 citations | 2587123 rows/sec\n",
      "  Chunk 13700: 1,370,000,000 rows | 284,083 citations | 2586827 rows/sec\n",
      "  Chunk 13750: 1,375,000,000 rows | 286,513 citations | 2586467 rows/sec\n",
      "  Chunk 13800: 1,380,000,000 rows | 289,300 citations | 2585992 rows/sec\n",
      "  Chunk 13850: 1,385,000,000 rows | 291,837 citations | 2585757 rows/sec\n",
      "  Chunk 13900: 1,390,000,000 rows | 294,499 citations | 2585472 rows/sec\n",
      "  Chunk 13950: 1,395,000,000 rows | 296,782 citations | 2585119 rows/sec\n",
      "  Chunk 14000: 1,400,000,000 rows | 300,122 citations | 2584733 rows/sec\n",
      "  Chunk 14050: 1,405,000,000 rows | 303,961 citations | 2448911 rows/sec\n",
      "  Chunk 14100: 1,410,000,000 rows | 306,921 citations | 2449113 rows/sec\n",
      "  Chunk 14150: 1,415,000,000 rows | 309,930 citations | 2449118 rows/sec\n",
      "  Chunk 14200: 1,420,000,000 rows | 311,992 citations | 2449343 rows/sec\n",
      "  Chunk 14250: 1,425,000,000 rows | 313,939 citations | 2449559 rows/sec\n",
      "  Chunk 14300: 1,430,000,000 rows | 321,172 citations | 2328859 rows/sec\n",
      "  Chunk 14350: 1,435,000,000 rows | 324,885 citations | 2329267 rows/sec\n",
      "  Chunk 14400: 1,440,000,000 rows | 328,411 citations | 2329770 rows/sec\n",
      "  Chunk 14450: 1,445,000,000 rows | 331,520 citations | 2330212 rows/sec\n",
      "  Chunk 14500: 1,450,000,000 rows | 333,513 citations | 2330701 rows/sec\n",
      "  Chunk 14550: 1,455,000,000 rows | 336,042 citations | 2331211 rows/sec\n",
      "  Chunk 14600: 1,460,000,000 rows | 340,198 citations | 2331670 rows/sec\n",
      "  Chunk 14650: 1,465,000,000 rows | 343,858 citations | 2332068 rows/sec\n",
      "  Chunk 14700: 1,470,000,000 rows | 346,851 citations | 2332564 rows/sec\n",
      "  Chunk 14750: 1,475,000,000 rows | 348,998 citations | 2333095 rows/sec\n",
      "  Chunk 14800: 1,480,000,000 rows | 351,546 citations | 2227530 rows/sec\n",
      "  Chunk 14850: 1,485,000,000 rows | 355,249 citations | 2228294 rows/sec\n",
      "  Chunk 14900: 1,490,000,000 rows | 357,915 citations | 2229032 rows/sec\n",
      "  Chunk 14950: 1,495,000,000 rows | 361,202 citations | 2229826 rows/sec\n",
      "  Chunk 15000: 1,500,000,000 rows | 364,113 citations | 2230546 rows/sec\n",
      "  Chunk 15050: 1,505,000,000 rows | 367,042 citations | 2231420 rows/sec\n",
      "  Chunk 15100: 1,510,000,000 rows | 370,123 citations | 2232255 rows/sec\n",
      "  Chunk 15150: 1,515,000,000 rows | 373,317 citations | 2233028 rows/sec\n",
      "  Chunk 15200: 1,520,000,000 rows | 376,642 citations | 2233820 rows/sec\n",
      "  Chunk 15250: 1,525,000,000 rows | 380,226 citations | 2234661 rows/sec\n",
      "  Chunk 15300: 1,530,000,000 rows | 383,880 citations | 2235453 rows/sec\n",
      "  Chunk 15350: 1,535,000,000 rows | 386,614 citations | 2236216 rows/sec\n",
      "  Chunk 15400: 1,540,000,000 rows | 389,651 citations | 2237042 rows/sec\n",
      "  Chunk 15450: 1,545,000,000 rows | 392,588 citations | 2237778 rows/sec\n",
      "  Chunk 15500: 1,550,000,000 rows | 395,867 citations | 2238541 rows/sec\n",
      "  Chunk 15550: 1,555,000,000 rows | 399,179 citations | 2239365 rows/sec\n",
      "  Chunk 15600: 1,560,000,000 rows | 402,386 citations | 2240168 rows/sec\n",
      "  Chunk 15650: 1,565,000,000 rows | 405,772 citations | 2240973 rows/sec\n",
      "  Chunk 15700: 1,570,000,000 rows | 406,440 citations | 2241498 rows/sec\n",
      "  Chunk 15750: 1,575,000,000 rows | 406,993 citations | 2241964 rows/sec\n",
      "  Chunk 15800: 1,580,000,000 rows | 408,445 citations | 2242576 rows/sec\n",
      "  Chunk 15850: 1,585,000,000 rows | 416,890 citations | 2242418 rows/sec\n",
      "\n",
      "Step 4 Complete\n",
      "  Time: 708.5 seconds\n",
      "  Citation links: 424,616\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Step 4: Building Citation Network ===\\n\")\n",
    "print(\"Processing PaperReferences.tsv (~32.41 GB)...\\n\")\n",
    "\n",
    "file_path = os.path.join(DATA_DIR, 'SciSciNet_PaperReferences.tsv')\n",
    "\n",
    "chunk_count = 0\n",
    "total_rows = 0\n",
    "filtered_rows = 0\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Peek at columns\n",
    "sample = pd.read_csv(file_path, sep='\\t', nrows=1)\n",
    "print(f\"Available columns: {sample.columns.tolist()}\\n\")\n",
    "\n",
    "pr_paper_col = None\n",
    "pr_ref_col = None\n",
    "\n",
    "# Try to identify the paper column (citing paper)\n",
    "for col in sample.columns:\n",
    "    col_lower = col.lower().replace(' ', '').replace('_', '')\n",
    "    # Look for main paper ID column (the one doing the citing)\n",
    "    if col_lower in ['paperid', 'citingpaperid']:\n",
    "        pr_paper_col = col\n",
    "        break\n",
    "    if 'paper' in col_lower and 'id' in col_lower and 'cited' not in col_lower and 'reference' not in col_lower:\n",
    "        pr_paper_col = col\n",
    "\n",
    "# Try to identify the reference column (cited paper)\n",
    "for col in sample.columns:\n",
    "    col_lower = col.lower().replace(' ', '').replace('_', '')\n",
    "    # Look for cited/reference paper ID column\n",
    "    if col_lower in ['referenceid', 'citedpaperid', 'referencepaperid']:\n",
    "        pr_ref_col = col\n",
    "        break\n",
    "    if ('cited' in col_lower or 'reference' in col_lower) and 'id' in col_lower:\n",
    "        pr_ref_col = col\n",
    "\n",
    "print(f\"Detected columns:\")\n",
    "print(f\"  Paper (citing): '{pr_paper_col}'\")\n",
    "print(f\"  Reference (cited): '{pr_ref_col}'\\n\")\n",
    "\n",
    "# Validate that both columns were found\n",
    "if pr_paper_col is None or pr_ref_col is None:\n",
    "    raise ValueError(\n",
    "        f\"Could not identify paper or reference columns!\\n\"\n",
    "        f\"Available columns: {sample.columns.tolist()}\\n\"\n",
    "        f\"Please manually specify the correct column names.\"\n",
    "    )\n",
    "\n",
    "# Process the file in chunks\n",
    "for chunk in pd.read_csv(file_path, sep='\\t', chunksize=CHUNK_SIZE):\n",
    "    chunk_count += 1\n",
    "    total_rows += len(chunk)\n",
    "    \n",
    "    # Filter for citations involving our CS papers\n",
    "    # Include citations where either the citing paper OR cited paper is in our set\n",
    "    filtered_chunk = chunk[\n",
    "        (chunk[pr_paper_col].isin(final_cs_papers_set)) |\n",
    "        (chunk[pr_ref_col].isin(final_cs_papers_set))\n",
    "    ]\n",
    "    filtered_rows += len(filtered_chunk)\n",
    "    \n",
    "    if len(filtered_chunk) > 0:\n",
    "        filtered_chunk_clean = filtered_chunk.rename(columns={\n",
    "            pr_paper_col: 'paper_id',\n",
    "            pr_ref_col: 'reference_id'\n",
    "        })[['paper_id', 'reference_id']]\n",
    "        \n",
    "        filtered_chunk_clean.to_sql('paper_references', conn, if_exists='append', index=False)\n",
    "    \n",
    "    if chunk_count % 50 == 0:\n",
    "        elapsed = (datetime.now() - start_time).total_seconds()\n",
    "        rate = total_rows / elapsed if elapsed > 0 else 0\n",
    "        print(f\"  Chunk {chunk_count}: {total_rows:,} rows | {filtered_rows:,} citations | {rate:.0f} rows/sec\")\n",
    "\n",
    "elapsed_time = (datetime.now() - start_time).total_seconds()\n",
    "print(f\"\\nStep 4 Complete\")\n",
    "print(f\"  Time: {elapsed_time:.1f} seconds\")\n",
    "print(f\"  Citation links: {filtered_rows:,}\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6797667b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 5: Adding Paper Details ===\n",
      "\n",
      "Processing PaperDetails.tsv (~28.72 GB)...\n",
      "\n",
      "Available columns: ['PaperID', 'DOI', 'DocType', 'PaperTitle', 'BookTitle', 'Year', 'Date', 'Publisher', 'JournalID', 'ConferenceSeriesID', 'Volume', 'Issue', 'FirstPage', 'LastPage', 'ReferenceCount', 'CitationCount', 'OriginalVenue', 'FamilyID', 'RetractionType']\n",
      "\n",
      "Extracting columns: ['PaperID']\n",
      "\n",
      "  Chunk 30: 3,000,000 rows | 220 details | 401912 rows/sec\n",
      "  Chunk 60: 6,000,000 rows | 468 details | 401643 rows/sec\n",
      "  Chunk 90: 9,000,000 rows | 701 details | 401145 rows/sec\n",
      "  Chunk 120: 12,000,000 rows | 908 details | 401309 rows/sec\n",
      "  Chunk 150: 15,000,000 rows | 1,133 details | 401430 rows/sec\n",
      "  Chunk 180: 18,000,000 rows | 1,350 details | 401276 rows/sec\n",
      "  Chunk 210: 21,000,000 rows | 1,572 details | 254383 rows/sec\n",
      "  Chunk 240: 24,000,000 rows | 1,795 details | 266589 rows/sec\n",
      "  Chunk 270: 27,000,000 rows | 2,023 details | 276889 rows/sec\n",
      "  Chunk 300: 30,000,000 rows | 2,261 details | 285688 rows/sec\n",
      "  Chunk 330: 33,000,000 rows | 2,518 details | 293247 rows/sec\n",
      "  Chunk 360: 36,000,000 rows | 2,724 details | 299934 rows/sec\n",
      "  Chunk 390: 39,000,000 rows | 2,963 details | 305850 rows/sec\n",
      "  Chunk 420: 42,000,000 rows | 3,178 details | 311111 rows/sec\n",
      "  Chunk 450: 45,000,000 rows | 3,401 details | 315799 rows/sec\n",
      "  Chunk 480: 48,000,000 rows | 3,621 details | 319985 rows/sec\n",
      "  Chunk 510: 51,000,000 rows | 3,881 details | 323859 rows/sec\n",
      "  Chunk 540: 54,000,000 rows | 4,116 details | 327380 rows/sec\n",
      "  Chunk 570: 57,000,000 rows | 4,339 details | 330600 rows/sec\n",
      "  Chunk 600: 60,000,000 rows | 4,553 details | 333552 rows/sec\n",
      "  Chunk 630: 63,000,000 rows | 4,774 details | 336224 rows/sec\n",
      "  Chunk 660: 66,000,000 rows | 5,002 details | 338709 rows/sec\n",
      "  Chunk 690: 69,000,000 rows | 5,238 details | 341017 rows/sec\n",
      "  Chunk 720: 72,000,000 rows | 5,479 details | 343163 rows/sec\n",
      "  Chunk 750: 75,000,000 rows | 5,715 details | 345161 rows/sec\n",
      "  Chunk 780: 78,000,000 rows | 5,938 details | 346999 rows/sec\n",
      "  Chunk 810: 81,000,000 rows | 6,166 details | 348729 rows/sec\n",
      "  Chunk 840: 84,000,000 rows | 6,372 details | 350339 rows/sec\n",
      "  Chunk 870: 87,000,000 rows | 6,606 details | 351856 rows/sec\n",
      "  Chunk 900: 90,000,000 rows | 6,813 details | 353228 rows/sec\n",
      "  Chunk 930: 93,000,000 rows | 7,051 details | 354564 rows/sec\n",
      "  Chunk 960: 96,000,000 rows | 7,267 details | 355869 rows/sec\n",
      "  Chunk 990: 99,000,000 rows | 7,513 details | 357064 rows/sec\n",
      "  Chunk 1020: 102,000,000 rows | 7,724 details | 358233 rows/sec\n",
      "  Chunk 1050: 105,000,000 rows | 7,947 details | 359281 rows/sec\n",
      "  Chunk 1080: 108,000,000 rows | 8,186 details | 360284 rows/sec\n",
      "  Chunk 1110: 111,000,000 rows | 8,398 details | 361295 rows/sec\n",
      "  Chunk 1140: 114,000,000 rows | 8,624 details | 362218 rows/sec\n",
      "  Chunk 1170: 117,000,000 rows | 8,838 details | 363120 rows/sec\n",
      "  Chunk 1200: 120,000,000 rows | 9,064 details | 363970 rows/sec\n",
      "  Chunk 1230: 123,000,000 rows | 9,280 details | 364765 rows/sec\n",
      "  Chunk 1260: 126,000,000 rows | 9,501 details | 365554 rows/sec\n",
      "  Chunk 1290: 129,000,000 rows | 9,726 details | 366273 rows/sec\n",
      "  Chunk 1320: 132,000,000 rows | 9,948 details | 367011 rows/sec\n",
      "  Chunk 1350: 135,000,000 rows | 10,181 details | 367679 rows/sec\n",
      "\n",
      "Step 5 Complete\n",
      "  Time: 371.5 seconds\n",
      "  Paper details added: 10,293\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Step 5: Adding Paper Details ===\\n\")\n",
    "print(\"Processing PaperDetails.tsv (~28.72 GB)...\\n\")\n",
    "\n",
    "file_path = os.path.join(DATA_DIR, 'SciSciNet_PaperDetails.tsv')\n",
    "\n",
    "chunk_count = 0\n",
    "total_rows = 0\n",
    "filtered_rows = 0\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Peek at columns\n",
    "sample = pd.read_csv(file_path, sep='\\t', nrows=1)\n",
    "available_cols = sample.columns.tolist()\n",
    "print(f\"Available columns: {available_cols}\\n\")\n",
    "\n",
    "usecols = [PAPER_ID_COL]\n",
    "abstract_col = None\n",
    "\n",
    "for col in ['Abstract', 'OriginalAbstract']:\n",
    "    if col in available_cols:\n",
    "        abstract_col = col\n",
    "        usecols.append(col)\n",
    "        break\n",
    "\n",
    "\n",
    "print(f\"Extracting columns: {usecols}\\n\")\n",
    "\n",
    "for chunk in pd.read_csv(file_path, sep='\\t', chunksize=CHUNK_SIZE, usecols=usecols):\n",
    "    chunk_count += 1\n",
    "    total_rows += len(chunk)\n",
    "    \n",
    "    filtered_chunk = chunk[chunk[PAPER_ID_COL].isin(final_cs_papers_set)]\n",
    "    filtered_rows += len(filtered_chunk)\n",
    "    \n",
    "    if len(filtered_chunk) > 0:\n",
    "        cols_map = {PAPER_ID_COL: 'paper_id'}\n",
    "        if abstract_col:\n",
    "            cols_map[abstract_col] = 'abstract'\n",
    "        \n",
    "        filtered_chunk_clean = filtered_chunk.rename(columns=cols_map)\n",
    "        filtered_chunk_clean.to_sql('paper_details', conn, if_exists='append', index=False)\n",
    "    \n",
    "    if chunk_count % 30 == 0:\n",
    "        elapsed = (datetime.now() - start_time).total_seconds()\n",
    "        rate = total_rows / elapsed if elapsed > 0 else 0\n",
    "        print(f\"  Chunk {chunk_count}: {total_rows:,} rows | {filtered_rows:,} details | {rate:.0f} rows/sec\")\n",
    "\n",
    "elapsed_time = (datetime.now() - start_time).total_seconds()\n",
    "print(f\"\\nStep 5 Complete\")\n",
    "print(f\"  Time: {elapsed_time:.1f} seconds\")\n",
    "print(f\"  Paper details added: {filtered_rows:,}\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de46cbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Creating Database Indexes ===\n",
      "\n",
      "[OK] idx_papers_year\n",
      "[OK] idx_papers_id\n",
      "[OK] idx_paa_paper\n",
      "[OK] idx_paa_author\n",
      "[OK] idx_paa_affil\n",
      "[OK] idx_pf_paper\n",
      "[OK] idx_pf_field\n",
      "[OK] idx_pr_paper\n",
      "[OK] idx_pr_ref\n",
      "[OK] idx_pd_paper\n",
      "\n",
      "Indexes created\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Creating Database Indexes ===\\n\")\n",
    "\n",
    "indexes = [\n",
    "    (\"idx_papers_year\", \"CREATE INDEX idx_papers_year ON papers(year)\"),\n",
    "    (\"idx_papers_id\", \"CREATE INDEX idx_papers_id ON papers(paper_id)\"),\n",
    "    (\"idx_paa_paper\", \"CREATE INDEX idx_paa_paper ON paper_author_affiliations(paper_id)\"),\n",
    "    (\"idx_paa_author\", \"CREATE INDEX idx_paa_author ON paper_author_affiliations(author_id)\"),\n",
    "    (\"idx_paa_affil\", \"CREATE INDEX idx_paa_affil ON paper_author_affiliations(affiliation_id)\"),\n",
    "    (\"idx_pf_paper\", \"CREATE INDEX idx_pf_paper ON paper_fields(paper_id)\"),\n",
    "    (\"idx_pf_field\", \"CREATE INDEX idx_pf_field ON paper_fields(field_id)\"),\n",
    "    (\"idx_pr_paper\", \"CREATE INDEX idx_pr_paper ON paper_references(paper_id)\"),\n",
    "    (\"idx_pr_ref\", \"CREATE INDEX idx_pr_ref ON paper_references(reference_id)\"),\n",
    "    (\"idx_pd_paper\", \"CREATE INDEX idx_pd_paper ON paper_details(paper_id)\")\n",
    "]\n",
    "\n",
    "for idx_name, idx_sql in indexes:\n",
    "    try:\n",
    "        cursor.execute(idx_sql)\n",
    "        print(f\"[OK] {idx_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {idx_name}: {e}\")\n",
    "\n",
    "conn.commit()\n",
    "print(\"\\nIndexes created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "981c16b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================================================\n",
      "DATABASE SUMMARY\n",
      "========================================================================================================================\n",
      "\n",
      "Row Counts:\n",
      "\n",
      "  Affiliations                                   1 rows\n",
      "  Fields                                        39 rows\n",
      "  Papers                                    39,903 rows\n",
      "  Paper Details                             10,293 rows\n",
      "  Paper-Author-Affiliations                193,408 rows\n",
      "  Paper-Fields                              18,047 rows\n",
      "  Citations                                424,616 rows\n",
      "\n",
      "Papers by Year:\n",
      "\n",
      " year  count\n",
      " 2013   3453\n",
      " 2014   3776\n",
      " 2015   4190\n",
      " 2016   4193\n",
      " 2017   4495\n",
      " 2018   4439\n",
      " 2019   4845\n",
      " 2020   5096\n",
      " 2021   5307\n",
      " 2022    109\n",
      "\n",
      "Citation Network:\n",
      "\n",
      " total_citation_links  papers_that_cite  papers_cited\n",
      "               424616            125892        199122\n",
      "\n",
      "Author Statistics:\n",
      "\n",
      " unique_authors  papers_with_authors  avg_authors_per_paper\n",
      "          42152                94577                   2.04\n",
      "\n",
      "Patent Statistics:\n",
      "\n",
      " papers_with_patent_data  papers_with_patents  avg_patent_count  max_patent_count\n",
      "                   39903                 1016              0.06                34\n",
      "\n",
      "Database File:\n",
      "  Location: /ssd/Personal/sciscinet/full-stack/sciscinet-full-stack-backend/data/sciscinet_vt_cs_2013_2022.db\n",
      "  Size: 49.80 MB\n",
      "\n",
      "========================================================================================================================\n",
      "PREPROCESSING COMPLETE\n",
      "========================================================================================================================\n",
      "\n",
      "Database ready for both projects:\n",
      "\n",
      "Project 1 - Full-Stack Web Development:\n",
      "  T1 (Citation & Collaboration Networks): 2018-2022 (past 5 years)\n",
      "  T2 (Timeline + Patent Distribution): 2013-2022 (past 10 years)\n",
      "  T3 (Network Refinement): Uses T1 data\n",
      "\n",
      "Project 2 - LLM Agent Analysis:\n",
      "  Uses all VT CS papers from 2013-2022\n",
      "  All tables filtered for VT + CS + year range\n",
      "\n",
      "Database location: /ssd/Personal/sciscinet/full-stack/sciscinet-full-stack-backend/data/sciscinet_vt_cs_2013_2022.db\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"DATABASE SUMMARY\")\n",
    "print(\"=\"*120 + \"\\n\")\n",
    "\n",
    "tables = [\n",
    "    ('affiliations', 'Affiliations'),\n",
    "    ('fields', 'Fields'),\n",
    "    ('papers', 'Papers'),\n",
    "    ('paper_details', 'Paper Details'),\n",
    "    ('paper_author_affiliations', 'Paper-Author-Affiliations'),\n",
    "    ('paper_fields', 'Paper-Fields'),\n",
    "    ('paper_references', 'Citations')\n",
    "]\n",
    "\n",
    "print(\"Row Counts:\\n\")\n",
    "for table, display_name in tables:\n",
    "    count = pd.read_sql(f\"SELECT COUNT(*) as count FROM {table}\", conn).iloc[0]['count']\n",
    "    print(f\"  {display_name:35} {count:>12,} rows\")\n",
    "\n",
    "print(\"\\nPapers by Year:\\n\")\n",
    "papers_by_year = pd.read_sql(\"\"\"\n",
    "    SELECT year, COUNT(*) as count \n",
    "    FROM papers \n",
    "    GROUP BY year \n",
    "    ORDER BY year\n",
    "\"\"\", conn)\n",
    "print(papers_by_year.to_string(index=False))\n",
    "\n",
    "print(\"\\nCitation Network:\\n\")\n",
    "citation_stats = pd.read_sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_citation_links,\n",
    "        COUNT(DISTINCT paper_id) as papers_that_cite,\n",
    "        COUNT(DISTINCT reference_id) as papers_cited\n",
    "    FROM paper_references\n",
    "\"\"\", conn)\n",
    "print(citation_stats.to_string(index=False))\n",
    "\n",
    "print(\"\\nAuthor Statistics:\\n\")\n",
    "author_stats = pd.read_sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(DISTINCT author_id) as unique_authors,\n",
    "        COUNT(DISTINCT paper_id) as papers_with_authors,\n",
    "        ROUND(CAST(COUNT(*) AS FLOAT) / COUNT(DISTINCT paper_id), 2) as avg_authors_per_paper\n",
    "    FROM paper_author_affiliations\n",
    "\"\"\", conn)\n",
    "print(author_stats.to_string(index=False))\n",
    "\n",
    "print(\"\\nPatent Statistics:\\n\")\n",
    "patent_stats = pd.read_sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(patent_count) as papers_with_patent_data,\n",
    "        SUM(CASE WHEN patent_count > 0 THEN 1 ELSE 0 END) as papers_with_patents,\n",
    "        ROUND(AVG(patent_count), 2) as avg_patent_count,\n",
    "        MAX(patent_count) as max_patent_count\n",
    "    FROM papers\n",
    "    WHERE patent_count IS NOT NULL\n",
    "\"\"\", conn)\n",
    "print(patent_stats.to_string(index=False))\n",
    "\n",
    "db_size_mb = os.path.getsize(DB_PATH) / (1024**2)\n",
    "print(f\"\\nDatabase File:\")\n",
    "print(f\"  Location: {os.path.abspath(DB_PATH)}\")\n",
    "print(f\"  Size: {db_size_mb:.2f} MB\")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"PREPROCESSING COMPLETE\")\n",
    "print(\"=\"*120)\n",
    "print(f\"\\nDatabase ready for both projects:\")\n",
    "print(f\"\\nProject 1 - Full-Stack Web Development:\")\n",
    "print(f\"  T1 (Citation & Collaboration Networks): {START_YEAR_T1}-{END_YEAR} (past 5 years)\")\n",
    "print(f\"  T2 (Timeline + Patent Distribution): {START_YEAR_T2}-{END_YEAR} (past 10 years)\")\n",
    "print(f\"  T3 (Network Refinement): Uses T1 data\")\n",
    "print(f\"\\nProject 2 - LLM Agent Analysis:\")\n",
    "print(f\"  Uses all VT CS papers from {START_YEAR_T2}-{END_YEAR}\")\n",
    "print(f\"  All tables filtered for VT + CS + year range\")\n",
    "print(f\"\\nDatabase location: {os.path.abspath(DB_PATH)}\")\n",
    "print(\"=\"*120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a43be6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Year Range Verification ===\n",
      "\n",
      "Papers by year with applicable tasks:\n",
      "\n",
      " year  total_papers                      applicable_to\n",
      " 2022           109 Project 1 (T1, T2, T3) + Project 2\n",
      " 2021          5307 Project 1 (T1, T2, T3) + Project 2\n",
      " 2020          5096 Project 1 (T1, T2, T3) + Project 2\n",
      " 2019          4845 Project 1 (T1, T2, T3) + Project 2\n",
      " 2018          4439 Project 1 (T1, T2, T3) + Project 2\n",
      " 2017          4495         Project 1 (T2) + Project 2\n",
      " 2016          4193         Project 1 (T2) + Project 2\n",
      " 2015          4190         Project 1 (T2) + Project 2\n",
      " 2014          3776         Project 1 (T2) + Project 2\n",
      " 2013          3453         Project 1 (T2) + Project 2\n",
      "\n",
      "\n",
      "Papers available for:\n",
      "  Project 1 T1 (Citation & Collaboration, 2018-2022): 19,796 papers\n",
      "  Project 1 T2 (Timeline, 2013-2022): 39,903 papers\n",
      "  Project 2 (LLM Agent, 2013-2022): 39,903 papers\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Year Range Verification ===\\n\")\n",
    "\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "\n",
    "print(\"Papers by year with applicable tasks:\\n\")\n",
    "year_summary = pd.read_sql(f\"\"\"\n",
    "    SELECT \n",
    "        year,\n",
    "        COUNT(*) as total_papers,\n",
    "        CASE \n",
    "            WHEN year >= {START_YEAR_T1} THEN 'Project 1 (T1, T2, T3) + Project 2'\n",
    "            WHEN year >= {START_YEAR_T2} THEN 'Project 1 (T2) + Project 2'\n",
    "            ELSE 'Out of range'\n",
    "        END as applicable_to\n",
    "    FROM papers \n",
    "    GROUP BY year \n",
    "    ORDER BY year DESC\n",
    "\"\"\", conn)\n",
    "print(year_summary.to_string(index=False))\n",
    "\n",
    "t1_count = pd.read_sql(f\"\"\"\n",
    "    SELECT COUNT(*) as count FROM papers WHERE year >= {START_YEAR_T1}\n",
    "\"\"\", conn).iloc[0]['count']\n",
    "\n",
    "t2_count = pd.read_sql(f\"\"\"\n",
    "    SELECT COUNT(*) as count FROM papers WHERE year >= {START_YEAR_T2}\n",
    "\"\"\", conn).iloc[0]['count']\n",
    "\n",
    "print(f\"\\n\\nPapers available for:\")\n",
    "print(f\"  Project 1 T1 (Citation & Collaboration, {START_YEAR_T1}-{END_YEAR}): {t1_count:,} papers\")\n",
    "print(f\"  Project 1 T2 (Timeline, {START_YEAR_T2}-{END_YEAR}): {t2_count:,} papers\")\n",
    "print(f\"  Project 2 (LLM Agent, {START_YEAR_T2}-{END_YEAR}): {t2_count:,} papers\")\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcdb3d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample Queries ===\n",
      "\n",
      "Most recent 5 papers:\n",
      "\n",
      "  paper_id title  year  citation_count\n",
      "3159659315  None  2022              13\n",
      "3210404677  None  2022               3\n",
      "3197283713  None  2022               2\n",
      "3194526859  None  2022               2\n",
      "3193947462  None  2022               2\n",
      "\n",
      "\n",
      "Top 5 most cited papers:\n",
      "\n",
      "  paper_id title  year  citation_count\n",
      "2751884637  None  2017            8514\n",
      "2104549677  None  2015            8179\n",
      "2614986146  None  2016            3451\n",
      "2527824850  None  2016            3429\n",
      "2142428670  None  2014            2506\n",
      "\n",
      "\n",
      "Papers with most patent citations:\n",
      "\n",
      "  paper_id title  year  patent_count\n",
      "2010121929  None  2014            34\n",
      "1988386651  None  2013            28\n",
      "2104549677  None  2015            24\n",
      "1973838853  None  2013            23\n",
      "2010869577  None  2014            22\n",
      "\n",
      "\n",
      "Most prolific authors:\n",
      "\n",
      " author_id  paper_count\n",
      "1750292016          481\n",
      "1263832234          445\n",
      "1865774345          405\n",
      "1972755273          398\n",
      "2102906916          374\n",
      "\n",
      "\n",
      "Papers by field (for Project 2 analysis):\n",
      "\n",
      "                  field_name  paper_count\n",
      "            Computer science         8239\n",
      "     Artificial intelligence         1393\n",
      "                   Algorithm          625\n",
      "            Computer network          583\n",
      "       Distributed computing          549\n",
      "            Machine learning          497\n",
      "                  Simulation          450\n",
      "           Computer security          415\n",
      "         Real-time computing          412\n",
      "                Data science          395\n",
      "  Humanâ€“computer interaction          377\n",
      "                 Data mining          374\n",
      "       Computational biology          326\n",
      "             Computer vision          302\n",
      "        Knowledge management          265\n",
      "         Pattern recognition          250\n",
      "             Embedded system          245\n",
      "                  Multimedia          211\n",
      "          Parallel computing          197\n",
      "              World Wide Web          194\n",
      "Theoretical computer science          159\n",
      "              Bioinformatics          149\n",
      "        Software engineering          142\n",
      "            Internet privacy          134\n",
      "          Telecommunications          130\n",
      "       Computational physics          116\n",
      "            Operating system          104\n",
      "       Information retrieval          103\n",
      "        Computer engineering           96\n",
      "                    Database           88\n",
      "        Programming language           83\n",
      " Natural language processing           79\n",
      "       Computational science           63\n",
      "           Computer hardware           62\n",
      "          Speech recognition           53\n",
      "       Computer architecture           51\n",
      "           Cognitive science           51\n",
      "     Computational chemistry           44\n",
      "  Computer graphics (images)           41\n",
      "\n",
      "\n",
      "Sample queries complete\n",
      "Database ready for Project 1 and Project 2!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Sample Queries ===\\n\")\n",
    "\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "\n",
    "print(\"Most recent 5 papers:\\n\")\n",
    "recent_papers = pd.read_sql(\"\"\"\n",
    "    SELECT paper_id, title, year, citation_count\n",
    "    FROM papers\n",
    "    ORDER BY year DESC, citation_count DESC\n",
    "    LIMIT 5\n",
    "\"\"\", conn)\n",
    "print(recent_papers.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nTop 5 most cited papers:\\n\")\n",
    "top_cited = pd.read_sql(\"\"\"\n",
    "    SELECT paper_id, title, year, citation_count\n",
    "    FROM papers\n",
    "    WHERE citation_count IS NOT NULL\n",
    "    ORDER BY citation_count DESC\n",
    "    LIMIT 5\n",
    "\"\"\", conn)\n",
    "print(top_cited.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nPapers with most patent citations:\\n\")\n",
    "top_patents = pd.read_sql(\"\"\"\n",
    "    SELECT p.paper_id, p.title, p.year, p.patent_count\n",
    "    FROM papers p\n",
    "    WHERE p.patent_count > 0\n",
    "    ORDER BY p.patent_count DESC\n",
    "    LIMIT 5\n",
    "\"\"\", conn)\n",
    "print(top_patents.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nMost prolific authors:\\n\")\n",
    "top_authors = pd.read_sql(\"\"\"\n",
    "    SELECT author_id, COUNT(DISTINCT paper_id) as paper_count\n",
    "    FROM paper_author_affiliations\n",
    "    GROUP BY author_id\n",
    "    ORDER BY paper_count DESC\n",
    "    LIMIT 5\n",
    "\"\"\", conn)\n",
    "print(top_authors.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nPapers by field (for Project 2 analysis):\\n\")\n",
    "papers_by_field = pd.read_sql(\"\"\"\n",
    "    SELECT f.field_name, COUNT(DISTINCT pf.paper_id) as paper_count\n",
    "    FROM paper_fields pf\n",
    "    JOIN fields f ON pf.field_id = f.field_id\n",
    "    GROUP BY f.field_name\n",
    "    ORDER BY paper_count DESC\n",
    "\"\"\", conn)\n",
    "print(papers_by_field.to_string(index=False))\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"\\n\\nSample queries complete\")\n",
    "print(\"Database ready for Project 1 and Project 2!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
