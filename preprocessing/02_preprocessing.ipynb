{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e31fd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SciSciNet Preprocessing Notebook ===\n",
      "Python version: 3.12.9 (main, Feb 12 2025, 14:39:53) [GCC 6.3.0 20170516]\n",
      "Pandas version: 2.3.3\n",
      "\n",
      "Loaded configuration from validation:\n",
      "  VT Affiliations: ['Virginia Tech']\n",
      "  CS Fields: 39 fields\n",
      "\n",
      "Processing Configuration:\n",
      "  Database: sciscinet_vt_cs.db\n",
      "  Chunk size: 100,000 rows\n",
      "\n",
      "Year Ranges:\n",
      "  Project 1 T1 (Citation & Collaboration): 2020-2025 (past 5 years)\n",
      "  Project 1 T2 (Timeline): 2015-2025 (past 10 years)\n",
      "  Project 2 (LLM Agent Analysis): Uses all papers from 2015-2025\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=== SciSciNet Preprocessing Notebook ===\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Pandas version: {pd.__version__}\\n\")\n",
    "\n",
    "# Load validation config\n",
    "try:\n",
    "    from validation_config import (\n",
    "        VT_AFFILIATION_IDS, CS_FIELD_IDS, DATA_DIR,\n",
    "        AFFILIATION_ID_COL, AFFILIATION_NAME_COL,\n",
    "        FIELD_ID_COL, FIELD_NAME_COL,\n",
    "        PAPER_ID_COL, AUTHOR_ID_COL, PAA_AFFIL_COL,\n",
    "        VT_AFFILIATION_NAMES, CS_FIELD_NAMES\n",
    "    )\n",
    "    print(\"Loaded configuration from validation:\")\n",
    "    print(f\"  VT Affiliations: {VT_AFFILIATION_NAMES}\")\n",
    "    print(f\"  CS Fields: {len(CS_FIELD_NAMES)} fields\")\n",
    "except ImportError:\n",
    "    print(\"ERROR: validation_config.py not found\")\n",
    "    print(\"Run 01_validation.ipynb first\")\n",
    "    raise\n",
    "\n",
    "# Processing configuration\n",
    "DB_PATH = 'sciscinet_vt_cs.db'\n",
    "CHUNK_SIZE = 100000\n",
    "\n",
    "# Year ranges\n",
    "START_YEAR_T1 = 2020  # Project 1 T1: Citation & collaboration networks (past 5 years)\n",
    "START_YEAR_T2 = 2015  # Project 1 T2: Timeline visualization (past 10 years)\n",
    "END_YEAR = 2025\n",
    "\n",
    "print(f\"\\nProcessing Configuration:\")\n",
    "print(f\"  Database: {DB_PATH}\")\n",
    "print(f\"  Chunk size: {CHUNK_SIZE:,} rows\")\n",
    "print(f\"\\nYear Ranges:\")\n",
    "print(f\"  Project 1 T1 (Citation & Collaboration): {START_YEAR_T1}-{END_YEAR} (past 5 years)\")\n",
    "print(f\"  Project 1 T2 (Timeline): {START_YEAR_T2}-{END_YEAR} (past 10 years)\")\n",
    "print(f\"  Project 2 (LLM Agent Analysis): Uses all papers from {START_YEAR_T2}-{END_YEAR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b74687b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Creating Database Schema ===\n",
      "\n",
      "[OK] Created table: affiliations\n",
      "[OK] Created table: fields\n",
      "[OK] Created table: papers\n",
      "[OK] Created table: paper_details\n",
      "[OK] Created table: paper_author_affiliations\n",
      "[OK] Created table: paper_fields\n",
      "[OK] Created table: paper_references\n",
      "\n",
      "Database schema created\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Creating Database Schema ===\\n\")\n",
    "\n",
    "if os.path.exists(DB_PATH):\n",
    "    os.remove(DB_PATH)\n",
    "    print(f\"Removed existing database: {DB_PATH}\")\n",
    "\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE affiliations (\n",
    "    affiliation_id INTEGER PRIMARY KEY,\n",
    "    affiliation_name TEXT\n",
    ")\n",
    "''')\n",
    "print(\"[OK] Created table: affiliations\")\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE fields (\n",
    "    field_id INTEGER PRIMARY KEY,\n",
    "    field_name TEXT\n",
    ")\n",
    "''')\n",
    "print(\"[OK] Created table: fields\")\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE papers (\n",
    "    paper_id INTEGER PRIMARY KEY,\n",
    "    title TEXT,\n",
    "    year INTEGER,\n",
    "    citation_count INTEGER,\n",
    "    reference_count INTEGER,\n",
    "    patent_count INTEGER\n",
    ")\n",
    "''')\n",
    "print(\"[OK] Created table: papers\")\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE paper_details (\n",
    "    paper_id INTEGER PRIMARY KEY,\n",
    "    abstract TEXT\n",
    ")\n",
    "''')\n",
    "print(\"[OK] Created table: paper_details\")\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE paper_author_affiliations (\n",
    "    paper_id INTEGER,\n",
    "    author_id INTEGER,\n",
    "    affiliation_id INTEGER,\n",
    "    author_sequence INTEGER,\n",
    "    PRIMARY KEY (paper_id, author_id, affiliation_id)\n",
    ")\n",
    "''')\n",
    "print(\"[OK] Created table: paper_author_affiliations\")\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE paper_fields (\n",
    "    paper_id INTEGER,\n",
    "    field_id INTEGER,\n",
    "    score REAL,\n",
    "    PRIMARY KEY (paper_id, field_id)\n",
    ")\n",
    "''')\n",
    "print(\"[OK] Created table: paper_fields\")\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE paper_references (\n",
    "    paper_id INTEGER,\n",
    "    reference_id INTEGER,\n",
    "    PRIMARY KEY (paper_id, reference_id)\n",
    ")\n",
    "''')\n",
    "print(\"[OK] Created table: paper_references\")\n",
    "\n",
    "conn.commit()\n",
    "print(\"\\nDatabase schema created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21f30fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading Affiliations ===\n",
      "\n",
      "Total affiliations: 26,998\n",
      "Virginia Tech affiliations: 1\n",
      "Affiliations loaded to database\n",
      "\n",
      " affiliation_id affiliation_name\n",
      "      859038795    Virginia Tech\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Loading Affiliations ===\\n\")\n",
    "\n",
    "df_affiliations = pd.read_csv(\n",
    "    os.path.join(DATA_DIR, 'SciSciNet_Affiliations.tsv'),\n",
    "    sep='\\t'\n",
    ")\n",
    "\n",
    "print(f\"Total affiliations: {len(df_affiliations):,}\")\n",
    "\n",
    "vt_affiliations = df_affiliations[\n",
    "    df_affiliations[AFFILIATION_ID_COL].isin(VT_AFFILIATION_IDS)\n",
    "]\n",
    "\n",
    "print(f\"Virginia Tech affiliations: {len(vt_affiliations)}\")\n",
    "\n",
    "vt_affiliations_clean = vt_affiliations.rename(columns={\n",
    "    AFFILIATION_ID_COL: 'affiliation_id',\n",
    "    AFFILIATION_NAME_COL: 'affiliation_name'\n",
    "})[['affiliation_id', 'affiliation_name']]\n",
    "\n",
    "vt_affiliations_clean.to_sql('affiliations', conn, if_exists='replace', index=False)\n",
    "\n",
    "print(\"Affiliations loaded to database\")\n",
    "print(f\"\\n{vt_affiliations_clean.to_string(index=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b451f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading Fields ===\n",
      "\n",
      "Total fields: 311\n",
      "Computer Science fields: 39\n",
      "Fields loaded to database\n",
      "\n",
      "  field_id                   field_name\n",
      " 113775141         Computer engineering\n",
      " 124101348                  Data mining\n",
      "  56739046         Knowledge management\n",
      " 149635348              Embedded system\n",
      " 107457646   Humanâ€“computer interaction\n",
      "  11413529                    Algorithm\n",
      "  28490314           Speech recognition\n",
      " 111919701             Operating system\n",
      "  31972630              Computer vision\n",
      "  77088390                     Database\n",
      " 108827166             Internet privacy\n",
      "  76155785           Telecommunications\n",
      "   9390403            Computer hardware\n",
      " 154945302      Artificial intelligence\n",
      "  49774154                   Multimedia\n",
      "    459310        Computational science\n",
      " 199360897         Programming language\n",
      "  38652104            Computer security\n",
      "  79403827          Real-time computing\n",
      "  41008148             Computer science\n",
      " 204321447  Natural language processing\n",
      " 188147891            Cognitive science\n",
      " 136764020               World Wide Web\n",
      " 147597530      Computational chemistry\n",
      " 120314980        Distributed computing\n",
      "  80444323 Theoretical computer science\n",
      "  60644358               Bioinformatics\n",
      " 115903868         Software engineering\n",
      "2522767166                 Data science\n",
      "  31258907             Computer network\n",
      " 119857082             Machine learning\n",
      "  44154836                   Simulation\n",
      " 173608175           Parallel computing\n",
      "  23123220        Information retrieval\n",
      "  30475298        Computational physics\n",
      " 121684516   Computer graphics (images)\n",
      " 178980831          Pattern recognition\n",
      " 118524514        Computer architecture\n",
      "  70721500        Computational biology\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Loading Fields ===\\n\")\n",
    "\n",
    "df_fields = pd.read_csv(\n",
    "    os.path.join(DATA_DIR, 'SciSciNet_Fields.tsv'),\n",
    "    sep='\\t'\n",
    ")\n",
    "\n",
    "print(f\"Total fields: {len(df_fields):,}\")\n",
    "\n",
    "cs_fields = df_fields[\n",
    "    df_fields[FIELD_ID_COL].isin(CS_FIELD_IDS)\n",
    "]\n",
    "\n",
    "print(f\"Computer Science fields: {len(cs_fields)}\")\n",
    "\n",
    "cs_fields_clean = cs_fields.rename(columns={\n",
    "    FIELD_ID_COL: 'field_id',\n",
    "    FIELD_NAME_COL: 'field_name'\n",
    "})[['field_id', 'field_name']]\n",
    "\n",
    "cs_fields_clean.to_sql('fields', conn, if_exists='replace', index=False)\n",
    "\n",
    "print(\"Fields loaded to database\")\n",
    "print(f\"\\n{cs_fields_clean.to_string(index=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95a52a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 1: Filtering Paper-Author-Affiliations for VT ===\n",
      "\n",
      "Processing large file (~11.68 GB)...\n",
      "\n",
      "  Chunk 50: 5,000,000 rows | 1,933 VT records | 1917507 rows/sec\n",
      "  Chunk 100: 10,000,000 rows | 3,922 VT records | 1938113 rows/sec\n",
      "  Chunk 150: 15,000,000 rows | 4,954 VT records | 2012217 rows/sec\n",
      "  Chunk 200: 20,000,000 rows | 5,742 VT records | 2047885 rows/sec\n",
      "  Chunk 250: 25,000,000 rows | 7,701 VT records | 1996887 rows/sec\n",
      "  Chunk 300: 30,000,000 rows | 9,959 VT records | 1960397 rows/sec\n",
      "  Chunk 350: 35,000,000 rows | 12,298 VT records | 1938735 rows/sec\n",
      "  Chunk 400: 40,000,000 rows | 14,323 VT records | 1906457 rows/sec\n",
      "  Chunk 450: 45,000,000 rows | 17,179 VT records | 1884590 rows/sec\n",
      "  Chunk 500: 50,000,000 rows | 20,514 VT records | 1868009 rows/sec\n",
      "  Chunk 550: 55,000,000 rows | 23,959 VT records | 1850893 rows/sec\n",
      "  Chunk 600: 60,000,000 rows | 27,480 VT records | 1840711 rows/sec\n",
      "  Chunk 650: 65,000,000 rows | 30,837 VT records | 1828275 rows/sec\n",
      "  Chunk 700: 70,000,000 rows | 34,293 VT records | 1819503 rows/sec\n",
      "  Chunk 750: 75,000,000 rows | 37,689 VT records | 1048192 rows/sec\n",
      "  Chunk 800: 80,000,000 rows | 40,998 VT records | 761817 rows/sec\n",
      "  Chunk 850: 85,000,000 rows | 44,324 VT records | 788141 rows/sec\n",
      "  Chunk 900: 90,000,000 rows | 47,457 VT records | 813227 rows/sec\n",
      "  Chunk 950: 95,000,000 rows | 50,825 VT records | 837251 rows/sec\n",
      "  Chunk 1000: 100,000,000 rows | 54,159 VT records | 859822 rows/sec\n",
      "  Chunk 1050: 105,000,000 rows | 57,411 VT records | 881279 rows/sec\n",
      "  Chunk 1100: 110,000,000 rows | 60,721 VT records | 901741 rows/sec\n",
      "  Chunk 1150: 115,000,000 rows | 63,923 VT records | 742306 rows/sec\n",
      "  Chunk 1200: 120,000,000 rows | 67,244 VT records | 761015 rows/sec\n",
      "  Chunk 1250: 125,000,000 rows | 70,625 VT records | 779189 rows/sec\n",
      "  Chunk 1300: 130,000,000 rows | 73,844 VT records | 796819 rows/sec\n",
      "  Chunk 1350: 135,000,000 rows | 77,003 VT records | 813656 rows/sec\n",
      "  Chunk 1400: 140,000,000 rows | 80,388 VT records | 829522 rows/sec\n",
      "  Chunk 1450: 145,000,000 rows | 83,699 VT records | 845164 rows/sec\n",
      "  Chunk 1500: 150,000,000 rows | 87,005 VT records | 860168 rows/sec\n",
      "  Chunk 1550: 155,000,000 rows | 90,340 VT records | 874494 rows/sec\n",
      "  Chunk 1600: 160,000,000 rows | 93,602 VT records | 888075 rows/sec\n",
      "  Chunk 1650: 165,000,000 rows | 97,053 VT records | 901287 rows/sec\n",
      "  Chunk 1700: 170,000,000 rows | 100,426 VT records | 786718 rows/sec\n",
      "  Chunk 1750: 175,000,000 rows | 103,934 VT records | 799179 rows/sec\n",
      "  Chunk 1800: 180,000,000 rows | 107,241 VT records | 713988 rows/sec\n",
      "  Chunk 1850: 185,000,000 rows | 110,745 VT records | 725492 rows/sec\n",
      "  Chunk 1900: 190,000,000 rows | 114,305 VT records | 736811 rows/sec\n",
      "  Chunk 1950: 195,000,000 rows | 116,447 VT records | 748337 rows/sec\n",
      "  Chunk 2000: 200,000,000 rows | 117,987 VT records | 759895 rows/sec\n",
      "  Chunk 2050: 205,000,000 rows | 120,062 VT records | 771025 rows/sec\n",
      "  Chunk 2100: 210,000,000 rows | 123,000 VT records | 782241 rows/sec\n",
      "  Chunk 2150: 215,000,000 rows | 126,050 VT records | 793257 rows/sec\n",
      "  Chunk 2200: 220,000,000 rows | 127,978 VT records | 803805 rows/sec\n",
      "  Chunk 2250: 225,000,000 rows | 128,134 VT records | 814692 rows/sec\n",
      "  Chunk 2300: 230,000,000 rows | 128,860 VT records | 825369 rows/sec\n",
      "  Chunk 2350: 235,000,000 rows | 129,595 VT records | 835941 rows/sec\n",
      "  Chunk 2400: 240,000,000 rows | 130,259 VT records | 846364 rows/sec\n",
      "  Chunk 2450: 245,000,000 rows | 131,221 VT records | 856364 rows/sec\n",
      "  Chunk 2500: 250,000,000 rows | 132,230 VT records | 866237 rows/sec\n",
      "  Chunk 2550: 255,000,000 rows | 133,651 VT records | 875618 rows/sec\n",
      "  Chunk 2600: 260,000,000 rows | 135,432 VT records | 884518 rows/sec\n",
      "  Chunk 2650: 265,000,000 rows | 137,610 VT records | 893138 rows/sec\n",
      "  Chunk 2700: 270,000,000 rows | 139,410 VT records | 901785 rows/sec\n",
      "  Chunk 2750: 275,000,000 rows | 141,166 VT records | 910355 rows/sec\n",
      "  Chunk 2800: 280,000,000 rows | 142,997 VT records | 918802 rows/sec\n",
      "  Chunk 2850: 285,000,000 rows | 144,878 VT records | 927113 rows/sec\n",
      "  Chunk 2900: 290,000,000 rows | 147,122 VT records | 934989 rows/sec\n",
      "  Chunk 2950: 295,000,000 rows | 149,165 VT records | 942680 rows/sec\n",
      "  Chunk 3000: 300,000,000 rows | 151,003 VT records | 950417 rows/sec\n",
      "  Chunk 3050: 305,000,000 rows | 153,033 VT records | 957832 rows/sec\n",
      "  Chunk 3100: 310,000,000 rows | 155,156 VT records | 965134 rows/sec\n",
      "  Chunk 3150: 315,000,000 rows | 157,387 VT records | 972323 rows/sec\n",
      "  Chunk 3200: 320,000,000 rows | 159,563 VT records | 979360 rows/sec\n",
      "  Chunk 3250: 325,000,000 rows | 161,499 VT records | 986341 rows/sec\n",
      "  Chunk 3300: 330,000,000 rows | 163,111 VT records | 910569 rows/sec\n",
      "  Chunk 3350: 335,000,000 rows | 165,773 VT records | 917277 rows/sec\n",
      "  Chunk 3400: 340,000,000 rows | 167,425 VT records | 853898 rows/sec\n",
      "  Chunk 3450: 345,000,000 rows | 169,307 VT records | 860554 rows/sec\n",
      "  Chunk 3500: 350,000,000 rows | 171,362 VT records | 867055 rows/sec\n",
      "  Chunk 3550: 355,000,000 rows | 173,189 VT records | 873488 rows/sec\n",
      "  Chunk 3600: 360,000,000 rows | 174,156 VT records | 880191 rows/sec\n",
      "  Chunk 3650: 365,000,000 rows | 176,152 VT records | 886417 rows/sec\n",
      "  Chunk 3700: 370,000,000 rows | 177,896 VT records | 892561 rows/sec\n",
      "  Chunk 3750: 375,000,000 rows | 180,033 VT records | 898595 rows/sec\n",
      "  Chunk 3800: 380,000,000 rows | 181,965 VT records | 904672 rows/sec\n",
      "  Chunk 3850: 385,000,000 rows | 183,860 VT records | 910686 rows/sec\n",
      "  Chunk 3900: 390,000,000 rows | 185,500 VT records | 916664 rows/sec\n",
      "  Chunk 3950: 395,000,000 rows | 187,186 VT records | 922504 rows/sec\n",
      "  Chunk 4000: 400,000,000 rows | 188,782 VT records | 928218 rows/sec\n",
      "  Chunk 4050: 405,000,000 rows | 190,710 VT records | 933882 rows/sec\n",
      "  Chunk 4100: 410,000,000 rows | 192,254 VT records | 939523 rows/sec\n",
      "\n",
      "Step 1 Complete\n",
      "  Time: 438.4 seconds\n",
      "  Total rows: 413,869,501\n",
      "  VT records: 193,408\n",
      "  Unique VT papers: 94,577\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Step 1: Filtering Paper-Author-Affiliations for VT ===\\n\")\n",
    "print(\"Processing large file (~11.68 GB)...\\n\")\n",
    "\n",
    "file_path = os.path.join(DATA_DIR, 'SciSciNet_PaperAuthorAffiliations.tsv')\n",
    "vt_papers_set = set()\n",
    "\n",
    "chunk_count = 0\n",
    "total_rows = 0\n",
    "filtered_rows = 0\n",
    "start_time = datetime.now()\n",
    "\n",
    "for chunk in pd.read_csv(file_path, sep='\\t', chunksize=CHUNK_SIZE):\n",
    "    chunk_count += 1\n",
    "    total_rows += len(chunk)\n",
    "    \n",
    "    vt_chunk = chunk[chunk[PAA_AFFIL_COL].isin(VT_AFFILIATION_IDS)]\n",
    "    filtered_rows += len(vt_chunk)\n",
    "    \n",
    "    if len(vt_chunk) > 0:\n",
    "        vt_papers_set.update(vt_chunk[PAPER_ID_COL].dropna().astype(int).values)\n",
    "        \n",
    "        # Determine sequence column\n",
    "        seq_col = None\n",
    "        for col in ['AuthorSequenceNumber', 'SequenceNumber', 'AuthorSequence']:\n",
    "            if col in vt_chunk.columns:\n",
    "                seq_col = col\n",
    "                break\n",
    "        \n",
    "        cols_map = {\n",
    "            PAPER_ID_COL: 'paper_id',\n",
    "            AUTHOR_ID_COL: 'author_id',\n",
    "            PAA_AFFIL_COL: 'affiliation_id'\n",
    "        }\n",
    "        if seq_col:\n",
    "            cols_map[seq_col] = 'author_sequence'\n",
    "        \n",
    "        vt_chunk_clean = vt_chunk.rename(columns=cols_map)\n",
    "        keep_cols = [col for col in ['paper_id', 'author_id', 'affiliation_id', 'author_sequence'] if col in vt_chunk_clean.columns]\n",
    "        vt_chunk_clean[keep_cols].to_sql('paper_author_affiliations', conn, if_exists='append', index=False)\n",
    "    \n",
    "    if chunk_count % 50 == 0:\n",
    "        elapsed = (datetime.now() - start_time).total_seconds()\n",
    "        rate = total_rows / elapsed if elapsed > 0 else 0\n",
    "        print(f\"  Chunk {chunk_count}: {total_rows:,} rows | {filtered_rows:,} VT records | {rate:.0f} rows/sec\")\n",
    "\n",
    "elapsed_time = (datetime.now() - start_time).total_seconds()\n",
    "print(f\"\\nStep 1 Complete\")\n",
    "print(f\"  Time: {elapsed_time:.1f} seconds\")\n",
    "print(f\"  Total rows: {total_rows:,}\")\n",
    "print(f\"  VT records: {filtered_rows:,}\")\n",
    "print(f\"  Unique VT papers: {len(vt_papers_set):,}\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "399ea000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 2: Filtering Papers for VT + Year Range ===\n",
      "\n",
      "Processing Papers.tsv (~16.46 GB)...\n",
      "\n",
      "Using year range: 2015-2025 (to support all projects)\n",
      "\n",
      "  Chunk 50: 5,000,000 rows | 1,208 filtered | 255075 rows/sec\n",
      "  Chunk 100: 10,000,000 rows | 2,409 filtered | 254216 rows/sec\n",
      "  Chunk 150: 15,000,000 rows | 3,583 filtered | 253116 rows/sec\n",
      "  Chunk 200: 20,000,000 rows | 4,821 filtered | 252166 rows/sec\n",
      "  Chunk 250: 25,000,000 rows | 6,008 filtered | 251537 rows/sec\n",
      "  Chunk 300: 30,000,000 rows | 7,192 filtered | 251242 rows/sec\n",
      "  Chunk 350: 35,000,000 rows | 8,428 filtered | 250808 rows/sec\n",
      "  Chunk 400: 40,000,000 rows | 9,634 filtered | 250521 rows/sec\n",
      "  Chunk 450: 45,000,000 rows | 10,829 filtered | 250344 rows/sec\n",
      "  Chunk 500: 50,000,000 rows | 12,135 filtered | 250064 rows/sec\n",
      "  Chunk 550: 55,000,000 rows | 13,347 filtered | 249707 rows/sec\n",
      "  Chunk 600: 60,000,000 rows | 14,580 filtered | 249402 rows/sec\n",
      "  Chunk 650: 65,000,000 rows | 15,764 filtered | 249237 rows/sec\n",
      "  Chunk 700: 70,000,000 rows | 17,032 filtered | 249212 rows/sec\n",
      "  Chunk 750: 75,000,000 rows | 18,269 filtered | 249141 rows/sec\n",
      "  Chunk 800: 80,000,000 rows | 19,460 filtered | 249115 rows/sec\n",
      "  Chunk 850: 85,000,000 rows | 20,647 filtered | 249026 rows/sec\n",
      "  Chunk 900: 90,000,000 rows | 21,867 filtered | 248903 rows/sec\n",
      "  Chunk 950: 95,000,000 rows | 23,117 filtered | 248787 rows/sec\n",
      "  Chunk 1000: 100,000,000 rows | 24,303 filtered | 248649 rows/sec\n",
      "  Chunk 1050: 105,000,000 rows | 25,498 filtered | 248576 rows/sec\n",
      "  Chunk 1100: 110,000,000 rows | 26,747 filtered | 248543 rows/sec\n",
      "  Chunk 1150: 115,000,000 rows | 28,005 filtered | 248486 rows/sec\n",
      "  Chunk 1200: 120,000,000 rows | 29,186 filtered | 248448 rows/sec\n",
      "  Chunk 1250: 125,000,000 rows | 30,416 filtered | 248436 rows/sec\n",
      "  Chunk 1300: 130,000,000 rows | 31,637 filtered | 248417 rows/sec\n",
      "\n",
      "Step 2 Complete\n",
      "  Time: 540.1 seconds\n",
      "  VT papers in year range: 32,674\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Step 2: Filtering Papers for VT + Year Range ===\\n\")\n",
    "print(\"Processing Papers.tsv (~16.46 GB)...\\n\")\n",
    "print(f\"Using year range: {START_YEAR_T2}-{END_YEAR} (to support all projects)\\n\")\n",
    "\n",
    "file_path = os.path.join(DATA_DIR, 'SciSciNet_Papers.tsv')\n",
    "vt_papers_in_range_set = set()\n",
    "\n",
    "chunk_count = 0\n",
    "total_rows = 0\n",
    "filtered_rows = 0\n",
    "start_time = datetime.now()\n",
    "\n",
    "for chunk in pd.read_csv(file_path, sep='\\t', chunksize=CHUNK_SIZE):\n",
    "    chunk_count += 1\n",
    "    total_rows += len(chunk)\n",
    "    \n",
    "    # Identify year column\n",
    "    year_col = None\n",
    "    for col in ['Year', 'PublicationYear']:\n",
    "        if col in chunk.columns:\n",
    "            year_col = col\n",
    "            break\n",
    "    \n",
    "    filtered_chunk = chunk[\n",
    "        (chunk[PAPER_ID_COL].isin(vt_papers_set)) &\n",
    "        (chunk[year_col] >= START_YEAR_T2) &\n",
    "        (chunk[year_col] <= END_YEAR)\n",
    "    ]\n",
    "    filtered_rows += len(filtered_chunk)\n",
    "    \n",
    "    if len(filtered_chunk) > 0:\n",
    "        vt_papers_in_range_set.update(filtered_chunk[PAPER_ID_COL].dropna().astype(int).values)\n",
    "        \n",
    "        # Identify columns\n",
    "        title_col = None\n",
    "        for col in ['PaperTitle', 'OriginalTitle', 'Title']:\n",
    "            if col in filtered_chunk.columns:\n",
    "                title_col = col\n",
    "                break\n",
    "        \n",
    "        citation_col = None\n",
    "        for col in ['CitationCount', 'Citations', 'Citation_Count']:\n",
    "            if col in filtered_chunk.columns:\n",
    "                citation_col = col\n",
    "                break\n",
    "        \n",
    "        reference_col = None\n",
    "        for col in ['ReferenceCount', 'References', 'Reference_Count']:\n",
    "            if col in filtered_chunk.columns:\n",
    "                reference_col = col\n",
    "                break\n",
    "        \n",
    "        patent_col = None\n",
    "        for col in ['Patent_Count', 'PatentCount']:\n",
    "            if col in filtered_chunk.columns:\n",
    "                patent_col = col\n",
    "                break\n",
    "        \n",
    "        cols_map = {PAPER_ID_COL: 'paper_id'}\n",
    "        if year_col:\n",
    "            cols_map[year_col] = 'year'\n",
    "        if title_col:\n",
    "            cols_map[title_col] = 'title'\n",
    "        if citation_col:\n",
    "            cols_map[citation_col] = 'citation_count'\n",
    "        if reference_col:\n",
    "            cols_map[reference_col] = 'reference_count'\n",
    "        if patent_col:\n",
    "            cols_map[patent_col] = 'patent_count'\n",
    "        \n",
    "        filtered_chunk_clean = filtered_chunk.rename(columns=cols_map)\n",
    "        \n",
    "        keep_cols = [col for col in ['paper_id', 'title', 'year', 'citation_count', 'reference_count', 'patent_count'] if col in filtered_chunk_clean.columns]\n",
    "        \n",
    "        filtered_chunk_clean[keep_cols].to_sql('papers', conn, if_exists='append', index=False)\n",
    "    \n",
    "    if chunk_count % 50 == 0:\n",
    "        elapsed = (datetime.now() - start_time).total_seconds()\n",
    "        rate = total_rows / elapsed if elapsed > 0 else 0\n",
    "        print(f\"  Chunk {chunk_count}: {total_rows:,} rows | {filtered_rows:,} filtered | {rate:.0f} rows/sec\")\n",
    "\n",
    "elapsed_time = (datetime.now() - start_time).total_seconds()\n",
    "print(f\"\\nStep 2 Complete\")\n",
    "print(f\"  Time: {elapsed_time:.1f} seconds\")\n",
    "print(f\"  VT papers in year range: {len(vt_papers_in_range_set):,}\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "248a546b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 3: Filtering for Computer Science Papers ===\n",
      "\n",
      "Processing PaperFields.tsv (~11.62 GB)...\n",
      "\n",
      "Using columns: paper='PaperID', field='FieldID', score='None'\n",
      "\n",
      "  Chunk 50: 5,000,000 rows | 30 CS papers | 1142274 rows/sec\n",
      "  Chunk 100: 10,000,000 rows | 117 CS papers | 1126266 rows/sec\n",
      "  Chunk 150: 15,000,000 rows | 222 CS papers | 1110016 rows/sec\n",
      "  Chunk 200: 20,000,000 rows | 265 CS papers | 1101534 rows/sec\n",
      "  Chunk 250: 25,000,000 rows | 285 CS papers | 1102714 rows/sec\n",
      "  Chunk 300: 30,000,000 rows | 321 CS papers | 1103081 rows/sec\n",
      "  Chunk 350: 35,000,000 rows | 344 CS papers | 1105417 rows/sec\n",
      "  Chunk 400: 40,000,000 rows | 374 CS papers | 1106517 rows/sec\n",
      "  Chunk 450: 45,000,000 rows | 406 CS papers | 1105213 rows/sec\n",
      "  Chunk 500: 50,000,000 rows | 438 CS papers | 1104995 rows/sec\n",
      "  Chunk 550: 55,000,000 rows | 485 CS papers | 1103551 rows/sec\n",
      "  Chunk 600: 60,000,000 rows | 698 CS papers | 1104420 rows/sec\n",
      "  Chunk 650: 65,000,000 rows | 794 CS papers | 1109189 rows/sec\n",
      "  Chunk 700: 70,000,000 rows | 855 CS papers | 1115454 rows/sec\n",
      "  Chunk 750: 75,000,000 rows | 946 CS papers | 1119811 rows/sec\n",
      "  Chunk 800: 80,000,000 rows | 1,135 CS papers | 1121707 rows/sec\n",
      "  Chunk 850: 85,000,000 rows | 1,580 CS papers | 1121769 rows/sec\n",
      "  Chunk 900: 90,000,000 rows | 2,215 CS papers | 1121558 rows/sec\n",
      "  Chunk 950: 95,000,000 rows | 3,182 CS papers | 1121354 rows/sec\n",
      "  Chunk 1000: 100,000,000 rows | 4,077 CS papers | 1121210 rows/sec\n",
      "  Chunk 1050: 105,000,000 rows | 5,057 CS papers | 1121314 rows/sec\n",
      "  Chunk 1100: 110,000,000 rows | 5,948 CS papers | 1121455 rows/sec\n",
      "  Chunk 1150: 115,000,000 rows | 6,769 CS papers | 1123343 rows/sec\n",
      "  Chunk 1200: 120,000,000 rows | 7,119 CS papers | 1125530 rows/sec\n",
      "  Chunk 1250: 125,000,000 rows | 7,176 CS papers | 1126824 rows/sec\n",
      "  Chunk 1300: 130,000,000 rows | 7,295 CS papers | 1126191 rows/sec\n",
      "  Chunk 1350: 135,000,000 rows | 7,416 CS papers | 1124402 rows/sec\n",
      "  Chunk 1400: 140,000,000 rows | 7,541 CS papers | 1121996 rows/sec\n",
      "  Chunk 1450: 145,000,000 rows | 7,578 CS papers | 1120519 rows/sec\n",
      "  Chunk 1500: 150,000,000 rows | 7,615 CS papers | 1119301 rows/sec\n",
      "  Chunk 1550: 155,000,000 rows | 7,642 CS papers | 1118708 rows/sec\n",
      "  Chunk 1600: 160,000,000 rows | 7,686 CS papers | 1118005 rows/sec\n",
      "  Chunk 1650: 165,000,000 rows | 7,733 CS papers | 1117710 rows/sec\n",
      "  Chunk 1700: 170,000,000 rows | 7,764 CS papers | 1117566 rows/sec\n",
      "  Chunk 1750: 175,000,000 rows | 7,792 CS papers | 1117644 rows/sec\n",
      "  Chunk 1800: 180,000,000 rows | 7,827 CS papers | 1117427 rows/sec\n",
      "  Chunk 1850: 185,000,000 rows | 7,867 CS papers | 1117007 rows/sec\n",
      "  Chunk 1900: 190,000,000 rows | 7,901 CS papers | 1116491 rows/sec\n",
      "  Chunk 1950: 195,000,000 rows | 7,940 CS papers | 1116111 rows/sec\n",
      "  Chunk 2000: 200,000,000 rows | 8,036 CS papers | 1115460 rows/sec\n",
      "  Chunk 2050: 205,000,000 rows | 8,307 CS papers | 1115576 rows/sec\n",
      "  Chunk 2100: 210,000,000 rows | 8,500 CS papers | 1115868 rows/sec\n",
      "  Chunk 2150: 215,000,000 rows | 8,576 CS papers | 1118072 rows/sec\n",
      "  Chunk 2200: 220,000,000 rows | 8,651 CS papers | 1119809 rows/sec\n",
      "  Chunk 2250: 225,000,000 rows | 8,707 CS papers | 1121525 rows/sec\n",
      "  Chunk 2300: 230,000,000 rows | 8,867 CS papers | 1121972 rows/sec\n",
      "  Chunk 2350: 235,000,000 rows | 9,105 CS papers | 1122501 rows/sec\n",
      "  Chunk 2400: 240,000,000 rows | 9,669 CS papers | 1122480 rows/sec\n",
      "  Chunk 2450: 245,000,000 rows | 10,197 CS papers | 1122502 rows/sec\n",
      "  Chunk 2500: 250,000,000 rows | 10,842 CS papers | 1122187 rows/sec\n",
      "  Chunk 2550: 255,000,000 rows | 11,553 CS papers | 1121839 rows/sec\n",
      "  Chunk 2600: 260,000,000 rows | 12,351 CS papers | 1121651 rows/sec\n",
      "  Chunk 2650: 265,000,000 rows | 13,122 CS papers | 1121557 rows/sec\n",
      "  Chunk 2700: 270,000,000 rows | 13,844 CS papers | 1121604 rows/sec\n",
      "  Chunk 2750: 275,000,000 rows | 14,555 CS papers | 1122278 rows/sec\n",
      "\n",
      "Step 3 Complete\n",
      "  Time: 247.1 seconds\n",
      "  Final VT CS papers: 8,627\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Step 3: Filtering for Computer Science Papers ===\\n\")\n",
    "print(\"Processing PaperFields.tsv (~11.62 GB)...\\n\")\n",
    "\n",
    "file_path = os.path.join(DATA_DIR, 'SciSciNet_PaperFields.tsv')\n",
    "final_cs_papers_set = set()\n",
    "\n",
    "chunk_count = 0\n",
    "total_rows = 0\n",
    "filtered_rows = 0\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Peek at columns\n",
    "sample = pd.read_csv(file_path, sep='\\t', nrows=1)\n",
    "pf_paper_col = None\n",
    "pf_field_col = None\n",
    "pf_score_col = None\n",
    "\n",
    "for col in sample.columns:\n",
    "    col_lower = col.lower()\n",
    "    if 'paper' in col_lower and 'id' in col_lower:\n",
    "        pf_paper_col = col\n",
    "    if 'field' in col_lower and 'id' in col_lower:\n",
    "        pf_field_col = col\n",
    "    if 'score' in col_lower:\n",
    "        pf_score_col = col\n",
    "\n",
    "print(f\"Using columns: paper='{pf_paper_col}', field='{pf_field_col}', score='{pf_score_col}'\\n\")\n",
    "\n",
    "for chunk in pd.read_csv(file_path, sep='\\t', chunksize=CHUNK_SIZE):\n",
    "    chunk_count += 1\n",
    "    total_rows += len(chunk)\n",
    "    \n",
    "    cs_chunk = chunk[\n",
    "        (chunk[pf_paper_col].isin(vt_papers_in_range_set)) &\n",
    "        (chunk[pf_field_col].isin(CS_FIELD_IDS))\n",
    "    ]\n",
    "    filtered_rows += len(cs_chunk)\n",
    "    \n",
    "    if len(cs_chunk) > 0:\n",
    "        final_cs_papers_set.update(cs_chunk[pf_paper_col].dropna().astype(int).values)\n",
    "        \n",
    "        cols_map = {\n",
    "            pf_paper_col: 'paper_id',\n",
    "            pf_field_col: 'field_id'\n",
    "        }\n",
    "        if pf_score_col:\n",
    "            cols_map[pf_score_col] = 'score'\n",
    "        \n",
    "        cs_chunk_clean = cs_chunk.rename(columns=cols_map)\n",
    "        keep_cols = [col for col in ['paper_id', 'field_id', 'score'] if col in cs_chunk_clean.columns]\n",
    "        cs_chunk_clean[keep_cols].to_sql('paper_fields', conn, if_exists='append', index=False)\n",
    "    \n",
    "    if chunk_count % 50 == 0:\n",
    "        elapsed = (datetime.now() - start_time).total_seconds()\n",
    "        rate = total_rows / elapsed if elapsed > 0 else 0\n",
    "        print(f\"  Chunk {chunk_count}: {total_rows:,} rows | {filtered_rows:,} CS papers | {rate:.0f} rows/sec\")\n",
    "\n",
    "elapsed_time = (datetime.now() - start_time).total_seconds()\n",
    "print(f\"\\nStep 3 Complete\")\n",
    "print(f\"  Time: {elapsed_time:.1f} seconds\")\n",
    "print(f\"  Final VT CS papers: {len(final_cs_papers_set):,}\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54ff1187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 4: Building Citation Network ===\n",
      "\n",
      "Processing PaperReferences.tsv (~32.41 GB)...\n",
      "\n",
      "Available columns: ['Citing_PaperID', 'Cited_PaperID']\n",
      "\n",
      "Detected columns:\n",
      "  Paper (citing): 'Citing_PaperID'\n",
      "  Reference (cited): 'Cited_PaperID'\n",
      "\n",
      "  Chunk 50: 5,000,000 rows | 43 citations | 2662146 rows/sec\n",
      "  Chunk 100: 10,000,000 rows | 45 citations | 2725149 rows/sec\n",
      "  Chunk 150: 15,000,000 rows | 123 citations | 2732161 rows/sec\n",
      "  Chunk 200: 20,000,000 rows | 128 citations | 2743326 rows/sec\n",
      "  Chunk 250: 25,000,000 rows | 144 citations | 2707973 rows/sec\n",
      "  Chunk 300: 30,000,000 rows | 154 citations | 2695193 rows/sec\n",
      "  Chunk 350: 35,000,000 rows | 158 citations | 2688024 rows/sec\n",
      "  Chunk 400: 40,000,000 rows | 244 citations | 2678727 rows/sec\n",
      "  Chunk 450: 45,000,000 rows | 488 citations | 2674036 rows/sec\n",
      "  Chunk 500: 50,000,000 rows | 839 citations | 2666302 rows/sec\n",
      "  Chunk 550: 55,000,000 rows | 1,236 citations | 2654279 rows/sec\n",
      "  Chunk 600: 60,000,000 rows | 1,950 citations | 2644270 rows/sec\n",
      "  Chunk 650: 65,000,000 rows | 2,675 citations | 2626244 rows/sec\n",
      "  Chunk 700: 70,000,000 rows | 3,073 citations | 2610646 rows/sec\n",
      "  Chunk 750: 75,000,000 rows | 3,326 citations | 2598667 rows/sec\n",
      "  Chunk 800: 80,000,000 rows | 3,577 citations | 2589955 rows/sec\n",
      "  Chunk 850: 85,000,000 rows | 3,761 citations | 2583370 rows/sec\n",
      "  Chunk 900: 90,000,000 rows | 3,990 citations | 2575065 rows/sec\n",
      "  Chunk 950: 95,000,000 rows | 4,324 citations | 2567705 rows/sec\n",
      "  Chunk 1000: 100,000,000 rows | 4,651 citations | 2561780 rows/sec\n",
      "  Chunk 1050: 105,000,000 rows | 5,050 citations | 2555711 rows/sec\n",
      "  Chunk 1100: 110,000,000 rows | 5,289 citations | 2550842 rows/sec\n",
      "  Chunk 1150: 115,000,000 rows | 5,588 citations | 2545960 rows/sec\n",
      "  Chunk 1200: 120,000,000 rows | 5,941 citations | 2540363 rows/sec\n",
      "  Chunk 1250: 125,000,000 rows | 6,242 citations | 2535115 rows/sec\n",
      "  Chunk 1300: 130,000,000 rows | 6,549 citations | 2530590 rows/sec\n",
      "  Chunk 1350: 135,000,000 rows | 6,906 citations | 2526315 rows/sec\n",
      "  Chunk 1400: 140,000,000 rows | 7,195 citations | 2522866 rows/sec\n",
      "  Chunk 1450: 145,000,000 rows | 7,745 citations | 2517037 rows/sec\n",
      "  Chunk 1500: 150,000,000 rows | 8,420 citations | 2510441 rows/sec\n",
      "  Chunk 1550: 155,000,000 rows | 9,378 citations | 2502964 rows/sec\n",
      "  Chunk 1600: 160,000,000 rows | 9,452 citations | 2503873 rows/sec\n",
      "  Chunk 1650: 165,000,000 rows | 9,579 citations | 2505469 rows/sec\n",
      "  Chunk 1700: 170,000,000 rows | 9,602 citations | 2507082 rows/sec\n",
      "  Chunk 1750: 175,000,000 rows | 9,709 citations | 2508159 rows/sec\n",
      "  Chunk 1800: 180,000,000 rows | 9,743 citations | 2509620 rows/sec\n",
      "  Chunk 1850: 185,000,000 rows | 9,758 citations | 2511389 rows/sec\n",
      "  Chunk 1900: 190,000,000 rows | 9,894 citations | 2512610 rows/sec\n",
      "  Chunk 1950: 195,000,000 rows | 9,968 citations | 2514184 rows/sec\n",
      "  Chunk 2000: 200,000,000 rows | 9,980 citations | 2514522 rows/sec\n",
      "  Chunk 2050: 205,000,000 rows | 10,114 citations | 2514585 rows/sec\n",
      "  Chunk 2100: 210,000,000 rows | 10,148 citations | 2513274 rows/sec\n",
      "  Chunk 2150: 215,000,000 rows | 10,200 citations | 2514925 rows/sec\n",
      "  Chunk 2200: 220,000,000 rows | 10,244 citations | 2515870 rows/sec\n",
      "  Chunk 2250: 225,000,000 rows | 10,375 citations | 2515635 rows/sec\n",
      "  Chunk 2300: 230,000,000 rows | 10,543 citations | 2516615 rows/sec\n",
      "  Chunk 2350: 235,000,000 rows | 10,662 citations | 2517376 rows/sec\n",
      "  Chunk 2400: 240,000,000 rows | 10,742 citations | 2518378 rows/sec\n",
      "  Chunk 2450: 245,000,000 rows | 10,816 citations | 2518724 rows/sec\n",
      "  Chunk 2500: 250,000,000 rows | 10,855 citations | 2519825 rows/sec\n",
      "  Chunk 2550: 255,000,000 rows | 10,981 citations | 2520628 rows/sec\n",
      "  Chunk 2600: 260,000,000 rows | 10,981 citations | 2522174 rows/sec\n",
      "  Chunk 2650: 265,000,000 rows | 11,065 citations | 2523125 rows/sec\n",
      "  Chunk 2700: 270,000,000 rows | 11,162 citations | 2523622 rows/sec\n",
      "  Chunk 2750: 275,000,000 rows | 11,214 citations | 2523885 rows/sec\n",
      "  Chunk 2800: 280,000,000 rows | 11,261 citations | 2524689 rows/sec\n",
      "  Chunk 2850: 285,000,000 rows | 11,285 citations | 2525485 rows/sec\n",
      "  Chunk 2900: 290,000,000 rows | 11,302 citations | 2526330 rows/sec\n",
      "  Chunk 2950: 295,000,000 rows | 11,315 citations | 2527242 rows/sec\n",
      "  Chunk 3000: 300,000,000 rows | 11,575 citations | 2528703 rows/sec\n",
      "  Chunk 3050: 305,000,000 rows | 11,611 citations | 2531475 rows/sec\n",
      "  Chunk 3100: 310,000,000 rows | 11,765 citations | 2533809 rows/sec\n",
      "  Chunk 3150: 315,000,000 rows | 11,850 citations | 2536040 rows/sec\n",
      "  Chunk 3200: 320,000,000 rows | 11,937 citations | 2537660 rows/sec\n",
      "  Chunk 3250: 325,000,000 rows | 12,006 citations | 2539243 rows/sec\n",
      "  Chunk 3300: 330,000,000 rows | 12,069 citations | 2540613 rows/sec\n",
      "  Chunk 3350: 335,000,000 rows | 12,144 citations | 2541889 rows/sec\n",
      "  Chunk 3400: 340,000,000 rows | 12,277 citations | 2543349 rows/sec\n",
      "  Chunk 3450: 345,000,000 rows | 12,323 citations | 2544738 rows/sec\n",
      "  Chunk 3500: 350,000,000 rows | 12,394 citations | 2545738 rows/sec\n",
      "  Chunk 3550: 355,000,000 rows | 12,398 citations | 2547334 rows/sec\n",
      "  Chunk 3600: 360,000,000 rows | 12,411 citations | 2547914 rows/sec\n",
      "  Chunk 3650: 365,000,000 rows | 12,476 citations | 2549090 rows/sec\n",
      "  Chunk 3700: 370,000,000 rows | 12,547 citations | 2550623 rows/sec\n",
      "  Chunk 3750: 375,000,000 rows | 12,667 citations | 2551717 rows/sec\n",
      "  Chunk 3800: 380,000,000 rows | 12,813 citations | 2552921 rows/sec\n",
      "  Chunk 3850: 385,000,000 rows | 12,885 citations | 2554103 rows/sec\n",
      "  Chunk 3900: 390,000,000 rows | 12,995 citations | 2555097 rows/sec\n",
      "  Chunk 3950: 395,000,000 rows | 13,026 citations | 2556438 rows/sec\n",
      "  Chunk 4000: 400,000,000 rows | 13,069 citations | 2557814 rows/sec\n",
      "  Chunk 4050: 405,000,000 rows | 13,239 citations | 2558838 rows/sec\n",
      "  Chunk 4100: 410,000,000 rows | 13,310 citations | 2559964 rows/sec\n",
      "  Chunk 4150: 415,000,000 rows | 13,361 citations | 2561301 rows/sec\n",
      "  Chunk 4200: 420,000,000 rows | 13,651 citations | 2562305 rows/sec\n",
      "  Chunk 4250: 425,000,000 rows | 13,653 citations | 2563769 rows/sec\n",
      "  Chunk 4300: 430,000,000 rows | 13,672 citations | 2565003 rows/sec\n",
      "  Chunk 4350: 435,000,000 rows | 13,801 citations | 2565718 rows/sec\n",
      "  Chunk 4400: 440,000,000 rows | 13,947 citations | 2566345 rows/sec\n",
      "  Chunk 4450: 445,000,000 rows | 13,979 citations | 2567465 rows/sec\n",
      "  Chunk 4500: 450,000,000 rows | 14,024 citations | 2568601 rows/sec\n",
      "  Chunk 4550: 455,000,000 rows | 14,070 citations | 2569711 rows/sec\n",
      "  Chunk 4600: 460,000,000 rows | 14,100 citations | 2571026 rows/sec\n",
      "  Chunk 4650: 465,000,000 rows | 14,168 citations | 2571842 rows/sec\n",
      "  Chunk 4700: 470,000,000 rows | 14,398 citations | 2572269 rows/sec\n",
      "  Chunk 4750: 475,000,000 rows | 14,734 citations | 2572724 rows/sec\n",
      "  Chunk 4800: 480,000,000 rows | 14,792 citations | 2573482 rows/sec\n",
      "  Chunk 4850: 485,000,000 rows | 14,843 citations | 2574299 rows/sec\n",
      "  Chunk 4900: 490,000,000 rows | 14,860 citations | 2575100 rows/sec\n",
      "  Chunk 4950: 495,000,000 rows | 15,003 citations | 2575799 rows/sec\n",
      "  Chunk 5000: 500,000,000 rows | 15,017 citations | 2576601 rows/sec\n",
      "  Chunk 5050: 505,000,000 rows | 15,159 citations | 2577144 rows/sec\n",
      "  Chunk 5100: 510,000,000 rows | 15,271 citations | 2577911 rows/sec\n",
      "  Chunk 5150: 515,000,000 rows | 15,396 citations | 2578405 rows/sec\n",
      "  Chunk 5200: 520,000,000 rows | 15,484 citations | 2578971 rows/sec\n",
      "  Chunk 5250: 525,000,000 rows | 15,544 citations | 2579596 rows/sec\n",
      "  Chunk 5300: 530,000,000 rows | 15,593 citations | 2579958 rows/sec\n",
      "  Chunk 5350: 535,000,000 rows | 15,615 citations | 2580783 rows/sec\n",
      "  Chunk 5400: 540,000,000 rows | 15,675 citations | 2581762 rows/sec\n",
      "  Chunk 5450: 545,000,000 rows | 15,765 citations | 2582453 rows/sec\n",
      "  Chunk 5500: 550,000,000 rows | 15,882 citations | 2583080 rows/sec\n",
      "  Chunk 5550: 555,000,000 rows | 15,901 citations | 2583519 rows/sec\n",
      "  Chunk 5600: 560,000,000 rows | 16,004 citations | 2584054 rows/sec\n",
      "  Chunk 5650: 565,000,000 rows | 16,045 citations | 2584883 rows/sec\n",
      "  Chunk 5700: 570,000,000 rows | 16,109 citations | 2585388 rows/sec\n",
      "  Chunk 5750: 575,000,000 rows | 16,180 citations | 2586068 rows/sec\n",
      "  Chunk 5800: 580,000,000 rows | 16,214 citations | 2586598 rows/sec\n",
      "  Chunk 5850: 585,000,000 rows | 16,368 citations | 2587162 rows/sec\n",
      "  Chunk 5900: 590,000,000 rows | 16,376 citations | 2587765 rows/sec\n",
      "  Chunk 5950: 595,000,000 rows | 16,407 citations | 2588478 rows/sec\n",
      "  Chunk 6000: 600,000,000 rows | 16,434 citations | 2588754 rows/sec\n",
      "  Chunk 6050: 605,000,000 rows | 16,450 citations | 2589097 rows/sec\n",
      "  Chunk 6100: 610,000,000 rows | 16,458 citations | 2589831 rows/sec\n",
      "  Chunk 6150: 615,000,000 rows | 16,610 citations | 2590126 rows/sec\n",
      "  Chunk 6200: 620,000,000 rows | 16,748 citations | 2590282 rows/sec\n",
      "  Chunk 6250: 625,000,000 rows | 16,979 citations | 2590735 rows/sec\n",
      "  Chunk 6300: 630,000,000 rows | 17,008 citations | 2591153 rows/sec\n",
      "  Chunk 6350: 635,000,000 rows | 17,091 citations | 2591546 rows/sec\n",
      "  Chunk 6400: 640,000,000 rows | 17,156 citations | 2592224 rows/sec\n",
      "  Chunk 6450: 645,000,000 rows | 17,284 citations | 2592531 rows/sec\n",
      "  Chunk 6500: 650,000,000 rows | 17,363 citations | 2592774 rows/sec\n",
      "  Chunk 6550: 655,000,000 rows | 17,599 citations | 2593016 rows/sec\n",
      "  Chunk 6600: 660,000,000 rows | 17,625 citations | 2593633 rows/sec\n",
      "  Chunk 6650: 665,000,000 rows | 17,870 citations | 2594224 rows/sec\n",
      "  Chunk 6700: 670,000,000 rows | 17,900 citations | 2594520 rows/sec\n",
      "  Chunk 6750: 675,000,000 rows | 18,160 citations | 2594879 rows/sec\n",
      "  Chunk 6800: 680,000,000 rows | 18,219 citations | 2595359 rows/sec\n",
      "  Chunk 6850: 685,000,000 rows | 18,429 citations | 2595797 rows/sec\n",
      "  Chunk 6900: 690,000,000 rows | 18,456 citations | 2596478 rows/sec\n",
      "  Chunk 6950: 695,000,000 rows | 18,487 citations | 2597091 rows/sec\n",
      "  Chunk 7000: 700,000,000 rows | 18,562 citations | 2597015 rows/sec\n",
      "  Chunk 7050: 705,000,000 rows | 18,661 citations | 2597484 rows/sec\n",
      "  Chunk 7100: 710,000,000 rows | 18,709 citations | 2597696 rows/sec\n",
      "  Chunk 7150: 715,000,000 rows | 18,791 citations | 2598008 rows/sec\n",
      "  Chunk 7200: 720,000,000 rows | 18,898 citations | 2598362 rows/sec\n",
      "  Chunk 7250: 725,000,000 rows | 18,920 citations | 2598653 rows/sec\n",
      "  Chunk 7300: 730,000,000 rows | 19,060 citations | 2599076 rows/sec\n",
      "  Chunk 7350: 735,000,000 rows | 19,280 citations | 2599279 rows/sec\n",
      "  Chunk 7400: 740,000,000 rows | 19,383 citations | 2599632 rows/sec\n",
      "  Chunk 7450: 745,000,000 rows | 19,533 citations | 2600009 rows/sec\n",
      "  Chunk 7500: 750,000,000 rows | 19,567 citations | 2600494 rows/sec\n",
      "  Chunk 7550: 755,000,000 rows | 19,714 citations | 2600641 rows/sec\n",
      "  Chunk 7600: 760,000,000 rows | 19,834 citations | 2601002 rows/sec\n",
      "  Chunk 7650: 765,000,000 rows | 19,837 citations | 2601519 rows/sec\n",
      "  Chunk 7700: 770,000,000 rows | 19,848 citations | 2601997 rows/sec\n",
      "  Chunk 7750: 775,000,000 rows | 20,099 citations | 2602205 rows/sec\n",
      "  Chunk 7800: 780,000,000 rows | 20,143 citations | 2602674 rows/sec\n",
      "  Chunk 7850: 785,000,000 rows | 20,150 citations | 2602749 rows/sec\n",
      "  Chunk 7900: 790,000,000 rows | 20,223 citations | 2602980 rows/sec\n",
      "  Chunk 7950: 795,000,000 rows | 20,324 citations | 2603068 rows/sec\n",
      "  Chunk 8000: 800,000,000 rows | 20,404 citations | 2603545 rows/sec\n",
      "  Chunk 8050: 805,000,000 rows | 20,404 citations | 2604183 rows/sec\n",
      "  Chunk 8100: 810,000,000 rows | 20,420 citations | 2604576 rows/sec\n",
      "  Chunk 8150: 815,000,000 rows | 20,425 citations | 2604898 rows/sec\n",
      "  Chunk 8200: 820,000,000 rows | 20,430 citations | 2605313 rows/sec\n",
      "  Chunk 8250: 825,000,000 rows | 20,478 citations | 2605667 rows/sec\n",
      "  Chunk 8300: 830,000,000 rows | 20,616 citations | 2605866 rows/sec\n",
      "  Chunk 8350: 835,000,000 rows | 20,696 citations | 2606109 rows/sec\n",
      "  Chunk 8400: 840,000,000 rows | 20,704 citations | 2606419 rows/sec\n",
      "  Chunk 8450: 845,000,000 rows | 20,773 citations | 2606354 rows/sec\n",
      "  Chunk 8500: 850,000,000 rows | 20,778 citations | 2606659 rows/sec\n",
      "  Chunk 8550: 855,000,000 rows | 20,905 citations | 2606734 rows/sec\n",
      "  Chunk 8600: 860,000,000 rows | 21,241 citations | 2606076 rows/sec\n",
      "  Chunk 8650: 865,000,000 rows | 21,910 citations | 2604021 rows/sec\n",
      "  Chunk 8700: 870,000,000 rows | 22,538 citations | 2602747 rows/sec\n",
      "  Chunk 8750: 875,000,000 rows | 23,523 citations | 2601128 rows/sec\n",
      "  Chunk 8800: 880,000,000 rows | 24,267 citations | 2599782 rows/sec\n",
      "  Chunk 8850: 885,000,000 rows | 24,995 citations | 2598424 rows/sec\n",
      "  Chunk 8900: 890,000,000 rows | 25,890 citations | 2597051 rows/sec\n",
      "  Chunk 8950: 895,000,000 rows | 26,879 citations | 2595879 rows/sec\n",
      "  Chunk 9000: 900,000,000 rows | 28,031 citations | 2594138 rows/sec\n",
      "  Chunk 9050: 905,000,000 rows | 29,455 citations | 2592214 rows/sec\n",
      "  Chunk 9100: 910,000,000 rows | 30,311 citations | 2590785 rows/sec\n",
      "  Chunk 9150: 915,000,000 rows | 31,029 citations | 2589946 rows/sec\n",
      "  Chunk 9200: 920,000,000 rows | 31,286 citations | 2589234 rows/sec\n",
      "  Chunk 9250: 925,000,000 rows | 31,725 citations | 2588387 rows/sec\n",
      "  Chunk 9300: 930,000,000 rows | 32,238 citations | 2587436 rows/sec\n",
      "  Chunk 9350: 935,000,000 rows | 32,718 citations | 2586798 rows/sec\n",
      "  Chunk 9400: 940,000,000 rows | 33,591 citations | 2585302 rows/sec\n",
      "  Chunk 9450: 945,000,000 rows | 35,055 citations | 2583594 rows/sec\n",
      "  Chunk 9500: 950,000,000 rows | 35,939 citations | 2582146 rows/sec\n",
      "  Chunk 9550: 955,000,000 rows | 37,053 citations | 2580685 rows/sec\n",
      "  Chunk 9600: 960,000,000 rows | 37,719 citations | 2579167 rows/sec\n",
      "  Chunk 9650: 965,000,000 rows | 38,698 citations | 2577716 rows/sec\n",
      "  Chunk 9700: 970,000,000 rows | 39,748 citations | 2576163 rows/sec\n",
      "  Chunk 9750: 975,000,000 rows | 40,582 citations | 2574918 rows/sec\n",
      "  Chunk 9800: 980,000,000 rows | 41,029 citations | 2573768 rows/sec\n",
      "  Chunk 9850: 985,000,000 rows | 41,559 citations | 2572532 rows/sec\n",
      "  Chunk 9900: 990,000,000 rows | 42,347 citations | 2571332 rows/sec\n",
      "  Chunk 9950: 995,000,000 rows | 43,529 citations | 2569875 rows/sec\n",
      "  Chunk 10000: 1,000,000,000 rows | 45,401 citations | 2568232 rows/sec\n",
      "  Chunk 10050: 1,005,000,000 rows | 46,743 citations | 2566810 rows/sec\n",
      "  Chunk 10100: 1,010,000,000 rows | 48,398 citations | 2565360 rows/sec\n",
      "  Chunk 10150: 1,015,000,000 rows | 49,709 citations | 2563925 rows/sec\n",
      "  Chunk 10200: 1,020,000,000 rows | 51,247 citations | 2562303 rows/sec\n",
      "  Chunk 10250: 1,025,000,000 rows | 53,055 citations | 2560699 rows/sec\n",
      "  Chunk 10300: 1,030,000,000 rows | 54,617 citations | 2559228 rows/sec\n",
      "  Chunk 10350: 1,035,000,000 rows | 56,276 citations | 2557634 rows/sec\n",
      "  Chunk 10400: 1,040,000,000 rows | 57,952 citations | 2556243 rows/sec\n",
      "  Chunk 10450: 1,045,000,000 rows | 59,871 citations | 2554826 rows/sec\n",
      "  Chunk 10500: 1,050,000,000 rows | 61,247 citations | 2553475 rows/sec\n",
      "  Chunk 10550: 1,055,000,000 rows | 62,822 citations | 2552088 rows/sec\n",
      "  Chunk 10600: 1,060,000,000 rows | 65,167 citations | 2550666 rows/sec\n",
      "  Chunk 10650: 1,065,000,000 rows | 66,980 citations | 2549288 rows/sec\n",
      "  Chunk 10700: 1,070,000,000 rows | 68,431 citations | 2547728 rows/sec\n",
      "  Chunk 10750: 1,075,000,000 rows | 69,955 citations | 2546428 rows/sec\n",
      "  Chunk 10800: 1,080,000,000 rows | 71,500 citations | 2545056 rows/sec\n",
      "  Chunk 10850: 1,085,000,000 rows | 73,065 citations | 2543704 rows/sec\n",
      "  Chunk 10900: 1,090,000,000 rows | 74,635 citations | 2542401 rows/sec\n",
      "  Chunk 10950: 1,095,000,000 rows | 77,052 citations | 2541140 rows/sec\n",
      "  Chunk 11000: 1,100,000,000 rows | 79,510 citations | 2539738 rows/sec\n",
      "  Chunk 11050: 1,105,000,000 rows | 81,800 citations | 2538417 rows/sec\n",
      "  Chunk 11100: 1,110,000,000 rows | 84,216 citations | 2537059 rows/sec\n",
      "  Chunk 11150: 1,115,000,000 rows | 86,727 citations | 2535617 rows/sec\n",
      "  Chunk 11200: 1,120,000,000 rows | 89,657 citations | 2506491 rows/sec\n",
      "  Chunk 11250: 1,125,000,000 rows | 91,918 citations | 2505418 rows/sec\n",
      "  Chunk 11300: 1,130,000,000 rows | 93,846 citations | 2504217 rows/sec\n",
      "  Chunk 11350: 1,135,000,000 rows | 95,372 citations | 2502901 rows/sec\n",
      "  Chunk 11400: 1,140,000,000 rows | 97,564 citations | 2501721 rows/sec\n",
      "  Chunk 11450: 1,145,000,000 rows | 99,937 citations | 2500569 rows/sec\n",
      "  Chunk 11500: 1,150,000,000 rows | 101,500 citations | 2499492 rows/sec\n",
      "  Chunk 11550: 1,155,000,000 rows | 103,475 citations | 2498215 rows/sec\n",
      "  Chunk 11600: 1,160,000,000 rows | 105,874 citations | 2497037 rows/sec\n",
      "  Chunk 11650: 1,165,000,000 rows | 108,135 citations | 2496008 rows/sec\n",
      "  Chunk 11700: 1,170,000,000 rows | 111,166 citations | 2494893 rows/sec\n",
      "  Chunk 11750: 1,175,000,000 rows | 113,456 citations | 2493699 rows/sec\n",
      "  Chunk 11800: 1,180,000,000 rows | 115,567 citations | 2492670 rows/sec\n",
      "  Chunk 11850: 1,185,000,000 rows | 117,758 citations | 2491616 rows/sec\n",
      "  Chunk 11900: 1,190,000,000 rows | 120,221 citations | 2490414 rows/sec\n",
      "  Chunk 11950: 1,195,000,000 rows | 122,358 citations | 2489373 rows/sec\n",
      "  Chunk 12000: 1,200,000,000 rows | 124,822 citations | 2488297 rows/sec\n",
      "  Chunk 12050: 1,205,000,000 rows | 127,862 citations | 2487232 rows/sec\n",
      "  Chunk 12100: 1,210,000,000 rows | 130,779 citations | 2486235 rows/sec\n",
      "  Chunk 12150: 1,215,000,000 rows | 133,092 citations | 2485299 rows/sec\n",
      "  Chunk 12200: 1,220,000,000 rows | 135,159 citations | 2484301 rows/sec\n",
      "  Chunk 12250: 1,225,000,000 rows | 137,989 citations | 2483442 rows/sec\n",
      "  Chunk 12300: 1,230,000,000 rows | 140,181 citations | 2482586 rows/sec\n",
      "  Chunk 12350: 1,235,000,000 rows | 142,273 citations | 2481768 rows/sec\n",
      "  Chunk 12400: 1,240,000,000 rows | 144,462 citations | 2338701 rows/sec\n",
      "  Chunk 12450: 1,245,000,000 rows | 146,880 citations | 2338571 rows/sec\n",
      "  Chunk 12500: 1,250,000,000 rows | 149,507 citations | 2338338 rows/sec\n",
      "  Chunk 12550: 1,255,000,000 rows | 152,039 citations | 2338123 rows/sec\n",
      "  Chunk 12600: 1,260,000,000 rows | 154,497 citations | 2337883 rows/sec\n",
      "  Chunk 12650: 1,265,000,000 rows | 161,632 citations | 2337447 rows/sec\n",
      "  Chunk 12700: 1,270,000,000 rows | 165,728 citations | 2337173 rows/sec\n",
      "  Chunk 12750: 1,275,000,000 rows | 168,576 citations | 2336928 rows/sec\n",
      "  Chunk 12800: 1,280,000,000 rows | 171,198 citations | 2336696 rows/sec\n",
      "  Chunk 12850: 1,285,000,000 rows | 173,931 citations | 2336444 rows/sec\n",
      "  Chunk 12900: 1,290,000,000 rows | 176,697 citations | 2336223 rows/sec\n",
      "  Chunk 12950: 1,295,000,000 rows | 179,611 citations | 2335971 rows/sec\n",
      "  Chunk 13000: 1,300,000,000 rows | 182,667 citations | 2335724 rows/sec\n",
      "  Chunk 13050: 1,305,000,000 rows | 184,793 citations | 2335530 rows/sec\n",
      "  Chunk 13100: 1,310,000,000 rows | 188,737 citations | 2335252 rows/sec\n",
      "  Chunk 13150: 1,315,000,000 rows | 191,526 citations | 2335047 rows/sec\n",
      "  Chunk 13200: 1,320,000,000 rows | 194,509 citations | 2334799 rows/sec\n",
      "  Chunk 13250: 1,325,000,000 rows | 196,806 citations | 2334599 rows/sec\n",
      "  Chunk 13300: 1,330,000,000 rows | 200,133 citations | 2334350 rows/sec\n",
      "  Chunk 13350: 1,335,000,000 rows | 203,325 citations | 2334073 rows/sec\n",
      "  Chunk 13400: 1,340,000,000 rows | 206,341 citations | 2333829 rows/sec\n",
      "  Chunk 13450: 1,345,000,000 rows | 209,131 citations | 2333660 rows/sec\n",
      "  Chunk 13500: 1,350,000,000 rows | 210,939 citations | 2333397 rows/sec\n",
      "  Chunk 13550: 1,355,000,000 rows | 213,795 citations | 2333220 rows/sec\n",
      "  Chunk 13600: 1,360,000,000 rows | 216,364 citations | 2333132 rows/sec\n",
      "  Chunk 13650: 1,365,000,000 rows | 219,158 citations | 2332945 rows/sec\n",
      "  Chunk 13700: 1,370,000,000 rows | 221,554 citations | 2332814 rows/sec\n",
      "  Chunk 13750: 1,375,000,000 rows | 223,814 citations | 2332662 rows/sec\n",
      "  Chunk 13800: 1,380,000,000 rows | 226,447 citations | 2332524 rows/sec\n",
      "  Chunk 13850: 1,385,000,000 rows | 228,817 citations | 2332346 rows/sec\n",
      "  Chunk 13900: 1,390,000,000 rows | 231,296 citations | 2332157 rows/sec\n",
      "  Chunk 13950: 1,395,000,000 rows | 233,439 citations | 2332033 rows/sec\n",
      "  Chunk 14000: 1,400,000,000 rows | 236,571 citations | 2331815 rows/sec\n",
      "  Chunk 14050: 1,405,000,000 rows | 240,254 citations | 2331563 rows/sec\n",
      "  Chunk 14100: 1,410,000,000 rows | 243,029 citations | 2331343 rows/sec\n",
      "  Chunk 14150: 1,415,000,000 rows | 245,771 citations | 2331120 rows/sec\n",
      "  Chunk 14200: 1,420,000,000 rows | 247,654 citations | 2330940 rows/sec\n",
      "  Chunk 14250: 1,425,000,000 rows | 249,364 citations | 2330757 rows/sec\n",
      "  Chunk 14300: 1,430,000,000 rows | 256,418 citations | 2330379 rows/sec\n",
      "  Chunk 14350: 1,435,000,000 rows | 259,951 citations | 2330178 rows/sec\n",
      "  Chunk 14400: 1,440,000,000 rows | 263,316 citations | 2329966 rows/sec\n",
      "  Chunk 14450: 1,445,000,000 rows | 266,260 citations | 2329826 rows/sec\n",
      "  Chunk 14500: 1,450,000,000 rows | 268,126 citations | 2329727 rows/sec\n",
      "  Chunk 14550: 1,455,000,000 rows | 270,507 citations | 2329535 rows/sec\n",
      "  Chunk 14600: 1,460,000,000 rows | 274,511 citations | 2329330 rows/sec\n",
      "  Chunk 14650: 1,465,000,000 rows | 278,024 citations | 2329142 rows/sec\n",
      "  Chunk 14700: 1,470,000,000 rows | 280,870 citations | 2328975 rows/sec\n",
      "  Chunk 14750: 1,475,000,000 rows | 282,844 citations | 2328839 rows/sec\n",
      "  Chunk 14800: 1,480,000,000 rows | 285,240 citations | 2328749 rows/sec\n",
      "  Chunk 14850: 1,485,000,000 rows | 288,760 citations | 2328538 rows/sec\n",
      "  Chunk 14900: 1,490,000,000 rows | 291,263 citations | 2328382 rows/sec\n",
      "  Chunk 14950: 1,495,000,000 rows | 294,399 citations | 2328128 rows/sec\n",
      "  Chunk 15000: 1,500,000,000 rows | 297,148 citations | 2327901 rows/sec\n",
      "  Chunk 15050: 1,505,000,000 rows | 299,930 citations | 2327630 rows/sec\n",
      "  Chunk 15100: 1,510,000,000 rows | 302,850 citations | 2327415 rows/sec\n",
      "  Chunk 15150: 1,515,000,000 rows | 305,881 citations | 2220792 rows/sec\n",
      "  Chunk 15200: 1,520,000,000 rows | 308,989 citations | 2220932 rows/sec\n",
      "  Chunk 15250: 1,525,000,000 rows | 312,409 citations | 2221064 rows/sec\n",
      "  Chunk 15300: 1,530,000,000 rows | 315,930 citations | 2221245 rows/sec\n",
      "  Chunk 15350: 1,535,000,000 rows | 318,506 citations | 2221360 rows/sec\n",
      "  Chunk 15400: 1,540,000,000 rows | 321,404 citations | 2221523 rows/sec\n",
      "  Chunk 15450: 1,545,000,000 rows | 324,207 citations | 2221698 rows/sec\n",
      "  Chunk 15500: 1,550,000,000 rows | 327,321 citations | 2221854 rows/sec\n",
      "  Chunk 15550: 1,555,000,000 rows | 330,481 citations | 2222033 rows/sec\n",
      "  Chunk 15600: 1,560,000,000 rows | 333,521 citations | 2222160 rows/sec\n",
      "  Chunk 15650: 1,565,000,000 rows | 336,793 citations | 2222285 rows/sec\n",
      "  Chunk 15700: 1,570,000,000 rows | 337,192 citations | 2222341 rows/sec\n",
      "  Chunk 15750: 1,575,000,000 rows | 337,370 citations | 2222544 rows/sec\n",
      "  Chunk 15800: 1,580,000,000 rows | 338,474 citations | 2222741 rows/sec\n",
      "  Chunk 15850: 1,585,000,000 rows | 346,595 citations | 2221960 rows/sec\n",
      "\n",
      "Step 4 Complete\n",
      "  Time: 715.2 seconds\n",
      "  Citation links: 354,180\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Step 4: Building Citation Network ===\\n\")\n",
    "print(\"Processing PaperReferences.tsv (~32.41 GB)...\\n\")\n",
    "\n",
    "file_path = os.path.join(DATA_DIR, 'SciSciNet_PaperReferences.tsv')\n",
    "\n",
    "chunk_count = 0\n",
    "total_rows = 0\n",
    "filtered_rows = 0\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Peek at columns\n",
    "sample = pd.read_csv(file_path, sep='\\t', nrows=1)\n",
    "print(f\"Available columns: {sample.columns.tolist()}\\n\")\n",
    "\n",
    "pr_paper_col = None\n",
    "pr_ref_col = None\n",
    "\n",
    "# Try to identify the paper column (citing paper)\n",
    "for col in sample.columns:\n",
    "    col_lower = col.lower().replace(' ', '').replace('_', '')\n",
    "    # Look for main paper ID column (the one doing the citing)\n",
    "    if col_lower in ['paperid', 'citingpaperid']:\n",
    "        pr_paper_col = col\n",
    "        break\n",
    "    if 'paper' in col_lower and 'id' in col_lower and 'cited' not in col_lower and 'reference' not in col_lower:\n",
    "        pr_paper_col = col\n",
    "\n",
    "# Try to identify the reference column (cited paper)\n",
    "for col in sample.columns:\n",
    "    col_lower = col.lower().replace(' ', '').replace('_', '')\n",
    "    # Look for cited/reference paper ID column\n",
    "    if col_lower in ['referenceid', 'citedpaperid', 'referencepaperid']:\n",
    "        pr_ref_col = col\n",
    "        break\n",
    "    if ('cited' in col_lower or 'reference' in col_lower) and 'id' in col_lower:\n",
    "        pr_ref_col = col\n",
    "\n",
    "print(f\"Detected columns:\")\n",
    "print(f\"  Paper (citing): '{pr_paper_col}'\")\n",
    "print(f\"  Reference (cited): '{pr_ref_col}'\\n\")\n",
    "\n",
    "# Validate that both columns were found\n",
    "if pr_paper_col is None or pr_ref_col is None:\n",
    "    raise ValueError(\n",
    "        f\"Could not identify paper or reference columns!\\n\"\n",
    "        f\"Available columns: {sample.columns.tolist()}\\n\"\n",
    "        f\"Please manually specify the correct column names.\"\n",
    "    )\n",
    "\n",
    "# Process the file in chunks\n",
    "for chunk in pd.read_csv(file_path, sep='\\t', chunksize=CHUNK_SIZE):\n",
    "    chunk_count += 1\n",
    "    total_rows += len(chunk)\n",
    "    \n",
    "    # Filter for citations involving our CS papers\n",
    "    # Include citations where either the citing paper OR cited paper is in our set\n",
    "    filtered_chunk = chunk[\n",
    "        (chunk[pr_paper_col].isin(final_cs_papers_set)) |\n",
    "        (chunk[pr_ref_col].isin(final_cs_papers_set))\n",
    "    ]\n",
    "    filtered_rows += len(filtered_chunk)\n",
    "    \n",
    "    if len(filtered_chunk) > 0:\n",
    "        filtered_chunk_clean = filtered_chunk.rename(columns={\n",
    "            pr_paper_col: 'paper_id',\n",
    "            pr_ref_col: 'reference_id'\n",
    "        })[['paper_id', 'reference_id']]\n",
    "        \n",
    "        filtered_chunk_clean.to_sql('paper_references', conn, if_exists='append', index=False)\n",
    "    \n",
    "    if chunk_count % 50 == 0:\n",
    "        elapsed = (datetime.now() - start_time).total_seconds()\n",
    "        rate = total_rows / elapsed if elapsed > 0 else 0\n",
    "        print(f\"  Chunk {chunk_count}: {total_rows:,} rows | {filtered_rows:,} citations | {rate:.0f} rows/sec\")\n",
    "\n",
    "elapsed_time = (datetime.now() - start_time).total_seconds()\n",
    "print(f\"\\nStep 4 Complete\")\n",
    "print(f\"  Time: {elapsed_time:.1f} seconds\")\n",
    "print(f\"  Citation links: {filtered_rows:,}\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6797667b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 5: Adding Paper Details ===\n",
      "\n",
      "Processing PaperDetails.tsv (~28.72 GB)...\n",
      "\n",
      "Available columns: ['PaperID', 'DOI', 'DocType', 'PaperTitle', 'BookTitle', 'Year', 'Date', 'Publisher', 'JournalID', 'ConferenceSeriesID', 'Volume', 'Issue', 'FirstPage', 'LastPage', 'ReferenceCount', 'CitationCount', 'OriginalVenue', 'FamilyID', 'RetractionType']\n",
      "\n",
      "Extracting columns: ['PaperID']\n",
      "\n",
      "  Chunk 30: 3,000,000 rows | 179 details | 384195 rows/sec\n",
      "  Chunk 60: 6,000,000 rows | 383 details | 379934 rows/sec\n",
      "  Chunk 90: 9,000,000 rows | 582 details | 378352 rows/sec\n",
      "  Chunk 120: 12,000,000 rows | 749 details | 377865 rows/sec\n",
      "  Chunk 150: 15,000,000 rows | 933 details | 377949 rows/sec\n",
      "  Chunk 180: 18,000,000 rows | 1,127 details | 377598 rows/sec\n",
      "  Chunk 210: 21,000,000 rows | 1,327 details | 377392 rows/sec\n",
      "  Chunk 240: 24,000,000 rows | 1,511 details | 377278 rows/sec\n",
      "  Chunk 270: 27,000,000 rows | 1,705 details | 377361 rows/sec\n",
      "  Chunk 300: 30,000,000 rows | 1,904 details | 377354 rows/sec\n",
      "  Chunk 330: 33,000,000 rows | 2,124 details | 377104 rows/sec\n",
      "  Chunk 360: 36,000,000 rows | 2,299 details | 376760 rows/sec\n",
      "  Chunk 390: 39,000,000 rows | 2,509 details | 292043 rows/sec\n",
      "  Chunk 420: 42,000,000 rows | 2,692 details | 296738 rows/sec\n",
      "  Chunk 450: 45,000,000 rows | 2,874 details | 300883 rows/sec\n",
      "  Chunk 480: 48,000,000 rows | 3,058 details | 304559 rows/sec\n",
      "  Chunk 510: 51,000,000 rows | 3,279 details | 308037 rows/sec\n",
      "  Chunk 540: 54,000,000 rows | 3,471 details | 311117 rows/sec\n",
      "  Chunk 570: 57,000,000 rows | 3,657 details | 313970 rows/sec\n",
      "  Chunk 600: 60,000,000 rows | 3,839 details | 316595 rows/sec\n",
      "  Chunk 630: 63,000,000 rows | 4,018 details | 319102 rows/sec\n",
      "  Chunk 660: 66,000,000 rows | 4,207 details | 321392 rows/sec\n",
      "  Chunk 690: 69,000,000 rows | 4,405 details | 323444 rows/sec\n",
      "  Chunk 720: 72,000,000 rows | 4,603 details | 325391 rows/sec\n",
      "  Chunk 750: 75,000,000 rows | 4,802 details | 327223 rows/sec\n",
      "  Chunk 780: 78,000,000 rows | 4,989 details | 328919 rows/sec\n",
      "  Chunk 810: 81,000,000 rows | 5,169 details | 330474 rows/sec\n",
      "  Chunk 840: 84,000,000 rows | 5,332 details | 331948 rows/sec\n",
      "  Chunk 870: 87,000,000 rows | 5,520 details | 333351 rows/sec\n",
      "  Chunk 900: 90,000,000 rows | 5,689 details | 334678 rows/sec\n",
      "  Chunk 930: 93,000,000 rows | 5,895 details | 335864 rows/sec\n",
      "  Chunk 960: 96,000,000 rows | 6,080 details | 337016 rows/sec\n",
      "  Chunk 990: 99,000,000 rows | 6,287 details | 338108 rows/sec\n",
      "  Chunk 1020: 102,000,000 rows | 6,474 details | 339170 rows/sec\n",
      "  Chunk 1050: 105,000,000 rows | 6,659 details | 340132 rows/sec\n",
      "  Chunk 1080: 108,000,000 rows | 6,862 details | 340997 rows/sec\n",
      "  Chunk 1110: 111,000,000 rows | 7,040 details | 341857 rows/sec\n",
      "  Chunk 1140: 114,000,000 rows | 7,232 details | 342701 rows/sec\n",
      "  Chunk 1170: 117,000,000 rows | 7,419 details | 343439 rows/sec\n",
      "  Chunk 1200: 120,000,000 rows | 7,596 details | 344121 rows/sec\n",
      "  Chunk 1230: 123,000,000 rows | 7,762 details | 344795 rows/sec\n",
      "  Chunk 1260: 126,000,000 rows | 7,948 details | 345461 rows/sec\n",
      "  Chunk 1290: 129,000,000 rows | 8,144 details | 345998 rows/sec\n",
      "  Chunk 1320: 132,000,000 rows | 8,330 details | 346573 rows/sec\n",
      "  Chunk 1350: 135,000,000 rows | 8,532 details | 347141 rows/sec\n",
      "\n",
      "Step 5 Complete\n",
      "  Time: 393.5 seconds\n",
      "  Paper details added: 8,627\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Step 5: Adding Paper Details ===\\n\")\n",
    "print(\"Processing PaperDetails.tsv (~28.72 GB)...\\n\")\n",
    "\n",
    "file_path = os.path.join(DATA_DIR, 'SciSciNet_PaperDetails.tsv')\n",
    "\n",
    "chunk_count = 0\n",
    "total_rows = 0\n",
    "filtered_rows = 0\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Peek at columns\n",
    "sample = pd.read_csv(file_path, sep='\\t', nrows=1)\n",
    "available_cols = sample.columns.tolist()\n",
    "print(f\"Available columns: {available_cols}\\n\")\n",
    "\n",
    "usecols = [PAPER_ID_COL]\n",
    "abstract_col = None\n",
    "\n",
    "for col in ['Abstract', 'OriginalAbstract']:\n",
    "    if col in available_cols:\n",
    "        abstract_col = col\n",
    "        usecols.append(col)\n",
    "        break\n",
    "\n",
    "\n",
    "print(f\"Extracting columns: {usecols}\\n\")\n",
    "\n",
    "for chunk in pd.read_csv(file_path, sep='\\t', chunksize=CHUNK_SIZE, usecols=usecols):\n",
    "    chunk_count += 1\n",
    "    total_rows += len(chunk)\n",
    "    \n",
    "    filtered_chunk = chunk[chunk[PAPER_ID_COL].isin(final_cs_papers_set)]\n",
    "    filtered_rows += len(filtered_chunk)\n",
    "    \n",
    "    if len(filtered_chunk) > 0:\n",
    "        cols_map = {PAPER_ID_COL: 'paper_id'}\n",
    "        if abstract_col:\n",
    "            cols_map[abstract_col] = 'abstract'\n",
    "        # FIX: Removed patent_col from map\n",
    "        \n",
    "        filtered_chunk_clean = filtered_chunk.rename(columns=cols_map)\n",
    "        filtered_chunk_clean.to_sql('paper_details', conn, if_exists='append', index=False)\n",
    "    \n",
    "    if chunk_count % 30 == 0:\n",
    "        elapsed = (datetime.now() - start_time).total_seconds()\n",
    "        rate = total_rows / elapsed if elapsed > 0 else 0\n",
    "        print(f\"  Chunk {chunk_count}: {total_rows:,} rows | {filtered_rows:,} details | {rate:.0f} rows/sec\")\n",
    "\n",
    "elapsed_time = (datetime.now() - start_time).total_seconds()\n",
    "print(f\"\\nStep 5 Complete\")\n",
    "print(f\"  Time: {elapsed_time:.1f} seconds\")\n",
    "print(f\"  Paper details added: {filtered_rows:,}\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de46cbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Creating Database Indexes ===\n",
      "\n",
      "[OK] idx_papers_year\n",
      "[OK] idx_papers_id\n",
      "[OK] idx_paa_paper\n",
      "[OK] idx_paa_author\n",
      "[OK] idx_paa_affil\n",
      "[OK] idx_pf_paper\n",
      "[OK] idx_pf_field\n",
      "[OK] idx_pr_paper\n",
      "[OK] idx_pr_ref\n",
      "[OK] idx_pd_paper\n",
      "\n",
      "Indexes created\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Creating Database Indexes ===\\n\")\n",
    "\n",
    "indexes = [\n",
    "    (\"idx_papers_year\", \"CREATE INDEX idx_papers_year ON papers(year)\"),\n",
    "    (\"idx_papers_id\", \"CREATE INDEX idx_papers_id ON papers(paper_id)\"),\n",
    "    (\"idx_paa_paper\", \"CREATE INDEX idx_paa_paper ON paper_author_affiliations(paper_id)\"),\n",
    "    (\"idx_paa_author\", \"CREATE INDEX idx_paa_author ON paper_author_affiliations(author_id)\"),\n",
    "    (\"idx_paa_affil\", \"CREATE INDEX idx_paa_affil ON paper_author_affiliations(affiliation_id)\"),\n",
    "    (\"idx_pf_paper\", \"CREATE INDEX idx_pf_paper ON paper_fields(paper_id)\"),\n",
    "    (\"idx_pf_field\", \"CREATE INDEX idx_pf_field ON paper_fields(field_id)\"),\n",
    "    (\"idx_pr_paper\", \"CREATE INDEX idx_pr_paper ON paper_references(paper_id)\"),\n",
    "    (\"idx_pr_ref\", \"CREATE INDEX idx_pr_ref ON paper_references(reference_id)\"),\n",
    "    (\"idx_pd_paper\", \"CREATE INDEX idx_pd_paper ON paper_details(paper_id)\")\n",
    "]\n",
    "\n",
    "for idx_name, idx_sql in indexes:\n",
    "    try:\n",
    "        cursor.execute(idx_sql)\n",
    "        print(f\"[OK] {idx_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {idx_name}: {e}\")\n",
    "\n",
    "conn.commit()\n",
    "print(\"\\nIndexes created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "981c16b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================================================\n",
      "DATABASE SUMMARY\n",
      "========================================================================================================================\n",
      "\n",
      "Row Counts:\n",
      "\n",
      "  Affiliations                                   1 rows\n",
      "  Fields                                        39 rows\n",
      "  Papers                                    32,674 rows\n",
      "  Paper Details                              8,627 rows\n",
      "  Paper-Author-Affiliations                193,408 rows\n",
      "  Paper-Fields                              14,947 rows\n",
      "  Citations                                354,180 rows\n",
      "\n",
      "Papers by Year:\n",
      "\n",
      " year  count\n",
      " 2015   4190\n",
      " 2016   4193\n",
      " 2017   4495\n",
      " 2018   4439\n",
      " 2019   4845\n",
      " 2020   5096\n",
      " 2021   5307\n",
      " 2022    109\n",
      "\n",
      "Citation Network:\n",
      "\n",
      " total_citation_links  papers_that_cite  papers_cited\n",
      "               354180             97927        176480\n",
      "\n",
      "Author Statistics:\n",
      "\n",
      " unique_authors  papers_with_authors  avg_authors_per_paper\n",
      "          42152                94577                   2.04\n",
      "\n",
      "Patent Statistics:\n",
      "\n",
      " papers_with_patent_data  papers_with_patents  avg_patent_count  max_patent_count\n",
      "                   32674                  555              0.03                24\n",
      "\n",
      "Database File:\n",
      "  Location: /ssd/Personal/sciscinet/full-stack/sciscinet-full-stack-backend/preprocessing/sciscinet_vt_cs.db\n",
      "  Size: 44.71 MB\n",
      "\n",
      "========================================================================================================================\n",
      "PREPROCESSING COMPLETE\n",
      "========================================================================================================================\n",
      "\n",
      "Database ready for both projects:\n",
      "\n",
      "Project 1 - Full-Stack Web Development:\n",
      "  T1 (Citation & Collaboration Networks): 2020-2025 (past 5 years)\n",
      "  T2 (Timeline + Patent Distribution): 2015-2025 (past 10 years)\n",
      "  T3 (Network Refinement): Uses T1 data\n",
      "\n",
      "Project 2 - LLM Agent Analysis:\n",
      "  Uses all VT CS papers from 2015-2025\n",
      "  All tables filtered for VT + CS + year range\n",
      "\n",
      "Database location: /ssd/Personal/sciscinet/full-stack/sciscinet-full-stack-backend/preprocessing/sciscinet_vt_cs.db\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"DATABASE SUMMARY\")\n",
    "print(\"=\"*120 + \"\\n\")\n",
    "\n",
    "tables = [\n",
    "    ('affiliations', 'Affiliations'),\n",
    "    ('fields', 'Fields'),\n",
    "    ('papers', 'Papers'),\n",
    "    ('paper_details', 'Paper Details'),\n",
    "    ('paper_author_affiliations', 'Paper-Author-Affiliations'),\n",
    "    ('paper_fields', 'Paper-Fields'),\n",
    "    ('paper_references', 'Citations')\n",
    "]\n",
    "\n",
    "print(\"Row Counts:\\n\")\n",
    "for table, display_name in tables:\n",
    "    count = pd.read_sql(f\"SELECT COUNT(*) as count FROM {table}\", conn).iloc[0]['count']\n",
    "    print(f\"  {display_name:35} {count:>12,} rows\")\n",
    "\n",
    "print(\"\\nPapers by Year:\\n\")\n",
    "papers_by_year = pd.read_sql(\"\"\"\n",
    "    SELECT year, COUNT(*) as count \n",
    "    FROM papers \n",
    "    GROUP BY year \n",
    "    ORDER BY year\n",
    "\"\"\", conn)\n",
    "print(papers_by_year.to_string(index=False))\n",
    "\n",
    "print(\"\\nCitation Network:\\n\")\n",
    "citation_stats = pd.read_sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_citation_links,\n",
    "        COUNT(DISTINCT paper_id) as papers_that_cite,\n",
    "        COUNT(DISTINCT reference_id) as papers_cited\n",
    "    FROM paper_references\n",
    "\"\"\", conn)\n",
    "print(citation_stats.to_string(index=False))\n",
    "\n",
    "print(\"\\nAuthor Statistics:\\n\")\n",
    "author_stats = pd.read_sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(DISTINCT author_id) as unique_authors,\n",
    "        COUNT(DISTINCT paper_id) as papers_with_authors,\n",
    "        ROUND(CAST(COUNT(*) AS FLOAT) / COUNT(DISTINCT paper_id), 2) as avg_authors_per_paper\n",
    "    FROM paper_author_affiliations\n",
    "\"\"\", conn)\n",
    "print(author_stats.to_string(index=False))\n",
    "\n",
    "print(\"\\nPatent Statistics:\\n\")\n",
    "# FIX: Changed query to read from 'papers' table\n",
    "patent_stats = pd.read_sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(patent_count) as papers_with_patent_data,\n",
    "        SUM(CASE WHEN patent_count > 0 THEN 1 ELSE 0 END) as papers_with_patents,\n",
    "        ROUND(AVG(patent_count), 2) as avg_patent_count,\n",
    "        MAX(patent_count) as max_patent_count\n",
    "    FROM papers\n",
    "    WHERE patent_count IS NOT NULL\n",
    "\"\"\", conn)\n",
    "print(patent_stats.to_string(index=False))\n",
    "\n",
    "db_size_mb = os.path.getsize(DB_PATH) / (1024**2)\n",
    "print(f\"\\nDatabase File:\")\n",
    "print(f\"  Location: {os.path.abspath(DB_PATH)}\")\n",
    "print(f\"  Size: {db_size_mb:.2f} MB\")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"PREPROCESSING COMPLETE\")\n",
    "print(\"=\"*120)\n",
    "print(f\"\\nDatabase ready for both projects:\")\n",
    "print(f\"\\nProject 1 - Full-Stack Web Development:\")\n",
    "print(f\"  T1 (Citation & Collaboration Networks): {START_YEAR_T1}-{END_YEAR} (past 5 years)\")\n",
    "print(f\"  T2 (Timeline + Patent Distribution): {START_YEAR_T2}-{END_YEAR} (past 10 years)\")\n",
    "print(f\"  T3 (Network Refinement): Uses T1 data\")\n",
    "print(f\"\\nProject 2 - LLM Agent Analysis:\")\n",
    "print(f\"  Uses all VT CS papers from {START_YEAR_T2}-{END_YEAR}\")\n",
    "print(f\"  All tables filtered for VT + CS + year range\")\n",
    "print(f\"\\nDatabase location: {os.path.abspath(DB_PATH)}\")\n",
    "print(\"=\"*120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a43be6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Year Range Verification ===\n",
      "\n",
      "Papers by year with applicable tasks:\n",
      "\n",
      " year  total_papers                      applicable_to\n",
      " 2022           109 Project 1 (T1, T2, T3) + Project 2\n",
      " 2021          5307 Project 1 (T1, T2, T3) + Project 2\n",
      " 2020          5096 Project 1 (T1, T2, T3) + Project 2\n",
      " 2019          4845         Project 1 (T2) + Project 2\n",
      " 2018          4439         Project 1 (T2) + Project 2\n",
      " 2017          4495         Project 1 (T2) + Project 2\n",
      " 2016          4193         Project 1 (T2) + Project 2\n",
      " 2015          4190         Project 1 (T2) + Project 2\n",
      "\n",
      "\n",
      "Papers available for:\n",
      "  Project 1 T1 (Citation & Collaboration, 2020-2025): 10,512 papers\n",
      "  Project 1 T2 (Timeline, 2015-2025): 32,674 papers\n",
      "  Project 2 (LLM Agent, 2015-2025): 32,674 papers\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Year Range Verification ===\\n\")\n",
    "\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "\n",
    "print(\"Papers by year with applicable tasks:\\n\")\n",
    "year_summary = pd.read_sql(f\"\"\"\n",
    "    SELECT \n",
    "        year,\n",
    "        COUNT(*) as total_papers,\n",
    "        CASE \n",
    "            WHEN year >= {START_YEAR_T1} THEN 'Project 1 (T1, T2, T3) + Project 2'\n",
    "            WHEN year >= {START_YEAR_T2} THEN 'Project 1 (T2) + Project 2'\n",
    "            ELSE 'Out of range'\n",
    "        END as applicable_to\n",
    "    FROM papers \n",
    "    GROUP BY year \n",
    "    ORDER BY year DESC\n",
    "\"\"\", conn)\n",
    "print(year_summary.to_string(index=False))\n",
    "\n",
    "t1_count = pd.read_sql(f\"\"\"\n",
    "    SELECT COUNT(*) as count FROM papers WHERE year >= {START_YEAR_T1}\n",
    "\"\"\", conn).iloc[0]['count']\n",
    "\n",
    "t2_count = pd.read_sql(f\"\"\"\n",
    "    SELECT COUNT(*) as count FROM papers WHERE year >= {START_YEAR_T2}\n",
    "\"\"\", conn).iloc[0]['count']\n",
    "\n",
    "print(f\"\\n\\nPapers available for:\")\n",
    "print(f\"  Project 1 T1 (Citation & Collaboration, {START_YEAR_T1}-{END_YEAR}): {t1_count:,} papers\")\n",
    "print(f\"  Project 1 T2 (Timeline, {START_YEAR_T2}-{END_YEAR}): {t2_count:,} papers\")\n",
    "print(f\"  Project 2 (LLM Agent, {START_YEAR_T2}-{END_YEAR}): {t2_count:,} papers\")\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcdb3d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample Queries ===\n",
      "\n",
      "Most recent 5 papers:\n",
      "\n",
      "  paper_id title  year  citation_count\n",
      "3159659315  None  2022              13\n",
      "3210404677  None  2022               3\n",
      "3197283713  None  2022               2\n",
      "3194526859  None  2022               2\n",
      "3193947462  None  2022               2\n",
      "\n",
      "\n",
      "Top 5 most cited papers:\n",
      "\n",
      "  paper_id title  year  citation_count\n",
      "2751884637  None  2017            8514\n",
      "2104549677  None  2015            8179\n",
      "2614986146  None  2016            3451\n",
      "2527824850  None  2016            3429\n",
      "2751792491  None  2017            2030\n",
      "\n",
      "\n",
      "Papers with most patent citations:\n",
      "\n",
      "  paper_id title  year  patent_count\n",
      "2104549677  None  2015            24\n",
      "2053707749  None  2015            21\n",
      "1956340063  None  2015            19\n",
      "2896722260  None  2018            19\n",
      "1933349210  None  2015            17\n",
      "\n",
      "\n",
      "Most prolific authors:\n",
      "\n",
      " author_id  paper_count\n",
      "1750292016          481\n",
      "1263832234          445\n",
      "1865774345          405\n",
      "1972755273          398\n",
      "2102906916          374\n",
      "\n",
      "\n",
      "Papers by field (for Project 2 analysis):\n",
      "\n",
      "                  field_name  paper_count\n",
      "            Computer science         7103\n",
      "     Artificial intelligence         1216\n",
      "                   Algorithm          531\n",
      "            Computer network          448\n",
      "            Machine learning          445\n",
      "       Distributed computing          433\n",
      "         Real-time computing          357\n",
      "           Computer security          339\n",
      "                Data science          320\n",
      "  Humanâ€“computer interaction          309\n",
      "                  Simulation          302\n",
      "                 Data mining          294\n",
      "       Computational biology          275\n",
      "             Computer vision          241\n",
      "         Pattern recognition          217\n",
      "        Knowledge management          195\n",
      "             Embedded system          160\n",
      "          Parallel computing          141\n",
      "                  Multimedia          134\n",
      "              World Wide Web          130\n",
      "Theoretical computer science          124\n",
      "            Internet privacy          117\n",
      "        Software engineering          114\n",
      "              Bioinformatics          100\n",
      "       Computational physics           97\n",
      "          Telecommunications           92\n",
      "       Information retrieval           83\n",
      "        Computer engineering           82\n",
      "            Operating system           69\n",
      " Natural language processing           69\n",
      "                    Database           69\n",
      "        Programming language           60\n",
      "           Computer hardware           50\n",
      "       Computational science           48\n",
      "           Cognitive science           45\n",
      "          Speech recognition           44\n",
      "     Computational chemistry           36\n",
      "       Computer architecture           31\n",
      "  Computer graphics (images)           27\n",
      "\n",
      "\n",
      "Sample queries complete\n",
      "Database ready for Project 1 and Project 2!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Sample Queries ===\\n\")\n",
    "\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "\n",
    "print(\"Most recent 5 papers:\\n\")\n",
    "recent_papers = pd.read_sql(\"\"\"\n",
    "    SELECT paper_id, title, year, citation_count\n",
    "    FROM papers\n",
    "    ORDER BY year DESC, citation_count DESC\n",
    "    LIMIT 5\n",
    "\"\"\", conn)\n",
    "print(recent_papers.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nTop 5 most cited papers:\\n\")\n",
    "top_cited = pd.read_sql(\"\"\"\n",
    "    SELECT paper_id, title, year, citation_count\n",
    "    FROM papers\n",
    "    WHERE citation_count IS NOT NULL\n",
    "    ORDER BY citation_count DESC\n",
    "    LIMIT 5\n",
    "\"\"\", conn)\n",
    "print(top_cited.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nPapers with most patent citations:\\n\")\n",
    "top_patents = pd.read_sql(\"\"\"\n",
    "    SELECT p.paper_id, p.title, p.year, p.patent_count\n",
    "    FROM papers p\n",
    "    WHERE p.patent_count > 0\n",
    "    ORDER BY p.patent_count DESC\n",
    "    LIMIT 5\n",
    "\"\"\", conn)\n",
    "print(top_patents.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nMost prolific authors:\\n\")\n",
    "top_authors = pd.read_sql(\"\"\"\n",
    "    SELECT author_id, COUNT(DISTINCT paper_id) as paper_count\n",
    "    FROM paper_author_affiliations\n",
    "    GROUP BY author_id\n",
    "    ORDER BY paper_count DESC\n",
    "    LIMIT 5\n",
    "\"\"\", conn)\n",
    "print(top_authors.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nPapers by field (for Project 2 analysis):\\n\")\n",
    "papers_by_field = pd.read_sql(\"\"\"\n",
    "    SELECT f.field_name, COUNT(DISTINCT pf.paper_id) as paper_count\n",
    "    FROM paper_fields pf\n",
    "    JOIN fields f ON pf.field_id = f.field_id\n",
    "    GROUP BY f.field_name\n",
    "    ORDER BY paper_count DESC\n",
    "\"\"\", conn)\n",
    "print(papers_by_field.to_string(index=False))\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"\\n\\nSample queries complete\")\n",
    "print(\"Database ready for Project 1 and Project 2!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
